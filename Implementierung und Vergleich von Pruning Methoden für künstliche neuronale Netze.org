#+TITLE: Implementierung und Vergleich von Pruning Methoden für künstliche neuronale Netze
#+AUTHOR: Lucas Sas Brunschier
#+DESCRIPTION: Bachelor Arbeit
#+DATE: XX.XX.2020
#+LATEX_CLASS: report
#+language: de
#+LATEX_HEADER: \usepackage[ngerman]{babel}
#+LATEX_HEADER: \usepackage{a4wide}
#+LATEX_HEADER: \usepackage[backend=bibtex, style=numeric] {biblatex}
#+LATEX_HEADER: \addbibresource{references.bib}
#+LATEX_HEADER: \usepackage{acronym}
#+STARTUP: showall
#+STARTUP: hideblocks
#+TOC: tables

# Title Page
#+begin_src emacs-lisp :exports results :results none :eval export
  (make-variable-buffer-local 'org-latex-title-command)
  (setq org-latex-title-command (concat
     "\\begin{titlepage}\n"
     "\\begin{center}\n"
     "\\includegraphics[width=5cm]{./resources/haw_logo.jpg}\n"
     "\\vspace{2cm}\n"
     "{\\par \\LARGE Hochschule für angewandte Wissenschaften Landshut}\n"
     "\\vspace{0.6cm}"
     "{\\par \\Large Fakultät Informatik} \\vspace{1.2cm}\n"
     "{\\par \\Huge \\bf Bachelor Arbeit} \\vspace{1cm}\n"
     "{\\par \\LARGE %t } \\vspace{1cm}\n"
     "{\\par \\Large \\it von %a} \\vspace{0.2cm}\n"
     "{\\par Matrikel-Nr.: 1088709} \\vspace{1cm} \n"
     "{\\par Abgabedatum: %D} \\vspace{3cm}\n"
     "\\end{center}\n"
     "{\\par Betreuer: Prof. Dr. Mona Riemenschneider}\n"
     "{\\par Zweitkorrektor: Prof. Dr. Abdelmajid Khelil}\n"
     "\\end{titlepage}\n"))
#+end_src

# Abbildungsverzeichnis
#+BEGIN_LATEX
\newpage
\listoffigures
\newpage
#+END_LATEX

# Abkürzungsverzeichnis
#+BEGIN_LATEX
\begin{acronym}[Bash]
\acro{ANN}{künstliches neuronales Netz}
\acro{TF}{Tensor Flow}
\acro{ML}{Machine Learning oder zu deutsch maschinellen Lernen}
\acro{GPU}{Graphics Processing Unit}
\acro{TPU}{Tensor Processing Unit}
\end{acronym}
\newpage
#+END_LATEX

* Einleitung
** Neuronale Netze

Bei dem Forschungsbereich der neuronalen Netze handelt es sich um eine Teilmenge des Bereichs der künstlichen Intelligenz.
Genauer lassen sich künstliche neuronale Netze als einen Teil des maschinellen Lernens einordnen.
Die Inspiration für \acp{ANN} kommt ursprünglich aus der Biologie und der dort vorkommenden neuronalen Verbindungen in Nervensystemen von Lebewesen.
Jedoch sind die Forschungsgebiete zu neuronalen Netzen aus der Biologie und der Informatik/Mathematik zum Großteil disjunkt.
cite:10.5555/3086952

#+LABEL: fig:network
#+CAPTION[Diagramm eines künstlichen neuronalen Netzes]: Diagramm eines fully connected \ac{ANN}, mit einem Hidden Layer (hier blau gekennzeichnet).
#+CAPTION: Es ist gut zu erkennen, wie benachbarte Schichten
#+CAPTION: [[https://commons.wikimedia.org/wiki/File:NeuralNetwork.png][[quelle]​]]
#+ATTR_LATEX: :float wrap :width 8cm :center nil
[[./resources/neural_network.png]]

*** Historisches
Obwohl \ac{ANN}'s erst ca. 2008 ihre Blütezeit erreicht haben, ist die zu Grunde liegende Technologie bereits seit
Mitte bis Ende des 20. Jahrhunderts bekannt.
So schuf Frank Rosenblatt im Jahre 1975 das in Abbildung ref:fig:perceptron dargestellte Modell eines Perceptrons cite:werbos1975beyond, eine
mathematische Abstraktion des aus der Biologie bekannten Neuron.
Das Perceptron wird bis heute als Modell für ein alleinstehendes Neuron in einem \ac{ANN} verwendet.
In dem kommenden Kapitel [[perceptron]] wird noch im Detail auf das Modell des Perceptrons eingegangen.
Der Backpropagation Algorithmus, bei dem es sich um eine Implementation der Kettenregel zur automatischen Differenzierung
von Parametern eines \ac{ANN} handelt, wurde im Jahre 1986 publiziert cite:Rumelhart_1986.
Auch heute erweist sich der Backpropagation Algorithmus als eine sehr effiziente Methode die Gradienten[fn:gradient] eines \ac{ANN}'s zu berechnen und
findet beinahe unverändert Einsatz in verschiedensten modernen Deep Learning Frameworks.
Eine häufige Fehlinformation die über den Backpropagation Algorithmus verbreitet wird, ist dieser sei für das "Lernen" des neuronalen Netzes
verantwortlich.
Dies ist inkorrekt, da eigentlich der Gradient Descent Algorithmus cite:Curry_1944 die von dem Backpropagation berechneten Gradienten nutzt um
die Parameter des Netzes so zu manipulieren, dass der Loss[fn:loss] minimiert wird.

*** Strukturelle Beschaffenheit von neuronalen Netzen <<netstruct>>
#+begin_quote
In diesem Kapitel wird ausschließlich der Aufbau eines fully connected[fn:fullyconnected] feed forward[fn:feedforward] neural networks behandelt.
#+end_quote
Ein \ac{ANN} besteht primär aus mehreren Schichten (Layern) $L_1, \dots, L_n$.
Bei dem ersten Layer $L_1$ handelt es sich um den so genannten Input Layer, der für die Aufnahme von Eingabedaten zuständig ist.
Analog fungiert dieser als Eingabe-Interface des neuronalen Netzes für den Anwender des \acp{ANN}.
So erwartet ein Input-Layer mit $8$ Neuronen einen Input Vektor von $8$ Werten.
Schichten $L_2, \dots, L_{n-1}$ werden als Hidden-Layer des neuronalen Netzes bezeichnet, da diese für den Außenstehenden nicht direkt einsichtig sind.
Die letzte Schicht des \acp{ANN}, $L_n$ ist der Output-Layer des Netztes und dient als zweites Interface für den Nutzer.
In Abbildung ref:fig:network lässt sich die Architektur eines einfachen Feed Forward[fn:feedforward] Networks und dessen Layer klar erkennen.
cite:10.5555/3086952

*** Das Perceptron <<perceptron>>
Ein Perceptron ist ein Modell, das eine Reihe von Eingabedaten (Inputs) $a$ auf einen gemeinsamen Output $y$ nach der Form  $\mathbb{R}^n \rightarrow \mathbb{R}$ abbildet.
Die verschiedenen Inputs $a^n$ werden durch Gewichtungen $w^n$ verschieden stark gewichtet, also $a_1 \times w_1 + \dots + a_n \times w_n = y$ oder in Vektorschreibweise $a \times w^T = y$.
In heutigen State of the Art Deep Learning Frameworks enthält das Perceptron zusätzlich noch eine nicht-lineare Komponente in der Form einer Aktivierungsfunktion $\sigma$.
Einige der häufig eingesetzten Aktivierungsfunktionen können in Tabelle ref:tab:aktivierungsfunktion gefunden werden.
In Kapitel ref:activations dieser Arbeit, wird noch im Genaueren auf Zusammenhänge zwischen Aktivierungsfunktion und Sparsity[fn:sparsity] eines \ac{ANN} Modells eingegangen.
cite:Rosenblatt_1958

#+LABEL: tab:aktivierungsfunktion
#+CAPTION[Populäre Aktivierungsfunktionen]: Aktivierungsfunktionen enthalten meist eine nicht-linearität, die nötig ist um neuronale Netze
#+CAPTION: auf nicht lineare Zusammenhänge in Datensätzen trainieren zu können.
| Name                  | Funktion                                                        |
|-----------------------+-----------------------------------------------------------------|
| Logistische Funktion  | $\frac{1}{1+e^t}$                                               |
| Tangens Hyperbolicus  | $\frac{(e^x-e^{-x})}{(e^x+e^{-x})}$                             |
| Rectified Linear Unit | $f(x)= \begin{cases} 0\ for\ x\leq0 \\ x\ for\ x>0 \end{cases}$ |

Ein Perceptron kann statt in einem grafischen Modell visualisiert zu werden auch als eine mathematische Funktion eqref:eq:percept behandelt werden.

\begin{equation}f(a, w)=\sigma(a\times w^T)=y \label{eq:percept}\end{equation}


#+LABEL: fig:perceptron
#+CAPTION[Diagramm eines einfachen Perceptrons]: Abbildung eines einfachen Perceptrons.
#+CAPTION: Es ist gut zu erkennen, wie der Input Vektor des Layers $\begin{pmatrix} x_1 \\ \dots \\ x_m \end{pmatrix}$ und
#+CAPTION: der Weight Vektor $\begin{pmatrix}w_1 \\ \dots \\ w_m \end{pmatrix}$
#+CAPTION: auf die Variable $y$ durch $\begin{pmatrix} x_1 \\ \dots \\ x_m \end{pmatrix} \begin{pmatrix} w_1 \\ \dots \\ w_m \end{pmatrix}^T = y$ abgebildet werden.
#+CAPTION: Der Output des Layers wird durch die Anwendung einer Aktivierungsfunktion auf die Variable $y$ generiert.
#+CAPTION: cite:towardsdatascience
[[./resources/perceptron.png]]

*** Der Datenfluss in einem künstlichen neuronalen Netz
Daten in einem Feed-Forward \ac{ANN} verlaufen immer linear von Input-Layer in Richtung Output-Layer.
Da bereits in Kapitel ref:perceptron auf die Beschaffenheit eines Layers eingegangen wurde, können wir einen Layer $L$ als eine Funktion $f(x)$ betrachten.
Da der jeweilige Output eines Layers $L_i$ als der Input des Layers $L_{i+1}$ dient, können wir ein Netzwerk als eine Verkettung an Funktionen betrachten.
Im Allgemeinen kann dies in der Form eqref:eq:net ausgedrückt werden.

\begin{equation} {f_n(\dots (f_1(x)))=y \ \ \ \label{eq:net} \end{equation}

#+BEGIN_QUOTE
In anderen Layer-Architekturen wie Recurrent oder LSTM ist es durchaus möglich Daten auch an vorherige Layer abzugeben.
Diese Architekturen sind jedoch nicht Teil dieser Arbeit.
#+END_QUOTE
Die Möglichkeit ein \ac{ANN} als eine Verkettung von Funktionen formulieren zu können ist essentiell um Algorithmen wie Backpropagation zur
Differenzierung von Parametern nutzen zu können.

** Einführung in naive Pruning Methoden für künstliche neuronale Netze

Es lässt sich durch Beobachtung der künstlichen neuronalen Netze der letzten Jahre feststellen,
dass die Komplexität und die damit einhergehende Anzahl von Neuronen und deren Verbindungen immer weiter zunehmen. cite:altenberger18:_non_techn_survey_deep_convol
Gleichzeitig werden diese komplexeren und größeren \ac{ANN} Architekturen auch auf schwächeren eingebetteten Geräten eingesetzt.
Dadurch werden Optimierungen an neuronalen Netzen immer relevanter, da dies Inferenz-Zeit und Modellgröße minimieren kann.
Verfahren wie Quantisierung können die Laufzeit und den Speicherverbrauch von \ac{ANN}'s deutlich verbessern, jedoch können auch
Pruning Verfahren massive Verbesserungen versprechen. cite:Frankle2018
Pruning Verfahren versuchen durch das Entfernen von Verbindungen oder auch ganzen Neuronen, die Sparsity eines Modells zu erhöhen.
Weight oder auch Connection Pruning bezeichnet den Vorgang Verbindungen aus einem \ac{ANN} zu entfernen.
Dabei werden die Verbindungen eliminiert, also mit $0$ gewichtet. Die ist in Abbildung ref:fig:naiveweightpruning dargestellt.
Die ausgewählten Verbindungen oder Neuronen werden durch eine Heuristik bestimmt, eine Heuristik könnte beispielsweise die niedrigst gewichteten Verbindungen sein.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
from scripts import quad_plot
import sys
sys.path.append('./condense')
from condense.optimizer.layer_operations.weight_prune import w_prune_layer
model = keras.models.load_model('./resources/models/iris.h5')
layer = 1
quad_plot(w_prune_layer(model.get_weights()[0::2][layer], .85),
          model.get_weights()[0::2][layer],
          './resources/plots/iris-weight-pruning.png')
return './resources/plots/iris-weight-pruning.png'
#+END_SRC

#+LABEL: fig:naiveweightpruning
#+CAPTION[Visualisierung von Weight Pruning]:
#+CAPTION: In diesem hier dargestellten Dense Layers eines neuronalen Netzes, wurde die Sparsity des Modells durch Pruning der Verbindungen auf $85\%$ erhöht.
#+CAPTION: Es ist gut zu beobachten, wie nur leicht gewichtete Verbindungen durch Pruning deaktiviert werden, hier durch schwarze Pixel zu erkennen.
#+CAPTION: Bei dem Netz handelt es sich um ein durch TensorFlow 2.0 trainiertes Modell. Bei dem Training wurde der Iris Datensatz genutzt.
#+RESULTS[ab87174a1c51a0676afdc27b4d18b3dda9742200]:
[[file:./resources/plots/iris-weight-pruning.png]]


Analog zu dem Pruning der Verbindungen existiert auch Neuron-Pruning, also das entfernen ganzer Neuronen und deren Verbindungen aus dem \ac{ANN}.
Dies wird in Abbildung ref:fig:naiveneuronpruning durch die Visualisierung eines Layers vor und nach Neuron-Pruning gezeigt.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
from scripts import quad_plot
import sys
sys.path.append('./condense')
from condense.optimizer.layer_operations.unit_prune import u_prune_layer

layer= 1
model = keras.models.load_model('./resources/models/iris.h5')

quad_plot(u_prune_layer(model.get_weights()[0::2][layer], .4),
          model.get_weights()[0::2][layer], './resources/plots/iris-unit-pruning.png')
return './resources/plots/iris-unit-pruning.png'
#+END_SRC

#+LABEL: fig:naiveneuronpruning
#+CAPTION[Visualisierung von Unit/Neuron Pruning]:
#+CAPTION: In diesem Beispiel wird das oben verwendete Modell durch eine naive Implementation des Neuron Pruning um einen Faktor von $0.4$ optimiert.
#+CAPTION: Vertikale Linien repräsentieren in diesem Diagramm die Weights eines Neuronen.
#+CAPTION: Man kann sehr gut beobachten wie sich ganze Neuronen schwarz färben, also deaktiviert werden.
#+RESULTS[e9612a231390b236029f616c4dcf02b09ad4cd99]:
[[file:./resources/plots/iris-unit-pruning.png]]


** Industrierelevanz
Pruning von künstlichen neuronalen Netzen bietet vielen Unternehmen die Möglichkeit Optimierungen an schon bestehenden \ac{ANN} Modellen vorzunehmen.
Diese Optimierungen können unter Umständen ermöglichen komplexere Modelle auf schwächeren Computern zu nutzen.
Beispielsweise eingebettete Geräte können dabei effizienter Daten durch neuronale Netze auswerten.
Besonders in Situationen in denen das Modell möglichst schnell ein Prognose abgeben soll, wie beispielsweise bei Teilen von
selbständig fahrenden Autos bietet Pruning Chancen auf enorme Verbesserungen.
Zudem bietet Pruning eine Möglichkeit, \ac{ANN} Modelle ohne signifikante Einbußen von Genauigkeit zu optimieren. cite:Frankle2018
Dies sollte Pruning Methoden auf deutlich mehr \ac{ANN} Modellen einsetzbar machen.

** Ziel dieser Arbeit <<ziel>>
*** Erstellung eines Pruning Frameworks <<ziel_framework>>
Ziel dieser Arbeit ist es primär ein Python Framework zu entwickeln, das mehrere verschiedene Typen von Pruning Methoden implementieren soll.
Ein wichtiger Fokus sollte bei der Architektur des Framework sein, dies in Zukunft möglichst einfach erweitern zu können.
Dokumentation der verschiedenen Module ist aus diesem Grund sehr wichtig und sollte im Laufe der Arbeit auch immer aktualisiert werden.
Bei dem Design der Nutzer-Schnittstellen sollte auf eine möglichst einfachere und saubere Architektur geachtet werden,
da auch Nutzer ohne ausgibige Python Erfahrung im Stande sein sollte die Tools dieses Frameworks zu nutzen.

#+begin_src mermaid :file resources/plots/pruning-framework.png :theme forest :background transparent
graph LR
    input(Input Model) --> interface(High Level Interface)
    interface --> parser(Model Parser)
    pruning(Pruning Engine) --> output(Pruned Model)
    parser --> pruning
#+end_src

#+LABEL: fig:rough-project-structure
#+CAPTION[Pruning Framework Konzept]: Der hier gezeigte Graph soll das grobe Konzept, des im Laufe dieser Arbeit entstehenden Pruning Frameworks zeigen.
#+RESULTS:
[[file:resources/plots/pruning-framework.png]]

Zudem sollte das Framework kompatibel mit aktueller Deep Learning Software und deren Formate kompatibel sein.
Kompatibilität mit \ac{TF} [fn:tensorflow]/Keras[fn:keras] steht bei diesem Projekt im Vordergrund, da auch intern \ac{TF} für Trainings-Operationen genutzt wird.
Optional sollte auch die Möglichkeit bestehen ein Modell in dem ONNX[fn:onnx] Format zu exportieren, um auch Kompatibilität mit anderen Frameworks sicherzustellen.

*** Erkenntnisse über Pruning Methoden
Zudem sollte diese Arbeit durch die Forschung an diversen Pruning Methoden auch neue allgemeine Erkenntnisse hervorbringen.
Alle Resultate dieser Arbeit sollten klar nachvollziehbar und durch das Lesen dieser Arbeit erkenntlich sein.
Um Lesern die Resultate dieser Arbeit möglichst nachvollziehbar zu gestalten, werden alle nötigen Dateien dieser Arbeit in Form eines
GitHub Repositories[fn:github] veröffentlicht.
Der Quellcode um alle Grafiken/Diagramme und Resultate dieser Arbeit zu erzeugen ist dort zu finden.

* Methodik
** Erstellung des Frameworks
Wie bereits in Kapitel ref:ziel_framework eingegangen wurde, sollte bei der Erstellung des Frameworks ein
großer Fokus auf die zukünftige Erweiterbarkeit liegen.
Aus diesem Grund wird im Besonderen auf die Architektur, Tests und der Dokumentation sehr viel Wert gelegt.

*** Wahl der Sprache & Frameworks
Python 3 lag als primäre Programmiersprache nahe, da diese sehr weit in der KI/ML Gemeinschaft verbreitet ist.
So basieren die meisten Frameworks für neuronale Netze wie Theano, Tensorflow oder Torch auf Python oder einer Implementation in C/C++
die mit der Hilfe von der CPython Bridge angesprochen werden kann.
Somit ist eine sehr gute Leistung trotz einfacher API's erreichbar.
Zudem können in der weiteren Entwicklung dieses Projekt, gewisse Module auf eine performantere Sprache wie C/C++ ausgelagert werden.
Jedoch sind die meisten rechenaufwendigen Opertionen wie Matrix-Operationen bereits in Frameworks wie ~numpy~[fn:numpy] bereits implementiert.

Eine noch effizienter Methode, die sich besonders bei neuronalen Netzen als eine enorme Leistungssteigerung erwiesen hat,
ist die Nutzung von \acp{GPU} oder \acp{TPUS} um einen hohen Grad von Parallelisierung erreichen zu können.
Das Framework ~TensorFlow~[fn:tensorflow] stellt Implementationen von auf \ac{GPU} durchführbaren mathematischen Operationen
in Form eines Python Interfaces bereit.
Besonders Matrix Operationen lassen sich meist bestens parallelisieren und Deep Learning besteht zum Großteil aus genau diesen.

*** Architektur

*** Dokumentation

**** Allgemeine Dokumentation des Projekts

Durch GitHub Pages[fn:pages] und dem Tool Docsify[fn:docsify] ist es sehr einfach möglich eine ausgesprochen zugängliche Dokumentation
bzw. Landing Page für das Projekt zu generieren.
Der Inhalt dieser Dokumentation ist manuell erstellt und soll dem Benutzer nur einen groben Überblick über die wichtigsten Aspekte des Frameworks geben.
Detailliertere Informationen zu internen Schnittstellen können jedoch trotzdem sehr einfach über die Modul Dokumentation aus Unterpunkt ref:pdoc eingesehen werden.

**** Automatisierte Generierung von Dokumentation aus Source Code des Projektes <<pdoc>>

Durch das Tool pdoc3[fn:pdoc] kann aus dem Source Code eines Python Modules und dessen Docstrings[fn:docstring] eine Dokumentation in Form einer
HTML Seite generiert werden.
Diese ist in die Allgemeine Dokumentation des Projekts direkt eingebettet und erfordert keine separate Website.
Da bei der Generierung dieser Dokumentation keine weitere manuelle Arbeit geleistet werden muss, kann diese ohne weitere Umstände automatisiert
über GitHub Actions[fn:actions] realisiert werden.
So wird beispielsweise bei einer Änderung des Modules auf dem ~master~ Branch des Projekts ein Script ausgelöst, die eine aktualisierte Dokumentation auf
der öffentlichen Webseite zur Verfügung stellt.
Natürlich koaliert die Qualität der generierten Dokumentation direkt mit der Qualität der im Source Code verfassten Docstrings,
somit ist zudem sicherzustellen, dass auch hier ein gewisser Qualitätsstandart einzuhalten ist.
Wie dies innerhalb dieses Projekts implementiert wurde wird in Kapitel ref:tests Punkt ref:docstyle_tests genauer erläutert.

*** Tests <<tests>>

**** Unit Tests

Um sicherzustellen, dass die Qualität der Software einen gewissen Standard erfüllt, sind Unit Tests mit Sicherheit ein essentieller Bestandteil dieses Projekts.
Dazu wurde das sehr weit verbreitete Testing Framework pytest[fn:pytest] genutzt.
Zusätzlich werden Daten über die Test-Coverage der Tests Dank des pytest-cov plugins für pytest generiert.

**** Linting

Um eine ästhetisch ansprechende Formatierung des Quellcodes im Laufe des Projekts beizubehalten.
Durch das Tool pylint wird auch dies automatisiert möglich mit der Hilfe von GitHub Actions möglich.
Einige der wichtigsten von der Software überprüften Punkte sind:
- unnötige ~import~ Statements
- korrekte Variablennamen
- Zeichen per Zeile
- Zeilen-Abstände

**** Docstyle Tests <<docstyle_tests>>

Um auch wichtige Teile wie die Dokumentation von Funktionen nicht im Laufe des Projekts zu vernachlässigen, wurde das Tool pydocstyle[fn:pydocstyle] genutzt um auch
Docstrings auf Korrektheit zu überprüfen.
Als Style der Docstrings wurde sich auf den von Google genutzten Styleguide[fn:styleguide] berufen.
Durch diese Methodik, müssen alle Module, Klassen und Funktionen über Docstrings verfügen, da sie sonst nicht auf einen der nicht-feature branches des Repositories gepullt werden können.
Dadurch lässt sich eine enorm detaillierte Dokumention aller öffentlichen Schnittstellen automatisiert generieren.

** Datensätze <<datensatz>>
Die meisten Datensätze, die in dieser Arbeit verwendet wurden, wurden durch das Python 3 Modul ~tensorflow_datasets~ cite:TFDS bezogen.
In Tabelle ref:tab:dataset sind alle in dieser Arbeit verwendeten Datensätze gelistet.

#+LABEL: tab:dataset
#+CAPTION[In dieser Arbeit verwendeten Datensätze]: Eine Liste von, in dieser Arbeit vewendeten Datensätzen.
| Datensatz                     | Beschreibung | Quelle                                       |
|-------------------------------+--------------+----------------------------------------------|
| Iris Dataset cite:fisher36lda |              | https://archive.ics.uci.edu/ml/datasets/iris |
| ImageNet cite:imagenet_cvpr09                     |              | http://www.image-net.org                     |
| ImageNet V2                   |              | https://github.com/modestyachts/ImageNetV2   |

* Implementierung
** Sparsity Mask <<sparsity_mask>>

Der Begriff Sparsity Mask/Tensor bezieht sich in dieser Arbeit auf einen Binären Tensor der definiert, welche Felder aus einem Weights Tenor eine $0$ enthalten.
Die Sparsity Mask ist ein essentieller Bestandteil für fast jede Pruning Methode.
Deswegen ist die Klärung dieses Begriffs auch sehr wichtig für kommende Kapitel dieser Arbeit.
Auswirkungen der Sparsity Mask auf ein Array wird in Abbildung ref:fig:simplesparsity dargestellt.

#+BEGIN_SRC python :exports results :results file :cache yes
import numpy as np
import matplotlib.pyplot as plt

a = np.random.rand(10, 20)
m = np.random.rand(10, 20) < 0.4

plt.figure(figsize=(10, 2))
plt.subplot(131)
plt.imshow(a)
plt.title('Ursprüngliches Array $a$')
plt.subplot(132)
plt.imshow(m*-1, cmap='binary')
plt.title('Sparsity Mask $m$')
plt.subplot(133)
plt.imshow(a*m)
plt.title('Maske auf Array angewandt $a \\times m$')
plt.tight_layout()
plt.savefig('resources/plots/masking.png')
return 'resources/plots/masking.png'
#+END_SRC

#+LABEL: fig:simplesparsity
#+CAPTION[Anwendung von einer Sparsity Mask auf ein einfaches Array]:
#+CAPTION: In dieser Grafik wird jedes Feld durch weiß ($1$) und schwarz ($0$) binär visualisiert.
#+CAPTION: Die Multiplikation der Maske $m$ mit dem Array $a$ resultiert in der durch $m$ 'gefilterten' Version des ursprünglichen Arrays $a$.
#+RESULTS[b489619f2ae1bf26670eaaa816a599d772d79015]:
[[file:resources/plots/masking.png]]

** Keras/TensorFlow als Backend
Operationen des neuronalen Netzes wie das Training oder die Evaluierung wird durch das auf neuronale Netze ausgelegte \ac{ML} Framework TensorFlow ausgeführt.
Dies bietet Nutzern erhebliche Vorteile wie die mögliche Ausführung auf verschiedensten Plattformen wie GPU/CPU oder Hardware Beschleunigern.
Zudem lassen sich TensorFlow "Layer" eines neuronalen Netzes einfach durch eine öffentliche API erweitern.
Somit lässt sich im weiteren Verlauf des Projekts eine direkte Integration in das TensorFlow Ökosystem anstreben.
TensorFlow bietet zusätzlich durch das externe Modul ~model-optimization~[fn:model_opt] Optimierungen an einem Keras/TensorFlow Modell vorzunehmen.
Auch Pruning wird derzeit von dem Tool unterstützt, indem ein vorhandenes Modell durch die Augmentation von Layern um Pruning Funktionalität erweitert werden kann.
Es besteht somit die Möglichkeit auch ein schon bestehendes Backend für Pruning Operationen zu nutzen.
Somit können Pruning Operationen zuerst auf das ~model-optimization~ Modul ausgelagert werden und sich auf das Refitting und die Analyse von neuronalen Netzen konzentriert werden.

** Einfaches in Keras integriertes Pruning <<keras_module>>
Um eine einfache und klare Schnittstelle zu Keras Modellen zu bieten, gibt ~condense~ Nutzern die Möglichkeit
einzelne Keras Layer an einen Wrapper (~condense.keras.PruningWrapper~) zu übergeben.
Dieser implementiert Schnittstellen für andere Komponenten des Frameworks.
Diese werden genutzt um verschiedenste Layer-Manipulationen vornehmen zu können.

#+BEGIN_QUOTE
Die Hilfsfunktion ~condense.keras.wrap_layer(model, sparsity_function)~ instantiiert ein Modell und augmentiert alle möglichen Layer durch ~PruningWrapper~.
Für die meisten Use-Cases ist dies der empfohlene Weg Keras Modelle zu augmentieren/optimieren.
#+END_QUOTE

Dem ~PruningWrapper~ muss zusätzlich eine Sparsity Funktion übergeben werden.
Diese definiert wie sich das gewünschte Sparsity Ziel im Laufe des Trainings verhalten wird.
~condense~ stellt einige dieser Funktionen zur Verfügung, bietet aber die Möglichkeit durch die Implementierung
der abstrakten Klasse ~SparsityFunction~ ein anderes Verhalten zu bestimmen.
Beispiele für bereits existierende Funktionen sind:
- ~Constant(t_sparsity)~: Ziel Sparsity bleibt über den kompletten Trainingsprozess konstant
- ~Linear(t_sparsity)~: Ziel Sparsity nimmt mit laufenden Training bis zu dem letztendlichen Wert ~t_sparsity~ zu

In dem unten gezeigten Code Beispiel wird ein sequentielles Keras Modell mit $4$ Schichten erzeugt.
Dabei werden Input (Schicht $0$) und Output (Schicht $4$) nicht durch Pruning optimiert.
Dabei werden die Hidden Layer[fn:hidden] (Schicht $2$ und Schicht $3$) jeweils mit zwei verschiedenen Sparsity Funktionen
optimiert. Schicht $2$ wird mit einer konstanten Ziel Sparsity von ~0.5~ optimiert und Schicht $3$ mit einer über das Training
ansteigenden Ziel Sparsity bis zu ~0.7~.
Bei dem Training (~mode.fit(..., callbacks=[PruningCallback()])~) wird durch den ~PruningWrapper~ automatisch eine entsprechende Pruning Operation durchgeführt.

#+BEGIN_EXAMPLE
model = keras.models.Sequential(layers=[
    Dense(20, input_shape=(4,)),
    PruningWrapper(Dense(10), Constant(0.5)),
    PruningWrapper(Dense(40), Linear(0.7)),
    Dense(2)
])
#+END_EXAMPLE

Leider ist es durch die Architektur von Keras/TensorFlow notwendig bei dem Trainings-Funktionsaufruf ~.fit()~ auch den
Callback ~condense.keras.PruningCallback~ zu übergeben.
Dieser ist intern für die den ~.prune()~ Funktionsaufruf der jeweiligen ~PruningWrapper~ Instanzen verantwortlich.

#+CAPTION[Quellcode Beispiel: Pruning durch Keras Wrapper]:
#+CAPTION: Einfaches Pruning eines Modells ~model~ durch Pruning des ~condense.keras~ Moduls.
#+CAPTION:
#+BEGIN_SRC
import keras
import condense
from condense.keras import wrap_model, PruningCallback
from condense.optimizer.sparsity_functions import Constant

...

model = keras.models.load_model('...')
augmented = wrap_model(model, Linear(0.7))

augmented.fit(generator,
              epochs=10,
              steps_per_epoch=20,
              callbacks=[PruningCallback()])
#+END_SRC

** One-Shot Pruning
Unter One-Shot Pruning wird verstanden ein bereits trainiertes \ac{ANN} Modell durch eine Pruning Operation einmalig zu manipulieren.
Bei diesem Typ von Pruning werden keine Refitting (Kapitel ref:refitting) Operationen angewandt, also auch keine Datensätze benötigt.
Nachteile sind jedoch eine deutlich verlustbehafteten Optimierung im Kontrast zu iterativen Pruning (Kapitel ref:iterative).
Die Implementierung von One-Shot Pruning gestaltet sich im Vergleich mit iterativen Pruning als trivial.
Es wird eine Maskierungs-Funktion benötigt, um die zu prunenden Felder der Matrix zu ermitteln.
Eine einfache Maskierungs-Funktion wäre beispielsweise die Auswahl aller Felder, die unter eine festgelegten Threshold $t$ liegen.
Die durch diese Funktion resultierende Matrix $S$ wird auch Sparsity-Mask (siehe Kapitel ref:sparsity_mask) genannt.
$$
\caption{Beispiel: }\ W = \begin{pmatrix} 0.4 && 2 \\ 1.1 && 0\end{pmatrix} \text{ mit } t = 1.0 \Rightarrow S = \begin{pmatrix} 0 && 1 \\ 1 && 0 \end{pmatrix}
$$

#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
w = np.random.random((50,100))
t = 0.5
plt.figure(figsize=(15, 3))
plt.subplot(1, 4, 1)
plt.imshow(w, vmin=0.0, vmax=1.0)
plt.title('Source Matrix')

plt.subplot(1, 4, 2)
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=1.0)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.subplot(1, 4, 3)
t += .1
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=1.0)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.subplot(1, 4, 4)
t += .1
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=1.0)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.tight_layout()
plt.savefig('resources/plots/one-shot-random.png')
return 'resources/plots/one-shot-random.png'
#+END_SRC

#+LABEL: fig:oneshot
#+CAPTION[Visualisierung von One Shot Pruning]: Visualisierung von One Shot Pruning mit verschiedenen Thresholds $t$.
#+CAPTION: Die das Pruning durchführende Operation ~w[w < t] = 0~ setzt alle sich in der Maske ~w < t~ befindenden Felder der Matrix auf $0$.
#+CAPTION: Die Maske ~w < t~ kann durch eine beliebige (Maskierungs-)Funktion ersetzt werden.
#+RESULTS[4d4296df54d9b9d4b9103dabc59b43cf9a792ad6]:
[[file:resources/plots/one-shot-random.png]]

In Abbildung ref:fig:oneshot ist ein derartiges triviales Pruning von Feldern aus einer Matrix in visueller Form dargestellt.

** Iteratives Pruning <<iterative>>

*** Sparsity Mask <<sparsity_mask>>
Der Begriff Sparsity Mask/Tensor bezieht sich in dieser Arbeit auf einen Binären Tensor der definiert, welche Felder aus einem Weights Tenor eine $0$ enthalten.
Für iteratives Pruning ist eine Maske zwingend notwendig, da diese beim Refitting angibt, welche Weights geändert werden können. cite:Frankle2018
Wie Abbildung ref:fig:refitting zeigt, wird durch das Refitting ohne jegliche Maske die Sparsity des Modells zerstört.
Es ist gut in der rechts-unteren Grafik ersichtlich, wie das Refitting die Gewichtungen innerhalb eines Layers verändert.

#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
import sys
import keras
import tensorflow_datasets as tfds
sys.path.append('./condense')
import condense

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
ds = ds.repeat()

model = keras.models.load_model('resources/models/iris.h5')
layer = 2
pruning_intensity = 0.8

plt.figure(1, figsize=(10, 5))
plt.subplot(221)

plt.imshow(model.get_weights()[layer], vmax=1.0, vmin=.0)
plt.savefig('resources/plots/iterative-1.png')
plt.title('Weight Matrix eines trainierten Layers \n ohne Pruning Optimierungen')

plt.subplot(222)

plt.imshow((pruned := condense.one_shot(model, pruning_intensity)).get_weights()[layer], vmax=1.0, vmin=.0)
plt.savefig('resources/plots/iterative-1.png')
plt.title(f'Weight Matrix des optimierten Layers mit {round(condense.utils.model_utils.calc_model_sparsity(pruned), 4) * 100}% \n Modell Sparsity')

pre_training = pruned.get_weights()[layer]

# Retraining
plt.subplot(223)

pruned.compile('adam', 'mse')
pruned.fit(ds.batch(30), epochs=15, steps_per_epoch=100)
plt.imshow(pruned.get_weights()[layer], vmin=.0, vmax=1.0)
plt.title(f'Optimiertes Modell nach Refitting ({round(condense.utils.model_utils.calc_model_sparsity(pruned), 4) * 100}% Modell Sparsity)')

# Diff plot
plt.subplot(224)
plt.imshow(np.abs(pre_training - pruned.get_weights()[layer]), cmap='binary')
plt.title('Änderungen der Gewichtungen durch Refitting')

plt.tight_layout()

plt.savefig('resources/plots/iterative-1.png')

return 'resources/plots/iterative-1.png'
#+END_SRC

#+LABEL: fig:refitting
#+CAPTION[Weights eines Layers nach refitting]: In dieser Grafik wird der Layer eines Modells,
#+CAPTION: durch naives Weight und Neuron Pruning optimiert und anschließend durch refitting erneut trainiert.
#+RESULTS[c22c44c8b060e3dbd8f39ee7440c4dc97136acce]:
[[file:resources/plots/iterative-1.png]]

*** Refitting <<refitting>>

* Ergebnisse
** Einfluss von Aktivierungs-Funktionen auf Pruning Verfahren <<activations>>
* Fazit
* Ausblick

#+LATEX: \printbibliography

* Footnotes
[fn:sparsity] Sparsity beschreibt die Anzahl von Feldern in einem Tensor, die einer $0$ entsprechen.
Somit setzt sich die Sparsity eines künstlichen neuronalen Netzes die Sparsity jedes Layers zusammen.
[fn:tensorflow] Ein von Google entwickeltes Deep Learning Framework [[https://www.tensorflow.org][(tensorflow.org)]].
[fn:keras] Ehemalig externes Frontend von \ac{TF}, seit \ac{TF} 2.0 fester Bestandteil des Frameworks.
[fn:onnx] Universales Format für die Persistierung von \ac{ANN} Modellen.
[fn:loss] Als Loss wird der allgemeine Fehler des Netztes auf einem Datensatz bezeichnet.
Die Funktion die den Loss berechnet wird Loss Funktion benannt.
[fn:gradient] Alle Partiellen Ableitungen einer Funktion $f(x_1, \dots, x_n)$ werden als Gradienten $\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \dots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$ bezeichnet.
[fn:feedforward] Bei einem Feed Forward Netzwerk fließen die Daten immer linear durch das Netz und werden zu keinem Zeitpunkt an vorherige Schichten geleitet.
[fn:fullyconnected] Bei einem fully connected \ac{ANN} ist jedes Neuron aus Schicht $L$ mit allen Neuronen der Schicht $L+1$ verbunden.
[fn:pdoc] Open Source Projekt zur Generierung von Dokumentation aus Python Modulen. (https://pdoc3.github.io/pdoc/)
[fn:docstring] In Python wird ein Kommentar Block, der eine Funktion, Klasse oder ein Modul beschreibt als Docstring bezeichnet.
[fn:docsify] Framework mit dem Dokumentation in Form einer Web-App aus Markdown Dateien generiert werden kann. (https://docsify.js.org)
[fn:pages]  Eine von GitHub angebotene Dienstleistung Webseiten durch ein Repository bereitstellen zu können.(https://pages.github.com)
[fn:actions] Ein Dienst um automatisiert Tests oder Deployment Operationen durchzuführen. (https://github.com/features/actions)
[fn:pytest] Testing Framework für die Python Programmiersprache. (https://docs.pytest.org/)
[fn:pydocstyle] Tool um Docstrings eines Python Modules zu überprüfen. (https://github.com/PyCQA/pydocstyle)
[fn:styleguide] Ein von Google genutzter Styleguide für Python Projekte. (https://google.github.io/styleguide/pyguide.html)
[fn:pylint] Software um statische Syntax Analyse auf Python Source Code durchzuführen. (https://www.pylint.org)
[fn:model_opt] Toolkit für TensorFlow Modell Optimierungen. (https://github.com/tensorflow/model-optimization)
[fn:numpy] Numpy ist eine Mathematik Python Bibliothek, die von vielen wissenschaftlichen Modulen genutzt wird.
Mathematische Operationen, wie Matrix Multiplikation sind in C implementiert und somit sehr performant. (https://numpy.org)
[fn:hidden] Schichten eines Modells mit denen der Nutzer keine direkte Interaktion hat, werden als hidden bezeichnet.
Beispielsweise wären in einem sequentiellen Modell mit $4$ Schichten, Layer $2$ und $3$ hidden.
[fn:github] https://github.com/sirbubbls/pruning-ba
[fn:_]

bibliography:references.bib
bibliographystyle:apalike
