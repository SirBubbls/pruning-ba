#+TITLE: Implementierung und Vergleich von Pruning Methoden für künstliche neuronale Netze
#+AUTHOR: Lucas Sas Brunschier
#+DESCRIPTION: Bachelor Arbeit
#+DATE: 26.01.2021
#+LATEX_CLASS: report
#+language: de
#+LATEX_HEADER: \usepackage[a4paper, margin=1.1in]{geometry}
#+LATEX_HEADER: \usepackage[ngerman]{babel}
#+LATEX_HEADER: \usepackage[backend=bibtex, style=numeric] {biblatex}
#+LATEX_HEADER: \addbibresource{references.bib}
#+LATEX_HEADER: \usepackage{acronym}
#+LATEX_HEADER: \usepackage{pdfpages}

#+STARTUP: showall
#+STARTUP: hideblocks
#+TOC: nil

# Title Page
#+begin_src emacs-lisp :exports results :results none :eval export
  (make-variable-buffer-local 'org-latex-title-command)
  (setq org-latex-title-command (concat
     "\\begin{titlepage}\n"
     "\\begin{center}\n"
     "\\includegraphics[width=5cm]{./resources/haw_logo.jpg}\n"
     "\\vspace{5cm}\n"
     "{\\par \\LARGE Hochschule für angewandte Wissenschaften Landshut}\n"
     "\\vspace{0.6cm}"
     "{\\par \\Large Fakultät Informatik} \\vspace{1.2cm}\n"
     "{\\par \\Huge \\bf Bachelor Arbeit} \\vspace{1cm}\n"
     "{\\par \\LARGE %t } \\vspace{1cm}\n"
     "{\\par \\Large \\it von %a} \\vspace{0.2cm}\n"
     "{\\par Matrikel-Nr.: 1088709} \\vspace{1cm} \n"
     "{ Abgabedatum: 26.01.2021} \\vspace{3cm}\n"
     "\\end{center}\n"
     "{\\par Betreuer: Prof. Dr. Mona Riemenschneider}\n"
     "{\\par Zweitkorrektor: Prof. Dr. Abdelmajid Khelil}\n"
     "\\end{titlepage}\n"
     "\\includepdf{BA_Erklaerung.pdf}\n"
))
#+end_src

#+TOC: tables
# Danksagung

# Abbildungsverzeichnis
#+BEGIN_LATEX
\newpage
\listoffigures
\newpage
#+END_LATEX

# Abkürzungsverzeichnis
#+BEGIN_LATEX
\begin{acronym}[Bash]
\acro{ANN}{artificial neural network}
\acro{TF}{Tensor Flow}
\acro{ML}{Machine Learning oder zu deutsch maschinellen Lernen}
\acro{GPU}{Graphics Processing Unit}
\acro{TPU}{Tensor Processing Unit}
\end{acronym}
\newpage
#+END_LATEX

#+BEGIN_ABSTRACT
\begin{abstract}
Diese Arbeit beschreibt den Entwicklungsprozess und die Erkenntnisse des Python Pruning Frameworks \textbf{Condense} (github.com/sirbubbls/condense) für künstliche neuronale Netze.
Dabei werden Methoden von Pruning-Techniken und deren Auswirkungen erläutert, sowie verschiedenste Designentscheidungen und Anwendungen des unter dieser Arbeit entwickelten Frameworks erklärt und auf allgemeine Erkenntnisse hingewiesen.
Durch impirische Beispiele wird gezeigt, dass sich nicht bereits optimierte Modelle bestens durch Pruning-Methoden verbessern lassen und
somit eine hohe Parameter Sparsity mit minimalem Accuracy Verlust des künstlichen neuronalen Netzes erzeugt werden kann.
\end{abstract}
#+END_ABSTRACT

#+BEGIN_LATEX
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
An dieser Stelle möchte ich mich bei meinen Eltern,
\\~\\
Ronald Sas Brüning\\
Helga Brunschier de Sas \\
\\~\\
für die langjährige Unterstützung bedanken.
\\~\\
\\~\\
\\~\\
\\~\\
\\~\\
\\~\\
\end{center}
\vspace*{\fill}
#+END_LATEX

* Einleitung
** Neuronale Netze

Bei dem Forschungsbereich der neuronalen Netze handelt es sich um einen Sektor im Bereichs der künstlichen Intelligenz.
Genauer lassen sich künstliche neuronale Netze als einen Teil des maschinellen Lernens einordnen.
Die Inspiration für ein \ac{ANN} kommt ursprünglich aus der Biologie und den dort vorkommenden neuronalen Verbindungen in Nervensystemen von Lebewesen.
Jedoch sind die Forschungsgebiete zu neuronalen Netzen aus der Biologie und der Informatik/Mathematik zum Großteil disjunkt.
cite:10.5555/3086952

#+LABEL: fig:network
#+CAPTION[Diagramm eines künstlichen neuronalen Netzes]: Diagramm eines fully connected \ac{ANN}, mit einem Hidden Layer (hier blau gekennzeichnet).
#+CAPTION: Es ist gut zu erkennen, wie künstliche Neuronen benachbarter Schichten vollständig miteinander verbunden sind.
#+CAPTION: [[https://commons.wikimedia.org/wiki/File:NeuralNetwork.png][[quelle]​]]
#+ATTR_LATEX: :float wrap :width 7.5cm :center nil
[[./resources/neural_network.png]]

*** Historisches
Obwohl \ac{ANN}'s erst ca. 2008 ihre Blütezeit erreicht haben, ist die zu Grunde liegende Technologie bereits seit
Mitte bis Ende des 20. Jahrhunderts bekannt.
So schuf Frank Rosenblatt im Jahre 1975 das in Abbildung ref:fig:perceptron dargestellte Modell eines Perceptrons cite:werbos1975beyond.
Unter einem Perceptron versteht man die mathematische Abstraktion des aus der Biologie bekannten Neurons.
Dieses wird bis heute als Modell für ein alleinstehendes Neuron in einem \ac{ANN} verwendet.
In dem folgenden Kapitel [[perceptron]] wird noch im Detail auf das Modell des Perceptrons eingegangen.
Der Backpropagation Algorithmus, bei dem es sich um eine Implementation der Kettenregel zur automatischen Differenzierung
von Parametern eines \ac{ANN} handelt, wurde im Jahre 1986 publiziert cite:Rumelhart_1986.
Auch heute erweist sich der Backpropagation Algorithmus als eine sehr effiziente Methode die Gradienten[fn:gradient] eines \ac{ANN}'s zu berechnen und
findet beinahe unverändert Einsatz in verschiedensten modernen Deep-Learning-Frameworks.
Eine häufige Fehlinformation die über den Backpropagation Algorithmus verbreitet wird, ist, dieser sei für das "Lernen" des neuronalen Netzes
verantwortlich.
Dies ist inkorrekt, da eigentlich der Gradient Descent Algorithmus cite:Curry_1944 die von dem Backpropagation berechneten Gradienten nutzt, um
die Parameter des Netzes so zu manipulieren, dass der Loss[fn:loss] minimiert wird.

*** Strukturelle Beschaffenheit von neuronalen Netzen <<netstruct>>
#+begin_quote
In diesem Kapitel wird ausschließlich der Aufbau eines fully connected[fn:fullyconnected] feed forward[fn:feedforward] neural networks behandelt.
#+end_quote
Ein \ac{ANN} besteht primär aus mehreren Schichten (Layern) $L_1, \dots, L_n$.
Bei dem ersten Layer $L_1$ handelt es sich um den sogenannten Input Layer, der für die Aufnahme von Eingabedaten zuständig ist.
Analog fungiert dieser als Eingabe-Interface des neuronalen Netzes für den Anwender des \acp{ANN}.
So erwartet ein Input-Layer mit $8$ Neuronen einen Input Vektor von $8$ Werten.
Schichten $L_2, \dots, L_{n-1}$ werden als Hidden-Layer des neuronalen Netzes bezeichnet, da diese für den Außenstehenden nicht direkt einsichtig sind.
Die letzte Schicht des \acp{ANN}, $L_n$ ist der Output-Layer des Netztes und dient als zweites Interface für den Nutzer.
In Abbildung ref:fig:network lässt sich die Architektur eines einfachen Feedforward[fn:feedforward] Networks und dessen Layer klar erkennen.
cite:10.5555/3086952

*** Das Perceptron <<perceptron>>
Ein Perceptron ist ein Modell, das eine Reihe von Eingabedaten (Inputs) $a$ auf einen gemeinsamen Output $y$ nach der Form  $\mathbb{R}^n \rightarrow \mathbb{R}$ abbildet.
Die verschiedenen Inputs $a_n$ werden durch Gewichtungen $w_n$ verschieden stark gewichtet, also $a_1 \times w_1 + \dots + a_n \times w_n = y$ oder in Vektorschreibweise $a \times w^T = y$.
Auf dem heutigen Stand der Entwicklung des Deep-Learning-Frameworks enthält das Perceptron zusätzlich noch eine nicht-lineare Komponente in der Form einer Aktivierungsfunktion $\sigma$.
Einige der häufig eingesetzten Aktivierungsfunktionen können in Tabelle ref:tab:aktivierungsfunktion gefunden werden.
In Kapitel ref:activations dieser Arbeit, wird noch im Genaueren auf Zusammenhänge zwischen Aktivierungsfunktion und Sparsity[fn:sparsity] eines \ac{ANN} Modells eingegangen.
cite:Rosenblatt_1958

#+LABEL: tab:aktivierungsfunktion
#+CAPTION[Populäre Aktivierungsfunktionen]: Aktivierungsfunktionen enthalten meist eine Nichtlinearität, die nötig ist, um neuronale Netze
#+CAPTION: auf nicht lineare Zusammenhänge in Datensätzen trainieren zu können.
| Name                  | Funktion                                                        |
|-----------------------+-----------------------------------------------------------------|
| Logistische Funktion  | $\frac{1}{1+e^t}$                                               |
| Tangens Hyperbolicus  | $\frac{(e^x-e^{-x})}{(e^x+e^{-x})}$                             |
| Rectified Linear Unit | $f(x)= \begin{cases} 0\ for\ x\leq0 \\ x\ for\ x>0 \end{cases}$ |

Ein Perceptron kann, statt in einem grafischen Modell visualisiert zu werden, auch als eine mathematische Funktion eqref:eq:percept behandelt werden.

\begin{equation}f(a, w)=\sigma(a\times w^T)=y \label{eq:percept}\end{equation}


#+LABEL: fig:perceptron
#+CAPTION[Diagramm eines einfachen Perceptrons]: Abbildung eines einfachen Perceptrons.
#+CAPTION: Es ist gut zu erkennen, wie der Input Vektor des Layers $\begin{pmatrix} x_1 \\ \dots \\ x_m \end{pmatrix}$ und
#+CAPTION: der Weight Vektor $\begin{pmatrix}w_1 \\ \dots \\ w_m \end{pmatrix}$
#+CAPTION: auf die Variable $y$ durch $\begin{pmatrix} x_1 \\ \dots \\ x_m \end{pmatrix} \begin{pmatrix} w_1 \\ \dots \\ w_m \end{pmatrix}^T = y$ abgebildet werden.
#+CAPTION: Der Output des Layers wird durch die Anwendung einer Aktivierungsfunktion auf die Variable $y$ generiert.
#+CAPTION: cite:towardsdatascience
[[./resources/perceptron.png]]

*** Der Datenfluss in einem künstlichen neuronalen Netz
Daten in einem Feed-Forward \ac{ANN} verlaufen immer linear von Input-Layer in Richtung Output-Layer.
Da bereits in Kapitel ref:perceptron auf die Beschaffenheit eines Layers eingegangen wurde, können wir einen Layer $L$ als eine Funktion $f(x)$ betrachten.
Da der jeweilige Output eines Layers $L_i$ als der Input des Layers $L_{i+1}$ dient, können wir ein Netzwerk als eine Verkettung an Funktionen betrachten.
Im Allgemeinen kann dies in der Form eqref:eq:net ausgedrückt werden.

\begin{equation} {f_n(\dots (f_1(x)))=y \ \ \ \label{eq:net} \end{equation}

#+BEGIN_QUOTE
In anderen Layer-Architekturen wie Recurrent cite:Rumelhart_1986 oder LSTM cite:Hochreiter_1997 ist es durchaus möglich Daten auch an vorherige Layer abzugeben.
Diese Architekturen sind jedoch nicht Teil dieser Arbeit.
#+END_QUOTE
Die Möglichkeit ein \ac{ANN} als eine Verkettung von Funktionen formulieren zu können ist essentiell um Algorithmen wie Backpropagation zur
Differenzierung von Parametern nutzen zu können.
** Pruning

*** Einführung in naive Pruning Methoden für künstliche neuronale Netze

Durch die Beobachtung der künstlichen neuronalen Netze in den letzten Jahren lässt sich feststellen,
dass die Komplexität und die damit einhergehende Anzahl von Neuronen und deren Verbindungen immer weiter zunehmen cite:altenberger18:_non_techn_survey_deep_convol.
Gleichzeitig werden diese komplexeren und größeren \ac{ANN} Architekturen auch auf schwächeren eingebetteten Geräten eingesetzt.
\ac{ANN} Modelle die für selbstfahrende Autos eingesetzt werden, sollten beispielsweise eine möglichst schnelle und effiziente Ausführung bieten.
Durch diese schnellere Ausführung kann rascher auf Gefahrensituationen reagiert werden.
Deshalb werden Optimierungen an neuronalen Netzen immer relevanter, da dies Inferenz-Zeit und Modellgröße minimieren kann.
Verfahren wie Quantisierung können die Laufzeit und den Speicherverbrauch von \ac{ANN}'s deutlich verbessern;
jedoch lassen sich auch durch Pruning-Verfahren massive Verbesserungen bezüglich Laufzeit und Speicherverbrauch erzielen cite:Frankle2018.
Pruning-Verfahren versuchen durch das Entfernen von Verbindungen oder auch ganzen Neuronen, die Sparsity eines Modells zu erhöhen.
Als Weight- oder auch Connection-Pruning wird ein Vorgang bezeichnet, der Verbindungen aus einem \ac{ANN} entfernt.
Dabei werden die Verbindungen eliminiert, also mit $0$ gewichtet. Dies ist in Abbildung ref:fig:naiveweightpruning dargestellt.
Die ausgewählten Verbindungen oder Neuronen werden durch eine Heuristik bestimmt. Eine Heuristik könnte beispielsweise die niedrigst gewichteten Verbindungen sein.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import sys
sys.path.append('./condense')
import condense
import matplotlib.pyplot as plt

model = keras.models.load_model('./resources/models/iris.h5')
layer = 1

plt.figure(figsize=(10, 3))

plt.subplot(121)
plt.imshow(abs(model.get_weights()[layer*2]), cmap='inferno', vmax=0.4)
plt.title('Ungeprunter Kernel eines Layers')
model.build()
pruned = [condense.optimizer.layer_operations.weight_prune.w_prune_layer(weight, (perc := 0.85)) for weight in model.get_weights()]
model.set_weights(pruned)
plt.subplot(122)
plt.imshow(abs(model.get_weights()[layer*2]), cmap='inferno', vmax=0.4)
plt.title(f'Kernel mit {perc*100}% sparsity')
plt.tight_layout()
plt.savefig('./resources/plots/simple-pruning.png')
return './resources/plots/simple-pruning.png'
#+END_SRC

#+LABEL: fig:naiveweightpruning
#+CAPTION[Visualisierung von Weight Pruning]:
#+CAPTION: In diesem hier dargestellten Dense Layer eines neuronalen Netzes, wurde die Sparsity des Modells durch Pruning der Verbindungen auf $85\%$ erhöht.
#+CAPTION: Jeder Pixel repräsentiert dabei eine Verbindung von Neuron zu Neuron in einem \ac{ANN}.
#+CAPTION: Es ist gut zu sehen, wie nur leicht gewichtete Verbindungen durch Pruning deaktiviert werden (hier durch schwarze Pixel zu erkennen).
#+CAPTION: Dabei schadet diese Operation meist der Accuracy des Modells.
#+CAPTION: Bei dem Netz handelt es sich um ein durch TensorFlow 2.0 cite:Bisong_2019 trainiertes Modell. Bei dem Training wurde der Iris Datensatz genutzt. cite:fisher36lda
#+RESULTS[1a5c84b8f9bca2339b02bfada60a980efb193c0b]:
[[file:./resources/plots/simple-pruning.png]]

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import sys
sys.path.append('./condense')
import condense
import matplotlib.pyplot as plt

model = keras.models.load_model('./resources/models/iris.h5')
layer = 1

xl = 'Neuron Gewichtung'
yl = 'Anzahl'

plt.figure(figsize=(10, 3))
model.build()
plt.subplot(121)
X = model.get_weights()[layer*2].flatten()
X = X[X != 0]
plt.hist(X, density=True, rwidth=.5, bins=70)
plt.xlabel(xl)
plt.ylabel(yl)
plt.title('Ungeprunter Kernel eines Layers')
pruned = [condense.optimizer.layer_operations.weight_prune.w_prune_layer(weight, (perc := 0.85)) for weight in model.get_weights()]
model.set_weights(pruned)
plt.subplot(122)
X = model.get_weights()[layer*2].flatten()
X = X[X != 0]
plt.hist(X, rwidth=.5, bins=70)
plt.title(f'Kernel mit {perc * 100}% sparsity')
plt.xlabel(xl)
plt.ylabel(yl)
plt.tight_layout()
plt.savefig('./resources/plots/simple-pruning-hist.png')
return './resources/plots/simple-pruning-hist.png'
#+END_SRC

#+CAPTION[Histogramm der Weight Verteilung nach Pruning]: Nach dem in ref:fig:naiveweightpruning gezeigten Weight Pruning,
#+CAPTION: verändert sich die Verteilung der Gewichtswerte. Dabei werden die Gewichtungen im Bereich $[-x;x]$ mit $x$ als Threshold eliminiert.
#+CAPTION: Hier wird $x$ so gewählt, dass $\sim 85\%$ der Verbindungen innerhalb des Netzes eliminiert werden, um eine dementsprechende Model-Sparsity zu erreichen.
#+RESULTS[6a53703a855e51ff1154db65792a935831dd2b47]:
[[file:./resources/plots/simple-pruning-hist.png]]

*** Heuristiken zur Bestimmung von zu prunenden Parametern
Es gibt verschiedenste Heuristiken um die zu prunenden Parameter[fn:parameter] eines Modells zu wählen.
Die wohl am nächstliegendsten sind Unit- und Weight-Pruning.
Die Weight-Pruning Heuristik wählt jeweils die am niedrigst gewichtetsten Gewichtungen eines Layers oder des ganzen Modells aus.
In Abbildung ref:fig:naiveweightpruning ist der in der Grafik dargestellte Layer Kernel durch einfaches Weight-Pruning auf $85\%$ Sparsity optimiert worden.
Die Auswahl der Verbindungen (veranschaulicht durch jeweils ein Pixel) folgt dabei keinem spezifischen Muster, da sich die Heuristik ausschließlich
auf die absolute Gewichtung jeder Verbindung bezieht.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import sys
import matplotlib.pyplot as plt
sys.path.append('./condense')
from condense.optimizer.layer_operations.unit_prune import u_prune_layer

layer= 1
model = keras.models.load_model('./resources/models/iris.h5')
unit_pruned = u_prune_layer(model.get_weights()[0::2][layer], (t := .4))
plt.figure(figsize=(10, 3))
plt.subplot(121)
plt.title('Weights des zu prunenden Layers')
plt.imshow(abs(model.get_weights()[0::2][layer]), cmap='inferno', vmin=0)
plt.subplot(122)
plt.title(f'Weights des geprunten Layers ({t*100}% Sparsity)')
plt.imshow(abs(unit_pruned), cmap='inferno', vmin=0)
plt.tight_layout()
plt.savefig('./resources/plots/iris-unit-pruning.png')
return './resources/plots/iris-unit-pruning.png'
#+END_SRC

#+LABEL: fig:naiveunitpruning
#+CAPTION[Visualisierung von Unit Pruning]: Als Vergleich zum in Abbildung ref:fig:naiveweightpruning gezeigten Weight-Pruning eine Visualisierung von Unit(Neuron)-Pruning.
#+CAPTION: Wie auch in Abbildung ref:fig:naiveweightpruning entspricht jeder Pixel einer Verbindung in einem neuronalen Netz.
#+CAPTION: Vertikale angeordnete Pixel entsprechen dem Gewichtungs-Vektor eines einzelnen Neuronen.
#+RESULTS[e7ab1be8e0a42dd70c420d41d96c2a07c8f51b4c]:
[[file:./resources/plots/iris-unit-pruning.png]]

Etwas anders funktioniert Neuron/Unit-Pruning.
Dabei wird nicht jede Gewichtung einzeln betrachtet, sondern die Summe aller Gewichtungen $w$ eines Neurons $V$ also
$$
\text{Gewichtung eines Neurons $V$}=\sum_{n=0}^V\ w_n.
$$
Die am niedrigst gewichteten Gewichtungen $p\%$ aller Neuronen in einer Schicht werden daraufhin aus dem Modell eliminiert. D.h. alle Gewichtungen der Neuronen werden auf $0$ gesetzt.
Dies lässt sich in Abbildung ref:fig:naiveunitpruning durch die schwarzen (eliminierten) vertikalen Linien in der Gewichtungs-Matrix gut erkennen.
Unit-Pruning bietet sowohl Vorteile als auch Nachteile im Vergleich zu Weight-Pruning.
Zum einen werden eventuell wichtige Verbindungen für das Modell durch Kollateralschaden eliminiert.
Als Szenario wäre zum Beispiel denkbar, dass ein Neuron alle Input Variablen als schwach gewichtet sieht, jedoch auf gerade einen oder zwei sehr sensibel reagiert.
Doch eventuell kann gerade diese Reaktion auf diesen Input sehr wichtig für nachfolgende Schichten sein.
Durch Unit-Pruning würde dieses Neuron vermutlich aus dem Modell gestrichen werden, da die Summe aller Gewichtungen in diesem Neuron nicht sehr signifikant sein wird.
Weight-Pruning hingegen berücksichtigt diese eine sehr aktive Verbindung und wird auch nur tatsächlich schwache Gewichtungen streichen.
Es stellt sich natürlich die Frage, ob es überhaupt wünschenswert ist, Neuronen, die so sensibel auf bestimmte Inputs reagieren, in deinem \ac{ANN} zu behalten.

Einen enormen Vorteil, den Unit-Pruning mit sich bringt, ist die einfache Nutzung der Sparsity (Genaueres zu diesem Thema in Kapitel ref:kapit)
die durch Unit-Pruning erzeugt wird. So ist es recht einfach möglich das Neuron nicht nur in der Weight-Matrix mit $0$ zu füllen, sondern dieses komplett
aus der Matrix (oder Tensor) zu entfernen. So wird die Weight-Matrix einer Schicht mit $12$ Neuronen und der Dimensionen $(12,6)$ (also $72$ Parameter)
auf $(12, 5)$ (also $60$ Parameter) reduziert.
Auf diese Weise lässt sich durch die Entfernung eines Neurons direkt ca. $16\%$ Speicher sparen.
Die Sparsity die durch Weight-Pruning erzeugt wird, lässt sich sehr schwer nutzen, da es nicht möglich ist aus Matrizen oder Tensoren nur einzelne Werte zu eliminieren.
Jede Dimension muss immer die gleiche Anzahl an Elementen enthalten.

*** Verlust von Accuracy durch Pruning
Natürlich ist in den meisten Fällen ein Verlust von Genauigkeit des Netzes zu erwarten, wie in Abbildung ref:fig:naive-pruning-loss zu sehen ist.
Jedoch kann durch empirisches Beobachten festgestellt werden, dass Pruning Eigenschaften eines Regularizers aufweist.
So verschlechtert sich meist der Training-Loss[fn:training_loss] jedoch verbessert sich im Gegensatz dazu der Evaluation-Loss[fn:evaluation_loss] meist erheblich cite:Frankle2018 (siehe Zitat ref:quo:regular).

#+LABEL: quo:regular
#+begin_quote
"Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error.
These strategies are known collectively as regularization." cite:10.5555/3086952

/- Ian Goodfellow, Yoshua Bengio and Aaron Courville/
#+end_quote

Jedoch werden im weiteren Verlauf dieser Arbeit auch Methoden zur Kompensierung dieses Effekts behandelt.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import numpy as np
import sys
sys.path.append('condense')
from scripts import calculate_model_sparsity
from condense.optimizer.layer_operations.weight_prune import w_prune_layer
from copy import deepcopy
from random import choice

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
dataset = list(tfds.as_numpy(ds))
train = dataset[:int(len(dataset)*0.3)]
test = dataset[int(len(dataset)*0.7):]

def generator(batch_size, dataset):
    while True:
        X, y = [], []
        for _ in range(batch_size):
            _X, _y = choice(dataset)
            X.append(_X)
            y.append(_y)
        X, y = np.array(X), np.array(y)
        yield X.reshape(batch_size,4), keras.utils.to_categorical(y, 3).reshape(batch_size,3)

gen = generator(100, train)
eva = generator(100, test)

x = []
data = []
model = keras.models.load_model('./resources/models/iris.h5')
baseline = model.evaluate(eva, steps=10)
original_weights = deepcopy(model.get_weights())

for i in (X := np.linspace(0.001, 0.95, 25)):
    model.set_weights(original_weights)
    pruned_weights = [w_prune_layer(layer, i) for layer in model.get_weights()]
    pruned_weights[-1] = model.get_weights()[-1]
    model.set_weights(pruned_weights)
    x.append(calculate_model_sparsity(model.get_weights()))
    data.append(model.evaluate(eva, steps=10))

plt.subplots(figsize=(8, 3))
plt.plot(X*100, np.array(data), label="Eval Loss")
plt.plot(X*100, [baseline]*len(X), label="Base Eval Loss", ls='--')
plt.legend()
plt.xlabel('Model Sparsity in %')
plt.ylabel('Model Loss')
plt.grid()
plt.tight_layout()
plt.savefig('./resources/plots/iris-accuracy.png')
return './resources/plots/iris-accuracy.png'
#+END_SRC

#+LABEL: fig:naive-pruning-loss
#+CAPTION[Verfall von Genauigkeit mit zunehmend aggressiverem Pruning]:
#+CAPTION: In dieser Grafik wird das identische Modell aus Abbildung ref:fig:naiveweightpruning und ref:fig:naiveunitpruning durch immer aggressiveres Weight-Pruning optimiert.
#+CAPTION: Wie sich beobachten lässt, verschlechtert sich die Genauigkeit mit zunehmender Stärke des Prunings rapide.
#+CAPTION: Dabei beschreibt ~Base Eval Loss~ die Accuracy, die das ungeprunte Modell auf dem Evaluation Datensatz durch Training erreichen konnte.
#+RESULTS[bb55e758208f479dee958588a0c36d666847c00e]:
[[file:./resources/plots/iris-accuracy.png]]

** Industrierelevanz
Pruning von künstlichen neuronalen Netzen bietet vielen Unternehmen die Möglichkeit, Optimierungen an schon bestehenden \ac{ANN} Modellen vorzunehmen.
Diese Optimierungen ermöglichen es unter Umständen, komplexere Modelle auf schwächeren Computern zu nutzen.
Beispielsweise können eingebettete Geräte dabei effizienter Daten durch neuronale Netze auswerten.
Besonders in Situationen in denen das Modell möglichst schnell eine Prognose abgeben soll, wie beispielsweise bei Teilen von
selbständig fahrenden Autos, bietet Pruning Chancen auf außerordentliche Verbesserungen.
Zudem bietet Pruning eine Möglichkeit, \ac{ANN}-Modelle ohne signifikante Einbußen von Genauigkeit zu optimieren. cite:Frankle2018
Dies sollte Pruning Methoden auf deutlich mehr \ac{ANN}-Modellen einsetzbar machen.

** Ziel dieser Arbeit <<ziel>>
*** Erstellung eines Pruning Frameworks <<ziel_framework>>
Ziel dieser Arbeit ist es, primär ein Python Framework zu entwickeln, das mehrere verschiedene Typen von Pruning Methoden implementieren soll.
Ein wichtiger Fokus liegt dabei auf der Architektur des Frameworks.
Diese sollte in Zukunft möglichst einfach erweitert werden können.
Die Dokumentation der verschiedenen Module ist aus diesem Grund sehr wichtig und sollte im Laufe der Arbeit auch immer aktualisiert werden.
Bei dem Design der Nutzer-Schnittstellen sollte auf eine möglichst einfache und saubere Architektur geachtet werden,
da auch Nutzer ohne ausreichende Erfahrung mit Python im Stande sein sollten, die Tools dieses Frameworks zu nutzen.

#+begin_src mermaid :file resources/plots/pruning-framework.png :theme forest :background transparent
graph LR
    input(Input Model) --> interface(High Level Interface)
    interface --> parser(Model Parser)
    pruning(Pruning Engine) --> output(Pruned Model)
    parser --> pruning
#+end_src

#+LABEL: fig:rough-project-structure
#+CAPTION[Pruning Framework Konzept]: Der hier gezeigte Graph soll das grobe Konzept, des im Laufe dieser Arbeit entstehenden Pruning Frameworks zeigen.
#+RESULTS:
[[file:resources/plots/pruning-framework.png]]

Zudem sollte das Framework kompatibel mit aktueller Deep-Learning Software sein.
Kompatibilität mit \ac{TF} [fn:tensorflow]/Keras[fn:keras] steht bei diesem Projekt im Vordergrund, da auch intern \ac{TF} für Trainings-Operationen genutzt wird.
Optional sollte auch die Möglichkeit bestehen, ein Modell in dem ONNX[fn:onnx] Format zu exportieren, um auch Kompatibilität mit anderen Frameworks sicherzustellen.

*** Erkenntnisse über Pruning Methoden
Ein weiterer wichtiger Aspekt, ist die Forschung an diversen Pruning Methoden und die daraus entstehenden Erkenntnisse formell festzuhalten.
Alle Resultate sollten klar nachvollziehbar und durch das Lesen erkenntlich sein.
Um Lesern die Resultate dieser Arbeit möglichst nachvollziehbar zu gestalten, werden alle nötigen Dateien in Form eines
GitHub Repositories[fn:github] veröffentlicht.
Der Quellcode um alle Grafiken/Diagramme und Resultate dieser Arbeit zu erzeugen ist dort zu finden.

* Methodik
** Erstellung des Frameworks
Wie bereits in Kapitel ref:ziel_framework erwähnt wurde, sollte bei der Erstellung des Frameworks ein
großer Fokus auf die zukünftige Erweiterbarkeit liegen.
Aus diesem Grund wird besonders auf die Architektur, die Tests und die Dokumentation sehr viel Wert gelegt;
außerdem ist das gesamte Projekt als öffentliches GitHub Repository angelegt.
Demzufolge handelt es sich bei dem Framework auch um ein Open Source Projekt.
Damit kann, falls gewünscht, in der Zukunft eine Weiterentwicklung des Projekts ermöglicht werden.

*** Wahl der Sprache & Frameworks
Als primäre Programmiersprache bot sich Python an, da diese sehr weit in der KI/ML Gemeinschaft verbreitet ist.
So basieren die meisten Frameworks für neuronale Netze wie Theano, Tensorflow oder Torch auf Python oder einer Implementation in C/C++
die mit der Hilfe von der CPython Bridge angesprochen werden kann.
Damit ist eine sehr gute Leistung trotz einfacher API's erreichbar.
Zudem können in der weiteren Entwicklung dieses Projekt gewisse Module auf eine performantere Sprache wie C/C++ ausgelagert werden.
Jedoch sind die meisten rechenaufwendigen Opertionen wie Matrix-Operationen bereits in Frameworks wie ~numpy~[fn:numpy] implementiert.

Eine noch effizientere Methode, die sich besonders bei neuronalen Netzen als eine enorme Leistungssteigerung erwiesen hat,
ist die Nutzung von \acp{GPU} oder \acp{TPU} um einen hohen Grad von Parallelisierung erreichen zu können.
Das Framework ~TensorFlow~[fn:tensorflow] stellt Implementationen von auf \ac{GPU} durchführbaren mathematischen Operationen
in Form eines Python Interfaces bereit.
Besonders Matrix Operationen lassen sich meist gut parallelisieren, beziehungsweise durch spezialisierte Hardware besonders effizient berechnen.
Dies resultiert in einer sehr effiziente Optimierung des \ac{ANN} Modells.

*** Architektur
Der Anspruch der Architektur besteht darin, sie für den Nutzer so nachvollziehbar wie möglich zu gestalten.
Es soll einfach sein, Änderungen in Form von kollaborativen Programmieren im weiteren Verlauf des Projekts vorzunehmen.

*** Dokumentation

**** Allgemeine Dokumentation des Projekts

Durch GitHub Pages[fn:pages] und dem Tool Docsify[fn:docsify] ist es sehr einfach möglich, eine ausgesprochen zugängliche Dokumentation
bzw. Landing Page für das Projekt zu generieren.
Der Inhalt dieser Dokumentation ist manuell erstellt und soll dem Benutzer nur einen groben Überblick über die wichtigsten Aspekte des Frameworks geben.
Detailliertere Informationen zu internen Schnittstellen können jedoch trotzdem sehr einfach über die Modul Dokumentation aus Unterpunkt ref:pdoc eingesehen werden.

**** Automatisierte Generierung von Dokumentation aus Source Code des Projektes <<pdoc>>

Durch das Tool pdoc3[fn:pdoc] kann aus dem Source Code eines Python Modules und dessen Docstrings[fn:docstring] eine Dokumentation in Form einer
HTML Seite generiert werden.
Diese ist direkt in die allgemeine Dokumentation des Projekts eingebettet und erfordert keine separate Website.
Da bei der Generierung dieser Dokumentation keine weitere manuelle Arbeit geleistet werden muss, kann diese ohne weitere Umstände automatisiert
über GitHub Actions[fn:actions] realisiert werden.
So wird beispielsweise bei einer Änderung des Modules auf dem ~master~ Branch des Projekts ein Script ausgelöst,
das eine aktualisierte Dokumentation auf der öffentlichen Webseite zur Verfügung stellt.
Natürlich koaliert die Qualität der generierten Dokumentation direkt mit der Qualität der im Source Code verfassten Docstrings.
Deshalb ist sicherzustellen, dass auch hier ein gewisser Qualitätsstandard eingehalten wird.
Wie dies innerhalb dieses Projekts implementiert wurde, wird in Kapitel ref:tests Punkt ref:docstyle_tests genauer erläutert.

*** Tests <<tests>>

**** Unit Tests

Um sicherzustellen, dass die Qualität der Software den nötigen Standard erfüllt, sind Unit Tests ein essentieller Bestandteil dieses Projekts.
Dazu wurde das sehr weit verbreitete Testing Framework pytest[fn:pytest] genutzt.
Zusätzlich werden Daten über die Test-Coverage der Tests Dank des pytest-cov plugins für pytest generiert.

**** Linting

Um im Laufe des Projekts eine ästhetisch ansprechende Formatierung beizubehalten, wurde ~pylint~ verwendet.
Durch dieses Tool wird die Formatierung des Quellcodes geprüft und durch GitHub Actions automatisiert ausgeführt.

Einige der wichtigsten von der Software überprüften Punkte sind:
- unnötige ~import~ Statements
- korrekte Variablennamen
- Zeichen per Zeile
- Zeilen-Abstände

**** Docstyle Tests <<docstyle_tests>>

Um auch wichtige Teile, wie die Dokumentation von Funktionen, im Laufe des Projekts nicht zu vernachlässigen,
wurde das Tool ~pydocstyle~[fn:pydocstyle] verwendet, um auch Docstrings auf Korrektheit zu überprüfen.
Als Style der Docstrings wurde das von Google gebrauchte Styleguide[fn:styleguide] genutzt.
Durch diese Methodik, müssen alle Module, Klassen und Funktionen über Docstrings verfügen, da sie sonst nicht auf einen der nicht-feature branches des Repositories gepullt werden können.
Dadurch lässt sich eine enorm detaillierte Dokumention aller öffentlichen Schnittstellen automatisiert generieren.

** Datensätze <<datensatz>>
Die meisten, in dieser Arbeit verwendeten Datensätze, wurden durch das Python 3 Modul ~tensorflow_datasets~ cite:TFDS bezogen.
In Tabelle ref:tab:dataset sind alle verwendeten Datensätze gelistet.

#+LABEL: tab:dataset
#+CAPTION[In dieser Arbeit verwendete Datensätze]: Eine Liste von, in dieser Arbeit vewendeten Datensätzen.
| Datensatz                                            | Beschreibung | Quelle                                       |
|------------------------------------------------------+--------------+----------------------------------------------|
| Iris Dataset cite:fisher36lda                        |              | https://archive.ics.uci.edu/ml/datasets/iris |
| ImageNet cite:imagenet_cvpr09                        |              | http://www.image-net.org                     |
| ImageNet V2                                          |              | https://github.com/modestyachts/ImageNetV2   |
| MNIST cite:lecun-gradientbased-learning-applied-1998 |              | http://yann.lecun.com/exdb/mnist/            |

* Implementierung

** Sparsity Mask <<sparsity_mask>>
Der Begriff Sparsity Mask/Tensor bezieht sich in dieser Arbeit auf einen binären Tensor, der definiert, welche Felder aus einem Weights Tenor eine $0$ enthalten.
Mit anderen Worten ausgedrückt, maskiert der Sparsity Tensor die Parameter Tensoren eines Modells.
Die Sparsity Mask ist ein essentieller Bestandteil für fast jede Pruning Methode und bei der Implementierung dreht es sich in erster Linie darum,
diese Maske effizient zu generieren und anzuwenden.
Deshalb ist die Klärung dieses Begriffs auch sehr wichtig für die folgenden Kapitel dieser Arbeit.
Auswirkungen der Sparsity Mask auf ein Array wird in Abbildung ref:fig:simplesparsity dargestellt.

#+BEGIN_SRC python :exports results :results file :cache yes
import numpy as np
import matplotlib.pyplot as plt

a = np.random.rand(10, 20)
m = np.random.rand(10, 20) < 0.4

plt.figure(figsize=(10, 2))
plt.subplot(131)
plt.imshow(a)
plt.title('Ursprüngliches Array $a$')
plt.subplot(132)
plt.imshow(m*-1, cmap='binary')
plt.title('Sparsity Mask $m$')
plt.subplot(133)
plt.imshow(a*m)
plt.title('Maske auf Array angewandt $a \\times m$')
plt.tight_layout()
plt.savefig('resources/plots/masking.png')
return 'resources/plots/masking.png'
#+END_SRC

#+LABEL: fig:simplesparsity
#+CAPTION[Anwendung von einer Sparsity Mask auf ein einfaches Array]:
#+CAPTION: In dieser Grafik wird jedes Feld durch weiß ($1$) und schwarz ($0$) binär visualisiert.
#+CAPTION: Die Multiplikation der Maske $m$ mit dem Array $a$ resultiert in der durch $m$ maskierte Version des ursprünglichen Arrays $a$.
#+RESULTS[b489619f2ae1bf26670eaaa816a599d772d79015]:
[[file:resources/plots/masking.png]]

Bei der Implementierung der Pruning Operation bieten sich primär zwei Zeitpunkte während des Trainings an,
wobei die Sparsity Maske auf die Modell-Parameter angewendet werden kann.
1. Nach jedem Schritt des Optimizers werden die maskierten Felder wieder auf $0$ zurückgesetzt.
2. Die Parameter werden zur Initialisierung einmal maskiert.
   Daraufhin werden ausschließlich die Gradienten maskiert, um somit die Parameter Sparsity beizubehalten.

~condense~ verwendet beide dieser Ansätze, um die gewünschte Sparsity eines Modells zu erzielen.

** ~condense~ Modul
Wie in Kapitel ref:ziel erwähnt, ist auch die Erstellung eines Python Frameworks ein großer Teil dieser Arbeit.
Das englische Wort /condense/ (zu Deutsch /kondensieren/ oder auch /verdichten/) beschreibt die Operation des Prunings von neuronalen Netzen sehr gut,
da Teile des Netzes gelöscht, bzw. verdichtet werden.

*** Struktur
Eine klare und einfach verständliche Strukturierung der Schnittstellen ist (wie in Kapitel ref:ziel_framework beschrieben)
ein wichtiges Ziel dieser Arbeit.
Die grobe Struktur des ~condense~ Moduls wird in Abbildung ref:fig:condense_structure dargestellt.
In den folgenden Kapiteln werden die einzelnen Untermodule dieses Projekts genauer erläutert.

#+BEGIN_SRC mermaid :file resources/plots/condense-module.png :theme forest :background transparent :cache yes
graph TD
    condense --- keras
    condense --- torch
    condense --- optimizer
    condense --- utils
    condense -.- compressor
    condense --- o_shot(one_shot)
    keras --- prune_model(prune_model)
    utils --- model_utils
    utils --- layer_utils
    optimizer --- one_shot(one_shot)
#+END_SRC

#+LABEL: fig:condense_structure
#+CAPTION[Condense Modul Architektur]: Diagram des Python Moduls ~condense~ und dessen Untermodule.
#+CAPTION: Funktionen/Methoden sind hierbei mit abgerundeten Kanten dargestellt und Module/Klassen mit scharfen Kanten.
#+RESULTS[3335312817c5cdb4cb1fc617cff527a171587a7a]:
[[file:resources/plots/condense-module.png]]

**** Keras Kompatibilitäts-Modul (~condense.keras~)

Um eine direkte Integration in das Keras Machine Learning Framework bieten zu können, existiert im Umfang dieses Moduls ein Keras/\ac{TF} kompatibles Sub-Modul.
Dieses nutzt, im Vergleich zum im Punkt ref:optimization_module beschriebenen Optimierungs-Modul, keine ~numpy~[fn:numpy] Arrays, sondern \ac{TF} Tensoren.
Die Implementation von Pruning Methoden als TensorFlow Tensoren bietet die Möglichkeit, diese auf \acp{GPU} ausführen zu können,
was einen enormen Leistungszuwachs mit sich bringt.
Jedoch verursacht die Arbeit mit \ac{TF} Tensoren auch deutlich mehr Aufwand bei der Implementation.
In Kapitel ref:keras_module wird noch genauer auf die Methodik hinter diesem Modul eingegangen.

**** Torch (PyTorch) Kompatibilitäts-Modul (~condense.torch~)

Das zweite von ~condense~ unterstützte Framework ist PyTorch (o.a. Torch).
Ähnlich wie das Keras Kompatibilitäts-Modul, werden Network Operationen dadurch auf GPU's oder TPU's durchgeführt.
In Kapitel ref:pytorch wird noch näher auf die PyTorch spezifische Implementation eingegangen.

**** Optimierungs-Modul (~condense.optimizer~) <<optimization_module>>

Dieses Modul stellt Pruning Implementationen für ~numpy~[fn:numpy] Arrays bereit.
Bei der Implementierung wurde besonders auf die Utilisation von ~numpy~ Methoden Wert gelegt, um bestmögliche Leistung zu erreichen.
Teilweise werden auch sequenzielle ~keras~ Modelle von der API unterstützt,
wobei für ausführlichere Informationen auf die Dokumentation verwiesen werden sollte: https://sirbubbls.github.io/condense/#/pdoc/condense/optimizer/index.html.

**** Utils (~condense.utils~)

Dieses Submodul soll Nutzern eine handvoll nützlicher Tools bereitstellen; beispielsweise die Berechnung der Sparsity eines Layers.

**** Modul zur Kompression von optimierten Modellen (~condense.compressor~)

Um Modelle in einer effizienten Form zu speichern und in den Arbeitsspeicher laden zu können, müssen diese auch komprimiert werden.
In Zukunft soll dies in Form dieses Sub-Modules realisiert werden, jedoch ist dies nicht Ziel dieser Arbeit.
*** Installation
Natürlich ist es enorm wichtig, wie potentielle Nutzer das ~condense~ Modul installieren können.
Da das Projekt unter einem öffentlichen GitHub Repository[fn:gh_condense] entwickelt wird, können sich Nutzer durch einfaches Herunterladen des
Repositories, Zugang zu Implementationen verschaffen.
Durch das Python Modul ~pip~[fn:pip], ein Module Management System für Python, ist es einfach möglich, Module aus dem öffentlichen PyPi[fn:pypi]
Repository herunterzuladen und zu installieren.
Module und deren Versionen müssen jeweils bei einer neuen Version neu eingereicht werden.
Dies ist durch GitHub Actions Framework automatisierbar, indem durch Änderungen des Master Branches eine Veröffentlichung
ausgelöst wird, vorausgesetzt, es werden alle Tests des Moduls erfolgreich ausgeführt.

#+begin_quote
Eine Installation durch ~pip~ ist durch das Kommando: ~pip install condense~ möglich.
#+end_quote

** One-Shot Pruning <<one-shot>>
Die wohl trivialste Methode, um Parameter eines \ac{ANN} Modells zu prunen, ist ein sogenanntes One-Shot Pruning Verfahren anzuwenden.
Bei diesem Typ von Pruning werden keine Refitting (Kapitel ref:refitting) Operationen angewandt; also auch keine Datensätze benötigt.
Nachteil ist jedoch eine deutlich verlustbehaftete Optimierung im Gegensatz zu iterativen Pruning (Kapitel ref:iterative).
Es wird eine Maskierungs-Funktion benötigt, um die zu prunenden Felder der Matrix zu ermitteln.
Eine einfache Maskierungs-Funktion wäre beispielsweise die Auswahl aller Felder, die unter einer festgelegten Threshold $t$ liegen.
Die durch diese Funktion resultierende Matrix $S$ wird auch Sparsity-Mask (siehe Kapitel ref:sparsity_mask) genannt.
$$
W = \begin{pmatrix} 0.4 && 2 \\ 1.4 && 0\end{pmatrix} \text{ mit } t = 1.0 \Rightarrow S = \begin{pmatrix} 0 && 1 \\ 1 && 0 \end{pmatrix}
$$
In Abbildung ref:fig:oneshot ist ein derartiges triviales Pruning von Feldern aus einer Matrix in visueller Form dargestellt.

#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
w = np.random.random((50,100))
t = 0.5
plt.figure(figsize=(15, 3))
plt.subplot(1, 4, 1)
plt.imshow(w, vmin=0.0, vmax=.4)
plt.title('Source Matrix')

plt.subplot(1, 4, 2)
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=.4)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.subplot(1, 4, 3)
t += .4
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=.4)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.subplot(1, 4, 4)
t += .4
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=.4)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.tight_layout()
plt.savefig('resources/plots/one-shot-random.png')
return 'resources/plots/one-shot-random.png'
#+END_SRC

#+LABEL: fig:oneshot
#+CAPTION[Visualisierung von One Shot Pruning]: Visualisierung von One Shot Pruning mit verschiedenen Thresholds $t$.
#+CAPTION: Die das Pruning durchführende Operation ~w[w < t] = 0~ setzt alle sich in der Maske ~w < t~ befindenden Felder der Matrix auf $0$.
#+CAPTION: Die Maske ~w < t~ kann durch eine beliebige (Maskierungs-) Funktion ersetzt werden.
#+RESULTS[e31988325076586e7bff351029260fa878d4cd85]:
[[file:resources/plots/one-shot-random.png]]

Das Modul ~condense~ bietet dabei mehrere verschiedene Implementationen und Schnittstellen für Nutzer, um Datenstrukturen zu prunen.
Die universellste Methode One-Shot Pruning auf Tensoren durchzuführen, ist die ~condense.optimizer.layer_operations~ API zu nutzen.
Diese erlaubt Pruning auf ~numpy.ndarrays~ durchzuführen.

#+BEGIN_SRC
model = keras.models.load_model('...')
pruned = condense.optimizer.one_shot(model, 0.75)
#+END_SRC

** Iteratives Pruning <<iterative>>
Das in Kapitel ref:one-shot angesprochene One-Shot Pruning-Verfahren, verursacht einen signifikanten Verlust von Modell Accuracy.
Jedoch lässt sich  One-Shot Pruning einfach in ein iteratives Pruning-Verfahren umwandeln.

*** Refitting <<refitting>>
Nach dem Pruning eines Modells kann dieses erneut trainiert werden.
Somit kann sich das Modell an die manipulierten Parameter adjustieren.
Es ist jedoch zwingend notwendig, die Sparsity Mask (Kapitel ref:sparsity_mask) der jeweiligen Layer zu speichern,
da diese auf die Parameter nach dem Training angewandt werden muss cite:Frankle2018.
Was passiert, wenn dies nicht gemacht wird, zeigt Abbildung ref:fig:refitting.

#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
import sys
import keras
import tensorflow_datasets as tfds
sys.path.append('./condense')
import condense

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
ds = ds.repeat()

model = keras.models.load_model('resources/models/iris.h5')
layer = 2
pruning_intensity = 0.8

plt.figure(1, figsize=(10, 5))
plt.subplot(221)

plt.imshow(model.get_weights()[layer], vmax=.4, vmin=.0)
plt.savefig('resources/plots/iterative-1.png')
plt.title('Weight Matrix eines trainierten Layers \n ohne Pruning Optimierungen')

plt.subplot(222)

plt.imshow((pruned := condense.one_shot(model, pruning_intensity)).get_weights()[layer], vmax=.4, vmin=.0)
plt.savefig('resources/plots/iterative-1.png')
plt.title(f'Weight Matrix des optimierten Layers mit {round(condense.utils.model_utils.calc_model_sparsity(pruned) * 100, 2)}% \n Modell Sparsity')

pre_training = pruned.get_weights()[layer]

# Retraining
plt.subplot(223)

pruned.compile('adam', 'mse')
pruned.fit(ds.batch(30), epochs=15, steps_per_epoch=50)
plt.imshow(pruned.get_weights()[layer], vmin=.0, vmax=.4)
plt.title(f'Optimiertes Modell nach Refitting ({round(condense.utils.model_utils.calc_model_sparsity(pruned) * 100, 2)}% Modell Sparsity)')

# Diff plot
plt.subplot(224)
plt.imshow(np.abs(pre_training - pruned.get_weights()[layer]), cmap='binary')
plt.title('Änderungen der Gewichtungen durch Refitting')

plt.tight_layout()
plt.savefig('resources/plots/iterative-1.png')

return 'resources/plots/iterative-1.png'
#+END_SRC

#+LABEL: fig:refitting
#+CAPTION[Weights eines Layers nach refitting]: In dieser Grafik wird der Layer eines Modells
#+CAPTION: durch naives Weight und Neuron Pruning optimiert und anschließend durch Refitting erneut trainiert.
#+CAPTION: Durch das Refitting ohne Sparsity Maske wird die Sparsity des Modells von ca. $77\%$ auf fast $0\%$ geändert.
#+CAPTION: Änderungen die durch das Refitting verursacht werden, sind in der Abbildung rechts unten dargestellt.
#+RESULTS[585ffcab6c0d18d41ee6c9266eae6f62449f73e8]:
[[file:resources/plots/iterative-1.png]]

** Implementierung der Lottery Ticket Hypothesis <<lottery_ticket>>
Der durch Jonathan Frankle und Michael Carbin publizierte Artikel
"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"cite:Frankle2018,
beschreibt eine Methode, ein bereits gepruntes Modell zu trainieren um ein schnelleres Training zu ermöglichen.

Dabei wird folgende Prozedur vorgeschlagen:
1. Initialisieren und Speichern aller Parameter des Modells
2. Trainieren des Modells
3. Prunen dieses Modells und Speichern der Layer Masken
4. Reinitialisieren des Modells durch die in Schritt 1 gespeicherten Parameter
5. Anwendung der durch das Prunen generierten Sparsity Masken
6. Erneutes Trainieren des Modells unter Berücksichtigung der Sparsity Maske

Durch dieses Verfahren ist es möglich, nicht nur kleinere Modelle schneller und effizienter zu trainieren, sondern auch eine allgemein bessere
Test-Accuracy des Netzes zu erreichen.
Besonders gefördert wird dadurch das Generalisierungspotential des Netzes.

*** Beispiele MNIST <<bsp_mnist>>
In diesem Beispiel werden verschiedene \acp{ANN} auf den ~MNIST~ cite:lecun-gradientbased-learning-applied-1998,TFDS
Datensatz (siehe Kapitel ref:datensatz) trainiert.
Ein Beispiel wie diese Pruning Vorgehensweise durch ~condense~ Methoden realisiert werden kann, ist in Abbildung ref:code:mnist gegeben.
Der Nutzer kann die Klasse ~condense.keras.Trainer(model, target_sparsity)~ verwenden, um automatisiert einen Trainingsprozess zu starten.
Sobald die Klasse initialisiert wurde, muss nur noch die Methode ~.train()~ aufgerufen werden. Dabei können dieselben Funktionsargumente
wie bei der ~keras~ Methode ~.fit()~ übergeben werden.
Die unten dargestellten Graphen in Abbildung ref:fig:lottery-mnist-1 zeigen Training/Testing Loss während des
automatisierten Trainings.
Beide geprunten Modelle wurden dabei mit einer Ziel-Sparsity von $80\%$ trainiert.
Aus den Graphen geht hervor, dass der Testing-Loss des optimierten Modells trotz der hohen Pruning Rate nicht schlechter ausfällt,
als bei demselben noch vollständigen Modell.

#+CAPTION[Lottery Ticket Pruning durch Condense]: Dank der einfachen Nutzerschnittstelle von ~condense~  label:code:mnist
#+CAPTION: ist es sehr einfach möglich automatisiert Pruning auf dem Ziel-Modell auszuführen.
#+BEGIN_SRC
import condense
import tensorflow_datasets as tfds

ds_train, ds_test = tfds.load('mnist', split=['train', 'test'],
                               shuffle_files=True,
                               as_supervised=True)

model = ...
model.compile(keras.optimizers.Adam(learning_rate=0.001),
              keras.losses.SparseCategoricalCrossentropy(from_logits=True))

trainer = condense.keras.Trainer(model, .75)
trainer.train(ds_train.batch(50),
              epochs=20,
              steps_per_epoch=2,
              eval_data=ds_test.batch(50))
#+END_SRC

#+BEGIN_SRC python :exports results :results file :cache yes
import sys
sys.path.append('condense')
import condense
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense
from copy import deepcopy
import tensorflow_datasets as tfds

EPOCHS = 50
SPARSITY = 0.8
ds_train, ds_test = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True)

model = keras.models.Sequential(layers=[
    keras.layers.Flatten(input_shape=(28,28,1)),
    Dense(1024, activation='relu', input_shape=(784,)),
    Dense(512, activation='relu'),
    Dense(256, activation='relu'),
    Dense(50, activation='relu'),
    Dense(10, name='output')
])
model.compile(keras.optimizers.Adam(learning_rate=0.001), keras.losses.SparseCategoricalCrossentropy(from_logits=True))
model.build()

INITIAL_WEIGHTS = deepcopy(model.get_weights())

classical_training = model.fit(ds_train.batch(50), epochs=EPOCHS, steps_per_epoch=2, validation_data=ds_test.batch(50), validation_steps=2)
assert (model.get_weights()[0] != INITIAL_WEIGHTS[0]).any()
model.set_weights(INITIAL_WEIGHTS)
model.compile(keras.optimizers.Adam(learning_rate=0.001), keras.losses.SparseCategoricalCrossentropy(from_logits=True))
assert (model.get_weights()[0] == INITIAL_WEIGHTS[0]).all()

trainer = condense.keras.Trainer(model, 0.75)
hist = trainer.train(ds_train.batch(50).cache(), EPOCHS, steps_per_epoch=2, eval_data=ds_test.batch(50).cache())
plt.figure(figsize=(10, 5))
plt.title(f'MNIST model pruned by {SPARSITY*100}%')
plt.plot(hist.history['val_loss'], label='Training on Sparse Model (Validation Loss)', lw=4)
plt.plot(hist.history['loss'], label='Training on Sparse Model (Training Loss)', ls='--')
plt.plot(classical_training.history['loss'], label='Training on Full Model (Training Loss)', ls='--')
plt.plot(classical_training.history['val_loss'], label='Training on Full Model (Validation Loss)')
plt.plot(trainer.history['ticket_search'].history['loss'], label='Search for winning ticket', ls=':')
plt.grid()
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.axis([0, EPOCHS, 0, 10])
plt.tight_layout()
plt.savefig('resources/plots/lottery-1.png')
return 'resources/plots/lottery-1.png'
#+END_SRC

#+LABEL: fig:lottery-mnist-1
#+CAPTION[Lottery Ticket Hypothesis auf MNIST]:
#+CAPTION: In dieser Grafik wird der Trainingsprozess eines geprunten Modells im Gegensatz zu einem unoptimierten Modell gezeigt.
#+CAPTION: Es sollte sich primär an den rot und blau gekennzeichneten Linien orientiert werden, da diese jeweils dem Evaluation-Loss entsprechen.
#+RESULTS[333f96f9eb909265676ad73c871fd79f451bba41]:
[[file:resources/plots/lottery-1.png]]

*** Vorbehalte
Der Vergleich geprunter und ungeprunter Modelle gestaltet sich jedoch nicht so trivial wie in Kapitel ref:bsp_mnist dargestellt.
Die Reduktion eines Großteils der Parameter wirkt sich extrem auf die Effekte von Hyperparametern[fn:hyperparameter] aus.
Somit ist es für einen aussagekräftigen Vergleich nötig, auch Werte wie Learning Rate vor dem Training anzupassen.
Um einen möglichst gerechten Vergleich zustande zu bringen, muss durch Hyperparameter-Tuning[fn:hyperparameter-tuning]
von beiden zu vergleichenden Modellen jeweils die beste mögliche Wahl von Hyperparametern getroffen werden.

** Keras Kompatibilitäts-Modul <<keras_module>>
Um eine einfache und klare Schnittstelle zu Keras Modellen zu bieten, gibt ~condense~ Nutzern die Möglichkeit,
einzelne Keras Layer an einen Wrapper (~condense.keras.PruningWrapper~) zu übergeben.
Dieser implementiert Schnittstellen für andere Komponenten des Frameworks.
Diese werden genutzt, um verschiedenste Layer-Manipulationen vornehmen zu können.

#+BEGIN_QUOTE
Die Hilfsfunktion ~condense.keras.wrap_layer(model, sparsity_function)~ instanziiert ein Modell und augmentiert alle möglichen Layer durch ~PruningWrapper~.
Für die meisten Use-Cases ist dies der empfohlene Weg, Keras Modelle zu augmentieren/optimieren.
#+END_QUOTE

Dem ~PruningWrapper~ muss zusätzlich eine Sparsity Funktion übergeben werden.
Diese definiert, wie sich das gewünschte Sparsity Ziel im Laufe des Trainings verhalten wird.
~condense~ stellt einige dieser Funktionen zur Verfügung, bietet aber die Möglichkeit, durch die Implementierung
der abstrakten Klasse ~SparsityFunction~ ein anderes Verhalten zu bestimmen.
Beispiele für bereits existierende Funktionen sind:
- ~Constant(t_sparsity)~: Ziel Sparsity bleibt über den kompletten Trainingsprozess konstant
- ~Linear(t_sparsity)~: Ziel Sparsity nimmt mit laufendem Training bis zu dem letztendlichen Wert ~t_sparsity~ zu

In dem unten gezeigten Code-Beispiel wird ein sequentielles Keras Modell mit vier Schichten erzeugt.
Dabei werden Input (Schicht $0$) und Output (Schicht $4$) nicht durch Pruning optimiert.
Die Hidden Layer[fn:hidden](Schicht $2$ und Schicht $3$) werden außerdem  jeweils mit zwei verschiedenen Sparsity Funktionen
optimiert. Schicht $2$ wird mit einer konstanten Ziel Sparsity von ~0.5~ optimiert und Schicht $3$ mit einer über das Training
ansteigenden Ziel Sparsity bis zu ~0.7~.
Bei dem Training (~model.fit(..., callbacks=[PruningCallback()])~) wird durch den ~PruningWrapper~ automatisch eine entsprechende Pruning Operation durchgeführt.

#+BEGIN_EXAMPLE
model = keras.models.Sequential(layers=[
    Dense(20, input_shape=(4,)),
    PruningWrapper(Dense(10), Constant(0.5)),
    PruningWrapper(Dense(40), Linear(0.7)),
    Dense(2)
])
#+END_EXAMPLE

Leider ist es durch die Architektur von Keras/TensorFlow notwendig bei dem Trainings-Funktionsaufruf ~.fit()~ auch den
Callback ~condense.keras.PruningCallback~ zu übergeben.
Dieser ist intern dafür verantwortlich, dass die ~.prune()~-Funktion der jeweiligen ~PruningWrapper~ Instanzen aufgerufen wird.

#+CAPTION[Quellcode Beispiel: Pruning durch Keras Wrapper]:
#+CAPTION: Einfaches Pruning eines Modells ~model~ durch Pruning des ~condense.keras~ Moduls.
#+CAPTION:
#+BEGIN_SRC
import keras
import condense
from condense.keras import wrap_model, PruningCallback
from condense.optimizer.sparsity_functions import Constant

...

model = keras.models.load_model('...')
augmented = wrap_model(model, Linear(0.7))

augmented.fit(generator,
              epochs=10,
              steps_per_epoch=20,
              callbacks=[PruningCallback()])
#+END_SRC

** PyTorch Kompatibilitäts-Modul <<pytorch>>
PyTorch hat in den letzten Jahren deutlich an Popularität gewonnen. Dies liegt neben mehreren anderen Gründen vor allem
an der einfachen Handhabe im Zusammenhang mit Python.
Aufgrund der Architektur lassen sich Pruning Masken in PyTorch signifikant eleganter und einfacher implementieren,
als mit TensorFlow/Keras.
Vor allem im Vergleich zu ~Keras~ gestaltete sich die Implementierung von Pruning Masken für die Parameter eines Modells deutlich einfacher
und genereller.
So lassen sich in PyTorch beliebige Modelle durch ~condense~ pruning Implementationen optimieren.
Selbst die Implementation des Training-Loops ist nicht durch ~condense~ vorgeschrieben.
Wie bei der Implementation für Keras ist es notwendig, sein Modell in einer Wrapper Klasse (~condenes.torch.PruningAgent~) zu packen.

#+BEGIN_SRC
from condense.torch import PruningAgent
from condense.optimizer.sparsity_functions import Constant

model = ...
agent = PruningAgent(model, Constant(0.75))
#+END_SRC

Das Modell wurde nun auf eine Sparsity von $75\%$ gepruned und kann nun regulär trainiert werden.
Ein Training-Callback wird bei dieser Implementation nicht benötigt.
Der ~PruningAgent~ führt bei der Initialisierung folgende Operationen durch:
1. Er legt eine ~HashMap~ an, die Parameter des Modells auf einen Tensor gleicher Größe abbildet.
   #+begin_quote
   Um gewisse Parameter oder Module eines Modells vom Pruning auszuschließen, kann das Konstruktor-Argument ~ignored_params~ verwendet werden.
   Es handelt sich dabei um kein benötigtes Argument, jedoch empfiehlt es sich, Output Layer eines Netzes nicht zu prunen.
   #+end_quote
2. Jeder Parameter erhält einen Callback der automatisch ausgeführt wird, sobald der jeweilige Gradient berechnet wurde.
   Dieser multipliziert die Maske auf den Gradienten; also eliminiert maskierte Felder.
   Auf diese Weise wird sichergestellt, dass maskierte Felder durch den Optimizer nicht manipuliert werden können.

** Pruning während des Trainings-Prozesses
#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import sys
sys.path.append('condense')
import condense

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True).repeat()

model = keras.models.Sequential(layers=[
    keras.layers.Dense(40, input_shape=(4,), activation='relu'),
    keras.layers.Dense(80, activation='relu'),
    keras.layers.Dense(3)
])

model.compile(keras.optimizers.Adam(learning_rate=0.001),
              keras.losses.SparseCategoricalCrossentropy(from_logits=True))
model.build()
w = model.get_weights()
unpruned_loss = model.fit(ds.batch(100), epochs=20, steps_per_epoch=50)
model.compile(keras.optimizers.Adam(learning_rate=0.001),
              keras.losses.SparseCategoricalCrossentropy(from_logits=True))

assert (w[0] != model.get_weights()[0]).any()

model.set_weights(w)

assert (w[0] == model.get_weights()[0]).all()

model = condense.keras.wrap_model(model,
                                  condense.optimizer.sparsity_functions.Constant(.3))
pruned_loss = model.fit(ds.batch(100),
                  epochs=20,
                  steps_per_epoch=50,
                  callbacks=[condense.keras.PruningCallback()])

plt.figure(figsize=(6, 3))
plt.plot(unpruned_loss.history['loss'], label='unpruned loss')
plt.plot(pruned_loss.history['loss'], label='pruned loss')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.tight_layout()
plt.savefig('resources/plots/keras-2.png')
return 'resources/plots/keras-2.png'
#+END_SRC

#+CAPTION: Training-Loss des Modells mit pruning und ohne.
#+ATTR_LATEX: :float wrap :width 8cm :center nil
#+RESULTS[a4f518ecb23fc697566f978a7f12547bb0bbc26f]:
[[file:resources/plots/keras-2.png]]

Eine Möglichkeit ein Modell während des Trainings-Prozesses zu prunen ist, die jeweilig zu prunenden Parameter nach bestimmten Trainingsabschnitten immer wieder erneut zu prunen.

Integriert wurde dies in dieser Implementation in der Form eines Wrappers für Keras Layer.
So müssen die Layer eines bestehenden Modells nur an den Constructor der Wrapper Klasse übergeben werden und dieser berechnet einen geeigneten Sparsity-Tensor während des Trainings.
Der Ablauf von Operationen während des Trainings kann in Abbildung ref:fig:pruning-process betrachtet werden.
Durch die Callback API, die von Keras zur Verfügung gestellt wird, werden die Pruning Operationen in definierten Intervallen während des Trainings ausgeführt.
Bei der Pruning Operation handelt es sich um die elementare Multiplikation der Sparsity Mask $M$ und des jeweiligen Parameters $p$ Tensors $p_{neu} = p_{alt} \times M$.

#+begin_src mermaid :file resources/plots/training-process.png :theme forest :background transparent :cache yes
graph LR
    fit(.fit) --> forward-pass[Forward Propagation]
    forward-pass --> bp[Back Propagation]
    bp --> update[Update Sparsity Mask]
    update --> pruning[Perform Pruning]
#+end_src

#+LABEL: fig:pruning-process
#+CAPTION: Ablauf des Pruning Prozesses während des Trainings.
#+RESULTS[95245539d25fae59ce110bf3118accc7226369a4]:
[[file:resources/plots/training-process.png]]


#+BEGIN_SRC python :exports results :results file :cache yes
import sys
sys.path.append('condense')
import condense
import keras
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
from keras.layers import Dense

model = keras.models.Sequential(layers=[
    keras.layers.Dense(40, input_shape=(4,), activation='relu'),
    keras.layers.Dense(80, activation='relu'),
    keras.layers.Dense(1)
])
model.compile('adam', 'mse')

t_sparsity = 0.8

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
ds = ds.repeat()
layer = 2

old_weights = model.get_weights()

plt.figure(figsize=(12, 6))
plt.subplot(221)
plt.imshow(abs(model.get_weights()[2]), vmin=0, vmax=.4)
plt.title(f'Layer {layer} des ursprünglichen Modells')

hist1 = model.fit(ds.batch(200), steps_per_epoch=200, epochs=10)
plt.subplot(222)
plt.imshow(abs(model.get_weights()[2]), vmin=0, vmax=.4)
plt.title(f'Layer {layer} des trainierten Modells (ohne Pruning)')

model = condense.keras.wrap_model(model, condense.optimizer.sparsity_functions.Constant(t_sparsity))
hist2 = model.fit(ds.batch(200), steps_per_epoch=200, epochs=10, callbacks=[condense.keras.callbacks.PruningCallback()])

plt.subplot(223)
plt.imshow(abs(model.layers[1].kernel), vmin=0, vmax=.4)
plt.title(f'Layer {layer} des trainierten Modells (mit {t_sparsity*100}% Pruning)')

plt.subplot(224)
plt.imshow(abs(model.layers[1].mask), cmap='gist_gray')
plt.title(f'Sparsity Mask des Layers {layer}')

plt.tight_layout()
plt.savefig('resources/plots/keras-1.png')
return 'resources/plots/keras-1.png'
#+END_SRC

#+CAPTION[Weights training mit Pruning und ohne]: Ausschnitts der Weights eines Layers trainiert mit und ohne Pruning.
#+CAPTION: Das Modell wurde auf dem Iris Dataset cite:TFDS (Kapitel ref:datensatz) trainiert
#+RESULTS[0a481db813a8cbb39cf65b0aa86bce939c3e1e32]:
[[file:resources/plots/keras-1.png]]

** Keras/TensorFlow als Backend
Operationen des neuronalen Netzes, wie das Training oder die Evaluierung, werden durch das, auf neuronale Netze ausgelegte \ac{ML} Framework TensorFlow ausgeführt.
Dies bietet Nutzern erhebliche Vorteile wie die mögliche Ausführung auf verschiedensten Plattformen wie GPU/CPU oder Hardware Beschleunigern.
Ein weiterer Vorteil ist, dass sich TensorFlow "Layer" eines neuronalen Netzes ohne großen Aufwand durch eine öffentliche API erweitern lassen.
Somit lässt sich im weiteren Verlauf des Projekts eine direkte Integration in das TensorFlow Ökosystem anstreben.
Mit TensorFlow bietet sich zusätzlich die Möglichkeit an, durch das externe Modul ~model-optimization~[fn:model_opt], Optimierungen an einem Keras/TensorFlow Modell vorzunehmen.
Auch Pruning wird derzeit von dem Tool unterstützt, indem ein vorhandenes Modell durch die Augmentation von Layern um Pruning Funktionalität erweitert werden kann.
Es besteht folglich die Möglichkeit, auch ein schon bestehendes Backend für Pruning Operationen zu nutzen.
Demzufolge können Pruning Operationen zuerst auf das ~model-optimization~ Modul ausgelagert werden und es kann sich auf das Refitting und die Analyse von neuronalen Netzen konzentriert werden.

* Ergebnisse <<results>>

** Einfluss der Modell Architektur und des Datensatzes
Die Effektivität der in dieser Arbeit behandelten Pruning Methoden hängt stark von dem zu prunenden Modell und der damit
einhergehenden Komplexität der Trainingsdaten ab.
Sei $C_M$ die maximale zu erfassenden Komplexität des Modells und $C_D$ die Komplexität der Trainingsdaten.
Ist $C_M < C_D$ so wird das Modell nicht in der Lage sein, die numerischen Zusammenhänge aus dem Trainingsdatensatz zu erfassen.
Analog gilt für $C_M > C_D$ das Gegenteil.
Durch Pruning wird versucht, die maximal zu erfassende Komplexität des Modells so zu verringern,
dass die Genauigkeit des Modells bezüglich der Test Daten möglichst unverändert bleibt.
Demnach ist Pruning besonders effektiv, wenn die maximale Modell Komplexität $C_M$ sehr viel größer ist
als die zugrunde liegenden Zusammenhänge des Datensatzes $C_D$.

** One Shot Pruning
Das One Shot Interface des ~condense~ Moduls ist durch ~condense.one_shot()~ zu nutzen.
So ist es sehr leicht möglich, Keras Modelle durch One-Shot Pruning zu optimieren.
In diesem Kapitel werden verschiedenste Modelle durch diese Methode optimiert und evaluiert.
Ein Beispiel für das One-Shot Interface des ~condense~ Moduls ist in Abbildung ref:src:one-shot zu sehen.

#+LABEL: src:one-shot
#+CAPTION: Beispiel des One-Shot Pruning Interface
#+BEGIN_SRC
model = keras.models.load_model(...)  # keras model
pruned = condense.one_shot(model, 0.9)  # 90% target sparsity
#+END_SRC

*** Iris Dataset <<one_shot_iris>>
#+begin_quote
Bei diesem Modell handelt es sich um ein eigens auf den Iris Datensatz (Kapitel ref:datensatz) trainiertes Dense \ac{ANN}.
#+end_quote
Naives Pruning in Form von One-Shot Pruning hat drastische Auswirkungen auf die Modell Accuracy,
wie in Abbildung ref:fig:iris-results-one-shot zu sehen ist.
Aus diesem Grund sollte One-Shot Pruning in den meisten Fällen nicht für die Optimierung von Modellen genutzt werden.

#+BEGIN_SRC python :exports results :results file :cache yes
import numpy as np
import sys
import keras
import matplotlib.pyplot as plt
sys.path.append("condense")
import condense
import tensorflow_datasets as tfds

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
ds = ds.repeat()

model = keras.models.load_model('resources/models/iris.h5')

def testing_function(model, refrence):
    model.compile(optimizer='adam', loss="mse")
    return model.evaluate(ds.batch(200), steps=20)

steps = np.arange(0, .99, .05)

plt.figure(figsize=(8, 4))
plt.plot(steps*100,
         [testing_function(condense.one_shot(model, acc), model) for acc in steps])

plt.title('Verlust von Accuracy mit zunehmender Pruning Stärke')
plt.xlabel('Sparsity des Modells in %')
plt.ylabel('Model Accuracy (mse)')
plt.grid()
plt.savefig('resources/plots/iris-accuracy-1.png')
return 'resources/plots/iris-accuracy-1.png'
#+END_SRC

#+LABEL: fig:iris-results-one-shot
#+CAPTION[One-Shot Pruning Accuracy eines Dense Modells]: Dense Model Accuracy mit zunehmender Pruning Stärke.
#+ATTR_LaTeX: :height 5cm :placement [!htpb]
#+RESULTS[066034f98af50c842543c2d7c7bc5bef23414868]:
[[file:resources/plots/iris-accuracy-1.png]]

*** Convolutional Model
In diesem Beispiel wird ein Convolutional Model durch immer stärkeres One-Shot Pruning optimiert.
Das Modell umfasst deutlich mehr Parameter als das Dense Modell aus Kapitel ref:one_shot_iris.
Aus der Abbildung ref:fig:mnist-results-one-shot ist zu entnehmen, dass sich die Test-Accuracy des Modells bis zu einer Parameter-Sparsity von $40\%$ kaum negativ verändert.
Daraus lässt sich die relativ niedrige Informationsdichte des Modells schlussfolgern.
Wie bereits angesprochen, eignen sich genau diese Modelle für Pruning Optimierungen deutlich besser, als Modelle mit einer dichteren Informationsdichte.
In dem in Abbildung ref:src:one-shot-mnist gezeigten Quellcodeausschnitt
wird das One-Shot Interface des PyTorch Kompatibilitätsmodul gezeigt (Kapitel ref:pytorch).
Hervorzuheben ist dabei das Konstruktor Argument ~ignored_params~,
bei dem spezifiziert wird, die letzte Schicht des Modells (also der Output Layer) nicht zu prunen.
Das Argument ~apply_mask=True~ sorgt für die sofortige Anwendung der generierten Maske und nimmt somit One-Shot Pruning vor.

#+LABEL: src:one-shot-mnist
#+CAPTION: One-Shot Pruning eines Torch Models durch den ~condense.torch.PruningAgent~
#+BEGIN_SRC
model = ...  # pytorch model
pruned = PruningAgent(model,
                      Constant(0.7),
                      # generierte maske wird direkt angewandt
                      apply_mask=True,
                      ignored_params=[list(model.parameters())[-1]])
#+END_SRC

#+BEGIN_SRC python :exports results :results file :cache yes
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import sys
import numpy as np
from datasets import mnist
sys.path.append('condense')
import condense
import torchviz
from graphviz import Source

train_loader, test_loader = mnist()

model = nn.Sequential(
    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=4, stride=2),
    nn.ReLU(),
    nn.Conv2d(in_channels=4, kernel_size=4, out_channels=1, stride=1),
    nn.ReLU(),
    nn.Flatten(),
    nn.Linear(100, out_features=255),
    nn.ReLU(),
    nn.Linear(255, out_features=255),
    nn.ReLU(),
    nn.Linear(255, out_features=128),
    nn.ReLU(),
    nn.Linear(128, out_features=10)
)

loss = torch.nn.CrossEntropyLoss()

optim = torch.optim.Adam(model.parameters(), lr=0.001)
hist, test_hist = [], []

for _, i in enumerate(train_loader):
    model.zero_grad()
    pred = model.forward(i[0])
    l = loss(pred, i[1])
    hist.append(l)
    l.backward()
    optim.step()

    _, (X, y) = next(enumerate(test_loader))
    test_hist.append(loss(model.forward(X), y))

def testing_function(model, acc):
    agent = condense.torch.PruningAgent(model,
                                        condense.optimizer.sparsity_functions.Constant(acc),
                                        apply_mask=True,
                                        ignored_params=[list(model.parameters())[-1]])


    _, (X, y) = next(enumerate(test_loader))
    l = loss(model.forward(X), y)

    return l

plt.figure(figsize=(8, 4))
acc = [testing_function(model, a) for a in np.arange(0, .99, .05)]
plt.plot(np.arange(0, .99, .05)*100, acc)
plt.xlabel('Sparsity des Modells in %')
plt.ylabel('Model Loss (crossentropy)')
plt.title('Verlust von Accuracy mit zunehmender Pruning Stärke')
plt.grid()
plt.tight_layout()
plt.savefig('resources/plots/conv_one_shot.png')
return 'resources/plots/conv_one_shot.png'
#+END_SRC

#+LABEL: fig:mnist-results-one-shot
#+CAPTION[One-Shot Pruning Accuracy eines Convolutional Modells]: Convolutional Model Accuracy mit zunehmender Pruning Stärke.
#+RESULTS[3d38d99a1f6c222d7f832659b987956c2f75d35f]:
[[file:resources/plots/conv_one_shot.png]]

** Training eines Sub-Networks (Lottery Ticket Hypothesis cite:Frankle2018)
Bei vorangegangenen Pruning-Methoden wurde bisher nur nach oder während des Trainings-Prozesses Pruning vorgenommen.
Es ist jedoch auch möglich, das Training auf einem Sub-Network, also einem geprunten Netz des Originals, vorzunehmen cite:Frankle2018.
Für eine genauere Beschreibung dieser Methodik wird an dieser Stellte auf Kapitel ref:lottery_ticket dieser Arbeit verwiesen.

Durch die Anwendung der "Lottery Ticket Hypothesis" cite:Frankle2018 wird erreicht, den initialen Trainingsprozess deutlich zu verbessern.
Diese Pruning Methode ist nur für Modelle geeignet, die noch nicht trainiert wurden; für bereits trainierte Modelle sollte ein One-Shot Pruning Ansatz gewählt werden.
So zeigt Abbildung ref:fig:torch-lottery-1 die Auswirkungen der Ticket-Suche und Reinitialisierung auf den Evaluation Loss des letztendlichen Trainingsprozesses.

#+BEGIN_SRC python :exports results :results file :cache yes
import sys
sys.path.append('condense')
import torch
import condense
import torch.nn as nn
import matplotlib.pyplot as plt
import torchvision
from datasets import mnist

d_train, d_test = mnist()

class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.layer1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=4, stride=2)
        self.layer2 = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=2)
        self.dense = nn.Linear(288, out_features=50)
        self.dense2 = nn.Linear(50, out_features=50)
        self.output = nn.Linear(50, out_features=10)


    def forward(self, X):
        X = self.layer1.forward(X)
        X = self.layer2.forward(torch.relu(X))
        X = X.view(torch.relu(X).size(0), -1)
        X = self.dense.forward(torch.relu(X))
        X = self.dense2.forward(torch.relu(X))
        X = self.output.forward(torch.relu(X))
        return X

    def train(self, data, epochs=None, eval_data=None, lr=0.001):
        metrics = {
            "loss": [],
            "eval": []
        }
        criterion = nn.CrossEntropyLoss()
        optim = torch.optim.Adam(self.parameters(), lr=lr)
        for i, (X, y) in enumerate(data):
            if epochs and epochs < i:
                break

            self.zero_grad()
            pred = self.forward(X)
            l = criterion(pred, y)
            l.backward()
            optim.step()
            metrics['loss'].append(l)

            if eval_data:
                _, (X, y) = next(enumerate(eval_data))
                metrics['eval'].append(criterion(self.forward(X), y))

        return metrics

test_net = Network()

model = Network()
model.load_state_dict(test_net.state_dict())
p = condense.torch.PruningAgent(model, condense.optimizer.sparsity_functions.Constant(0.5), apply_mask=False, ignored_params=[model.output])

with condense.torch.TicketSearch(p):
    model.train(d_train, 200, d_test, lr=0.001)

fine_t_data = model.train(d_train, 300, d_test, lr=0.001)

# Training without pruning
without_data = test_net.train(d_train, 500, d_test, 0.001)

plt.figure(figsize=(9, 6))

plt.plot(fine_t_data['eval'], label='Evaluation Loss während des Fine Tuning Prozesses')
plt.plot(without_data['eval'], label='Evaluation Loss während des Trainings ohne Pruning')
plt.title('Training eines geprunten Modells ($50\\%$ Sparsity)')
plt.xlabel('# Epoch')
plt.ylabel('Eval Loss')
plt.grid()
plt.legend()
plt.axis([0, 200, 0, 2.3])
plt.tight_layout()
plt.savefig('resources/plots/torch-lottery-2.png')
return 'resources/plots/torch-lottery-2.png'
#+END_SRC

#+LABEL: fig:torch-lottery-1
#+CAPTION[Vergleich zwischen optimierten und unoptimierten Trainingsverläufen]:
#+CAPTION: Beide hier dargestellten Modelle wurden mit den identischen Parameter Werten initialisiert.
#+CAPTION: Es kann beobachtet werden, dass das "Winning Ticket" Modell deutlich schneller konvergiert, als das unoptimierte Modell.
#+ATTR_LATEX: :float wrap :width 8cm :center nil
#+RESULTS[687cc7ba324250020c1712a56f0185dc27528bb8]:
[[file:resources/plots/torch-lottery-2.png]]

Bei der Wahl verschiedener Learning Raten der Modelle kann, wie in Abbildung ref:fig:torch-lottery-lr gezeigt, konstant eine effizientere
Konvergenz des Training/Evaluation Errors erreicht werden.
Folglich bietet das Training auf einem geprunten Modell je nach Datensatz und verwendeter Modell-Architektur enormes Potential, ein schnelleres und
eventuell besser generalisierendes Modell zu erhalten.
Demnach ist es durch die Implementation des unter dieser Arbeit entwickelten Frameworks gelungen, die Ergebnisse des ursprünglichen Artikels zu reproduzieren.

#+BEGIN_SRC python :exports results :results file :cache yes
import sys
sys.path.append('condense')
import torch
import condense
import torch.nn as nn
import matplotlib.pyplot as plt
import torchvision
from datasets import mnist

d_train, d_test = mnist()

class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.layer1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=4, stride=2)
        self.layer2 = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=2)
        self.dense = nn.Linear(288, out_features=50)
        self.dense2 = nn.Linear(50, out_features=50)
        self.output = nn.Linear(50, out_features=10)


    def forward(self, X):
        X = self.layer1.forward(X)
        X = self.layer2.forward(torch.relu(X))
        X = X.view(torch.relu(X).size(0), -1)
        X = self.dense.forward(torch.relu(X))
        X = self.dense2.forward(torch.relu(X))
        X = self.output.forward(torch.relu(X))
        return X

    def train(self, data, epochs=None, eval_data=None, lr=0.001):
        metrics = {
            "loss": [],
            "eval": []
        }
        criterion = nn.CrossEntropyLoss()
        optim = torch.optim.Adam(self.parameters(), lr=lr)
        for i, (X, y) in enumerate(data):
            if epochs and epochs < i:
                break

            self.zero_grad()
            pred = self.forward(X)
            l = criterion(pred, y)
            l.backward()
            optim.step()
            metrics['loss'].append(l)

            if eval_data:
                _, (X, y) = next(enumerate(eval_data))
                metrics['eval'].append(criterion(self.forward(X), y))

        return metrics

test_net = Network()

def benchmark_function(lr_search, lr_train):
    model = Network()
    model.load_state_dict(test_net.state_dict())
    p = condense.torch.PruningAgent(model, condense.optimizer.sparsity_functions.Constant(0.7), apply_mask=False, ignored_params=[model.output])

    with condense.torch.TicketSearch(p):
        search = model.train(d_train, 200, d_test, lr=lr_search)

    return model.train(d_train, 300, d_test, lr=lr_train), search


plt.figure(figsize=(14, 6))

for i, (lr_train, c) in enumerate(zip([0.01, 0.005, 0.001], ['#32a852', '#264653', '#9d0208'])):
    fine_t_data, search_data = benchmark_function(lr_train, lr_train)
    # plt.subplot(3,1,i+1)
    plt.plot(fine_t_data['eval'], label=f'Fine Tuning ({lr_train} lr)', lw=1.8, c=c)
    plt.plot(search_data['eval'], label=f'Ticket Searching ({lr_train} lr)', ls='--', c=c)
    plt.xlabel('# Epoch')
    plt.ylabel('Eval Loss')
    plt.grid()
    plt.legend()

plt.title('Convolutional Model trainiert auf MNIST Dataset')
plt.tight_layout()
plt.savefig('resources/plots/torch-lottery.png')
return 'resources/plots/torch-lottery.png'
#+END_SRC

#+LABEL: fig:torch-lottery-lr
#+CAPTION[Lottery Ticket Hypothesis und Learning Rate]: In dem hier gezeigten Diagramm wird ein identisches Modell,
#+CAPTION: bestehend aus convolutional und dense Layern durch, die in der Lottery Ticket Hypothesis Methodik optimiert.
#+CAPTION: Dabei symbolisiert jede Farbe eine Learning Rate, die für das Suchen und Trainieren des Modells gewählt wurde.
#+CAPTION: Die gestrichelten Linien repräsentieren den jeweiligen Evaluation-Loss des Winning Ticket Such-Prozesses.
#+CAPTION: Die durchgezogenen Linien symbolisieren den Evaluation-Loss des Fine-Tuning Prozesses.
#+RESULTS[dcfd136a8394c0ab1b61cdcb81c589cf1c082cc9]:
[[file:resources/plots/torch-lottery.png]]

** Einfluss von Aktivierungs-Funktionen auf Pruning Verfahren <<activations>>
Es lässt sich bei der Analyse von Layer-Sparsity auf neuronalen Netzen, die mit unterschiedlichen Aktivierungsfunktionen
trainiert wurden, feststellen, dass Aktivierungsfunktionen starke Auswirkungen auf die Gewichtungsverteilung der Schichten eines \acp{ANN} hat.
Bei dem Betrachten der Abbildung ref:fig:activation_fn, werden die Auswirkungen der Aktivierungsfunktionen ersichtlicher.

#+BEGIN_SRC python :exports results :results file :cache yes
import tensorflow_datasets as tfds
import keras
import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize=(12,8))
AF = ['relu', 'sigmoid', 'tanh']
ds = tfds.load('iris', split=['train'], shuffle_files=True, as_supervised=True)[0]

for i, activation_fn in enumerate(AF):
    plt.subplot(3, len(AF), i+1)
    model = keras.models.Sequential(layers=[
        keras.layers.Dense(20, input_shape=(None, 4), activation=activation_fn),
        keras.layers.Dense(20, activation=activation_fn),
        keras.layers.Dense(3, activation='softmax')
    ])
    model.compile(keras.optimizers.Adam(learning_rate=0.3), 'sparse_categorical_crossentropy')
    model.fit(ds.batch(300), epochs=400, steps_per_epoch=10000)
    plt.imshow(np.abs(model.layers[1].kernel.numpy()), vmin=0.0, vmax=0.7)
    plt.title(f'{activation_fn} ({np.mean(np.abs(model.layers[1].kernel.numpy()))})')
    plt.subplot(3, len(AF), 3+i+1)
    plt.hist(np.abs(model.layers[1].kernel.numpy().flatten()))
    # Diff
    plt.subplot(3, len(AF), 6+i+1)
    plt.title('Weight Pruning $30\\%$')
    w = np.abs(model.layers[1].kernel.numpy())
    w[w > np.sort(w.flatten())[int(np.prod(w.shape) * 0.4)]] = 0
    plt.imshow(w, vmin=0.0, vmax=0.2, cmap='gist_gray')


plt.tight_layout()
plt.savefig('resources/plots/sparsity_activation.png')
return 'resources/plots/sparsity_activation.png'
#+END_SRC

#+LABEL: fig:activation_fn
#+CAPTION[Einfluss von Aktivierungsfunktionen auf die Verteilung von Verbindungsgewichtungen]:
#+CAPTION: Einfluss von verschiedenen Aktivierungsfunktionen auf die Gewichtungs-Verteilung einer \ac{ANN} Schicht.
#+CAPTION: Das dargestellte neuronale Netz wurde auf dem Iris Datensatz trainiert cite:fisher36lda.
#+CAPTION: Bei den dargestellten Werten, handelt es sich jeweils immer um den absoluten Wert einer Gewichtung.
#+RESULTS[9089ef3251e99a9134f406d0e66919b42e142e2d]:
[[file:resources/plots/sparsity_activation.png]]

Eine Aktivierungsfunktion, die tendenziell eine höhere Anzahl von Gewichtungen gegen $0$ optimiert,
ist für ein zu prunenden Netz begehrenswerter.
Somit gehen bei dem Pruning deutlich weniger stark gewichtete Verbindungen verloren.
In Abbildung ref:fig:activation_fn ist in der letzten Zeile zu erkennen, dass deutliche Unterschiede bei den zu prunenden Gewichtungen zu erkennen sind.
So ist es bei dem mit ~ReLU~ trainierten Netz auffällig, wie im Verhältnis die geprunten Verbindungen deutlich schwächer gewichtet sind als bei
den anderen Netzen.

Die Erkenntnisse in diesem Kapitel zeigen, dass es empfehlenswert ist, Aktivierungsfunktionen für das Training von zu prunenden \acp{ANN} zu nutzen,
um die Gewichtungen gegen $0$ zu optimieren.

** Anwendung der One-Shot API auf allgemeinen ANN Architekturen
Das Optimieren von bereits trainierten und öffentlich bereitgestellten neuronalen Netzen und deren Gewichtungen,
bringt viele Herausforderungen mit sich.
Besonders schwer lassen sich diese durch Pruning optimieren, da es sich meist um sehr optimierte (also informations-dichte) Netze handelt.

#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
import sys
sys.path.append('condense')
import condense
from condense.utils.model_utils import calc_model_sparsity
from condense.utils.layer_utils import calc_layer_sparsity
from keras.applications.resnet50 import ResNet50

model = ResNet50(weights='imagenet')
pruned = condense.one_shot(model, 0.3)

plt.figure(figsize=(10,4))
ax1 = plt.subplot(111)
ax1.bar(np.arange(0, len(model.get_weights()[10:]), 1),
        height=[np.mean(np.abs(layer)) for layer in model.get_weights()[10:]],
        label=f'Unpruned Model ({round(calc_model_sparsity(model)*100, 2)}% Sparsity)',
        width=3,
        zorder=1)
ax1.bar(np.arange(0, len(model.get_weights()[10:]), 1),
        height=[np.mean(np.abs(layer)) for layer in pruned.get_weights()[10:]],
        label=f'Pruned Model ({round(calc_model_sparsity(pruned)*100, 2)}% Sparsity)',
        width=1,
        zorder=2)

# ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
# ax2.bar(np.arange(0, len(model.get_weights()[10:]), 1),
#         height=[calc_layer_sparsity(layer) for layer in pruned.get_weights()[10:]],
#         label=f'Pruned Model ({round(calc_model_sparsity(pruned)*100, 2)}% Sparsity)',
#         color='grey',
#         alpha=0.7,
#         zorder=2)
ax1.legend()
ax1.set_title('ResNet50 Weight Distribution')
ax1.set_xlabel('Network Layer Nr.')
ax1.set_ylabel('Avg. Layer Weights')

plt.grid()
plt.legend()

plt.savefig('./resources/plots/resnet50_hist.png')
return './resources/plots/resnet50_hist.png'
#+END_SRC

#+CAPTION[ResNet50 Weight Matrix Histogramm]: ResNet50 Weight Matrix Histogramm
#+RESULTS[78171136576c486b045dda83dae5c98ed02b5565]:
[[file:./resources/plots/resnet50_hist.png]]

*** ResNet-50 cite:altenberger18:_non_techn_survey_deep_convol <<resnet50_oneshot>>
In diesem Beispiel wird ein \ac{ANN} Modell der ~ResNet-50~ cite:altenberger18:_non_techn_survey_deep_convol Architektur,
trainiert auf dem ~ImageNet~ cite:imagenet_cvpr09 Datensatz, durch One-Shot Pruning immer stärker optimiert.
Der dadurch resultierende Verlust von Test-Accuracy ist in Abbildung ref:fig:resnet50-one-shot-accuracy visualisiert.
Aus dem linearen Anstieg von Accuracy-Verlust in Abhängigkeit der Ansteigen Parameter-Sparsity,
lässt sich eine hohe Informationsdichte schlussfolgern.
Demzufolge bietet sich die ~ResNet-50~ Architektur nicht besonders als Modell für Pruning Optimierungen an.
Diese Resultate werden auch in dem wissenschaftlichen Artikel 'The Lottery Ticket Hypothesis' cite:Frankle2018 festgestellt.

#+BEGIN_SRC python :results file :exports results :cache yes
import numpy as np
import matplotlib.pyplot as plt
from random import choice
from datasets import imagenet
from keras.applications.resnet50 import ResNet50
from torch.utils.data import DataLoader
from keras.applications.imagenet_utils import preprocess_input, decode_predictions
from glob import glob
from PIL import Image
import keras
import sys
sys.path.append('condense')
import condense

DATA = '/Users/lucas/Dropbox/FH-Landshut Courses/Bachelor Arbeit/resources/datasets/imagenet/'
def test_data_generator():
    while True:
        label = choice(glob(f'{DATA}*/')).split('/')[-2]
        im = Image.open(choice(glob(f'{DATA}/{label}/*')))
        im = im.resize((224, 224))
        im = preprocess_input(np.asarray(im))
        y = np.zeros((1, 1000))
        y[0, int(label)] = 1.0
        X = np.array([im])
        yield X, y

gen = test_data_generator()
def evaluate(model):
    correct = 0
    X, y = [], []
    for _ in range(150):
        _X, _y = next(gen)
        X.append(_X)
        y.append(_y)
    X = np.array(X).reshape(150, 224, 224, 3)
    y = np.array(y).reshape(150, 1000)
    return np.mean(keras.losses.categorical_crossentropy(model.predict(X), y))

rang = np.arange(0, .99, .3)
acc = [evaluate(condense.one_shot(ResNet50(weights='imagenet'), acc)) for acc in rang]
plt.figure(figsize=(8, 3))
plt.plot(rang*100, acc)
plt.grid()
plt.xlabel('Sparsity Percentage')
plt.ylabel('Model Accuracy')
plt.title('Test-Accuracy Verlust durch One-Shot Pruning (Resnet50)')
plt.tight_layout()
plt.savefig('resources/plots/resnet50-one-hot.png')
return 'resources/plots/resnet50-one-hot.png'
#+END_SRC

#+LABEL: fig:resnet50-one-shot-accuracy
#+CAPTION[Verlust von Test-Accuracy durch One-Shot Pruning]: Verlust von Test-Accuracy durch One-Shot Pruning.
#+RESULTS[62cef4f7607137fd47367e4a6b6e2ef7d29e04d3]:
[[file:resources/plots/resnet50-one-hot.png]]

*** VGG16 cite:DBLP:journals/corr/SimonyanZ14a
Ein weiteres auf ~ImageNet~ cite:imagenet_cvpr09 trainiertes neuronales Netz der ~VGG16~ Architektur cite:DBLP:journals/corr/SimonyanZ14a
weist dieselben, in Kapitel <<resnet50_oneshot>> gefundenen Eigenschaften auf.
Auch hier ist ein massiver Verlust von Test-Accuracy durch One-Shot Pruning festzustellen, wie Abbildung ref:fig:vgg16-os zeigen soll.

#+BEGIN_SRC python :results file :exports results :cache yes
import numpy as np
import matplotlib.pyplot as plt
from random import choice
from datasets import imagenet
from keras.applications import VGG16
from torch.utils.data import DataLoader
from keras.applications.vgg16 import preprocess_input
from glob import glob
from PIL import Image
import keras
import sys
sys.path.append('condense')
import condense

DATA = '/Users/lucas/Dropbox/FH-Landshut Courses/Bachelor Arbeit/resources/datasets/imagenet/'
def test_data_generator():
    while True:
        label = choice(glob(f'{DATA}*/')).split('/')[-2]
        im = Image.open(choice(glob(f'{DATA}/{label}/*')))
        im = im.resize((224, 224))
        im = preprocess_input(np.asarray(im))
        y = np.zeros((1, 1000))
        y[0, int(label)] = 1.0
        X = np.array([im])
        yield X, y

gen = test_data_generator()
def evaluate(model):
    correct = 0
    X, y = [], []
    for _ in range(150):
        _X, _y = next(gen)
        X.append(_X)
        y.append(_y)
    X = np.array(X).reshape(150, 224, 224, 3)
    y = np.array(y).reshape(150, 1000)
    return np.mean(keras.losses.categorical_crossentropy(model.predict(X), y))

rang = np.arange(0, .99, .3)
acc = [evaluate(condense.one_shot(VGG16(weights='imagenet'), acc)) for acc in rang]
plt.figure(figsize=(8, 3))
plt.plot(rang*100, acc)
plt.grid()
plt.xlabel('Sparsity Percentage')
plt.ylabel('Model Accuracy')
plt.title('Test-Accuracy Verlust durch One-Shot Pruning (VGG16)')
plt.tight_layout()
plt.savefig('resources/plots/vgg16-one-hot.png')
return 'resources/plots/vgg16-one-hot.png'
#+END_SRC

#+LABEL: fig:vgg16-os
#+CAPTION: ~VGG16~ Accuracy Verlust durch One-Shot Pruning.
#+RESULTS[e01cb761bcc5a0ab4b37b9bbb4484a368df303f4]:
[[file:resources/plots/vgg16-one-hot.png]]


* Fazit

** Evaluierung des ~condense~ Python Moduls
Das unter dieser Arbeit entwickelte Python Modul ~condense~ erfüllt, wie in Kapitel ref:results gezeigt,
die in Kapitel ref:ziel_framework aufgestellten Anforderungen an das Framework gänzlich.
Einen Aspekt, den ~condense~ derzeit nicht erfüllt, ist die komprimierte Speicherung geprunter \ac{ANN} Modelle.
Diese Funktionalität war zwar keine direkte Anforderung an das Projekt, jedoch ein wichtiger Aspekt für viele Use-Cases von \ac{ANN} Pruning.
Es wurde im Umfang dieser Arbeit gezeigt (Kapitel ref:results), dass es mit der Hilfe von ~condense~ möglich ist,
mit einem meist niedrigen Accuracy Verlust eine hohe Sparsity bei einem \AC{ANN} zu erzielen.
Dabei ist die Effizienz der eingesetzten Algorithmen davon abhängig, welche \ac{ANN} Architektur
verwendet wird und welche Informationsdichte innerhalb des Modells vorliegt.
Zudem ist die ~condense~ Schnittstelle für die jeweiligen unterstützten Frameworks äußerst einfach zu bedienen und ohne ein umfangreiches Know-How nutzbar.
Ebenso ist es aber durch modulares Design möglich, das Verhalten des Frameworks auf den eigenen Use-Case anzupassen.

Im Laufe der Erstellung und Erprobung des Frameworks und dessen Pruning Operationen wurde deutlich,
dass diese primär für nicht optimierte \ac{ANN} Modelle geeignet sind.

** Kapitalisierung von Sparsity in neuronalen Netzen <<kapit>>
Aus der erhöhten Sparsity von neuronalen Netzen lässt sich leider kein direkter Leistungsanstieg in Form von Inferenz Zeit verzeichnen.
Herkömmliche Multiplikation von Matrizen wird nicht durch die erhöhte Sparsity beschleunigt.
Folglich kann meist lediglich eine bessere Generalisierung des Modells beobachtet werden.
Frameworks wie TensorFlow oder Torch bieten auf Sparse Tensoren spezialisierte Operationen,
wie beispielsweise Matrix-Multiplikation an.
Die Implementation für Element-Wise Matrix Multiplikation, die für die Maskierung der Tensoren benötigt wird, fehlt meist.
Jedoch beschäftigen sich bereits viele verschiedene Teams mit der Entwicklung von Hard- und Softwarelösungen um das Potential von Sparse \acp{ANN}
auszuschöpfen.
So lässt sich durch spezielle Hardware Beschleuniger eine Verbesserung der Inferenz-Zeit um einen Faktor von $3.6\times$ bis $12.9\times$ erreichen cite:8735526.
Ob sich derartige Hardware Beschleuniger beweisen können, wird sich in Zukunft zeigen.

Eine schon bestehende Software Lösung für derartige Optimierungen könnte das unter Facebook AI Entwickelte Framework ~GLOW~[fn:glow] bieten.
Bei diesem handelt es sich um einen Compiler, der \ac{ANN} Modelle direkt in Maschinen-Code übersetzen kann cite:rotem2019glow.
Durch das von ~GLOW~ genutzte ~LLVM~ Backend, lassen sich Optimierungen auf dem ~IR~ des Modell-Codes anwenden.
Besonders für die Entwicklung auf Embedded Devices ist dieses Tool sehr relevant, dabei bietet es auch äußerst vielversprechende Optimierungen für Sparse \ac{ANN} Modelle cite:Lewis2018.

* Ausblick

** Weiterentwicklung des Frameworks
Da die leichte Weiterentwicklung des Frameworks bereits zu Beginn eine wichtige Designanforderung des Projektes war,
ist die Implementierung weiterer Funktionalität ausgesprochen erstrebenswert.
Mögliche Aufgaben für die weitere Entwicklung wären beispielsweise:
- Implementierung weiterer Sparsity/Masking Funktionen
- allgemeine Verbesserungen der Effizienz verschiedener Operationen des Frameworks
- Unterstützung weiterer \ac{ML} Frameworks und Bibliotheken
- Weitere Ausarbeitung der Dokumentation
- Refactoring kleinerer Teile des Moduls
- weitere Automatisierungen
 
Es muss jedoch darauf hingewiesen werden, dass das Projekt auch ohne eine weitere Entwicklung definitiv für \ac{ML}-Projekte benutzbar ist.

** Forschung
Neural Network Optimierung und insbesondere Pruning ist ein sehr komplexes und unerforschtes Themengebiet und bietet dementsprechend auch
erhebliches Potential für weitere wissenschaftliche Erkenntnisse.
Besonders die Generierung von Parameter/Sparsity Masken bietet großes Potential für weitere Verbesserungen der Pruning-Effizienz.
Es wäre außerdem sehr interessant, weitere Tests des Pruning Frameworks auf verschiedenen, evtl. nicht standard \ac{ANN} Architekturen durchzuführen.
Möglicherweise können so neue Erkenntnisse über das Pruning Potential verschiedener \ac{ANN} Architekturen gesammelt werden.

Besonders erwähnenswert ist die weitere Forschung im Bereich der spezialisierten Hardware.
Wie in Kapitel ref:kapit bereits angesprochen wurde, verspricht die Verwendung von für Sparsity Tensor optimierte Hardware enormes Leistungspotential cite:8735526.
Womöglich lässt sich auch durch spezialisierte Software Lösungen ein ähnlicher Leistungsanstieg verzeichnen.

#+LATEX: \printbibliography

\appendix
\includepdf{Quick Start.pdf}

* Footnotes
[fn:sparsity] Sparsity beschreibt die Anzahl von Feldern in einem Tensor, die einer $0$ entsprechen.
Somit setzt sich die Sparsity eines künstlichen neuronalen Netzes aus der Sparsity eines jeden Layers des Networks zusammen.
[fn:tensorflow] Ein von Google entwickeltes Deep-Learning Framework [[https://www.tensorflow.org][(tensorflow.org)]].
[fn:keras] Ehemalig externes Frontend von \ac{TF}, seit \ac{TF} 2.0 fester Bestandteil des Frameworks.
[fn:onnx] Universales Format für die Persistierung von \ac{ANN} Modellen.
[fn:loss] Als Loss wird der allgemeine Fehler des Netztes auf einem Datensatz bezeichnet.
Die Funktion die den Loss berechnet wird Loss Funktion benannt.
[fn:gradient] Alle Partiellen Ableitungen einer Funktion $f(x_1, \dots, x_n)$ werden als Gradienten $\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \dots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$ bezeichnet.
Im Kontext des maschinellen Lernens wird mit Gradient oft die Werte des eigentlichen Gradienten zu einem bestimmten Punkt gemeint.
[fn:feedforward] Bei einem Feed Forward Netzwerk fließen die Daten immer linear durch das Netz und werden zu keinem Zeitpunkt an vorherige Schichten geleitet.
[fn:fullyconnected] Bei einem fully connected \ac{ANN} ist jedes Neuron aus Schicht $L$ mit allen Neuronen der Schicht $L+1$ verbunden.
[fn:pdoc] Open Source Projekt zur Generierung von Dokumentation aus Python Modulen. (https://pdoc3.github.io/pdoc/)
[fn:docstring] In Python wird ein Kommentar Block, der eine Funktion, Klasse oder ein Modul beschreibt als Docstring bezeichnet.
[fn:docsify] Framework mit dem Dokumentation in Form einer Web-App aus Markdown Dateien generiert werden kann. (https://docsify.js.org)
[fn:pages]  Eine von GitHub angebotene Dienstleistung Webseiten durch ein Repository bereitstellen zu können.(https://pages.github.com)
[fn:actions] Ein Dienst um automatisiert Tests oder Deployment Operationen durchzuführen. (https://github.com/features/actions)
[fn:pytest] Testing Framework für die Python Programmiersprache. (https://docs.pytest.org/)
[fn:pydocstyle] Tool um Docstrings eines Python Modules zu überprüfen. (https://github.com/PyCQA/pydocstyle)
[fn:styleguide] Ein von Google genutzter Styleguide für Python Projekte. (https://google.github.io/styleguide/pyguide.html)
[fn:pylint] Software um statische Syntax Analyse auf Python Source Code durchzuführen. (https://www.pylint.org)
[fn:model_opt] Toolkit für TensorFlow Modell Optimierungen. (https://github.com/tensorflow/model-optimization)
[fn:numpy] Numpy ist eine Mathematik Python Bibliothek, die von vielen wissenschaftlichen Modulen genutzt wird.
Mathematische Operationen, wie Matrix Multiplikation sind in C implementiert und somit sehr performant. (https://numpy.org)
[fn:hidden] Schichten eines Modells mit denen der Nutzer keine direkte Interaktion hat, werden als hidden bezeichnet.
Beispielsweise wären in einem sequentiellen Modell mit $4$ Schichten, Layer $2$ und $3$ hidden.
[fn:github] https://github.com/sirbubbls/pruning-ba
[fn:evaluation_loss] Unter dem Evaluation-Loss versteht man die Accuracy, die ein Modell auf einem Testdatensatz erzielt.
Dabei wurde das Modell nicht auf dem Datensatz trainiert und kennt diesen somit auch nicht.
Meist ist der Evaluation-Loss die aussagekräftigste Metrik über die Qualität eines KI Modells.
[fn:training_loss] Der Training-Loss eines Modells beschreibt, wie gut die Accuracy das Modells auf dem Trainings-Datensatz ist.
[fn:glow] https://github.com/pytorch/glow
[fn:hyperparameter] Parameter wie ~learning_rate~, die auf das Training Auswirkungen haben können.
[fn:hyperparameter-tuning] Das justieren der Hyperparameter um ein möglichst gutes Modell durch Training zu erhalten.
[fn:pypi] https://pypi.org
[fn:gh_condense] Das GitHub Verzeichnis des unter dieser Arbeit entwickelten Frameworks (https://github.com/sirbubbls/condense)
[fn:pip] Package Installer for Python ~pip~ (https://pypi.org/project/pip/)
[fn:parameter] Als Parameter eines Modells, werden alle Daten bezeichnet, die im Laufe des Trainings durch den Optimierungsalgorithmus angepasst werden.
[fn:_]

bibliography:references.bib
bibliographystyle:apalike
