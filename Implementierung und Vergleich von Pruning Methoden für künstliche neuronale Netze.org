#+TITLE: Implementierung und Vergleich von Pruning Methoden für künstliche neuronale Netze
#+AUTHOR: Lucas Sas Brunschier
#+DESCRIPTION: Bachelor Arbeit
#+DATE: XX.XX.2020
#+LATEX_CLASS: report
#+language: de
#+LATEX_HEADER: \usepackage[ngerman]{babel}
#+LATEX_HEADER: \usepackage{a4wide}
#+LATEX_HEADER: \usepackage[backend=bibtex, style=numeric] {biblatex}
#+LATEX_HEADER: \addbibresource{references.bib}
#+LATEX_HEADER: \usepackage{acronym}
#+STARTUP: showall
#+STARTUP: hideblocks
#+TOC: nil

# Title Page
#+begin_src emacs-lisp :exports results :results none :eval export
  (make-variable-buffer-local 'org-latex-title-command)
  (setq org-latex-title-command (concat
     "\\begin{titlepage}\n"
     "\\begin{center}\n"
     "\\includegraphics[width=5cm]{./resources/haw_logo.jpg}\n"
     "\\vspace{2cm}\n"
     "{\\par \\LARGE Hochschule für angewandte Wissenschaften Landshut}\n"
     "\\vspace{0.6cm}"
     "{\\par \\Large Fakultät Informatik} \\vspace{1.2cm}\n"
     "{\\par \\Huge \\bf Bachelor Thesis} \\vspace{1cm}\n"
     "{\\par \\LARGE %t } \\vspace{1cm}\n"
     "{\\par \\Large \\it von %a} \\vspace{0.2cm}\n"
     "{\\par Matrikel-Nr.: 1088709} \\vspace{1cm} \n"
     "{\\par Abgabedatum: %D} \\vspace{3cm}\n"
     "\\end{center}\n"
     "{\\par Betreuer: Prof. Dr. Mona Riemenschneider}\n"
     "{\\par Zweitkorrektor: Prof. Dr. Abdelmajid Khelil}\n"
     "\\end{titlepage}\n"))
#+end_src

#+BEGIN_ABSTRACT
\begin{abstract}
Diese Arbeit beschreibt Erkenntnisse und den Prozess der Entwicklung des Python Pruning Frameworks Condense (github.com/sirbubbls/condense) für künstliche neuronale Netze.
Dabei werden Methoden von Pruning Techniken und deren Auswirkungen erläutert.
Verschiedenste Design Entscheidungen und Anwendungen des unter dieser Arbeit entwickelten Frameworks erklärt und auf allgemeine Erkenntnisse hingewiesen.
\end{abstract}
#+END_ABSTRACT

#+TOC: tables

# Abbildungsverzeichnis
#+BEGIN_LATEX
\newpage
\listoffigures
\newpage
#+END_LATEX

# Abkürzungsverzeichnis
#+BEGIN_LATEX
\begin{acronym}[Bash]
\acro{ANN}{künstliches neuronales Netz}
\acro{TF}{Tensor Flow}
\acro{ML}{Machine Learning oder zu deutsch maschinellen Lernen}
\acro{GPU}{Graphics Processing Unit}
\acro{TPU}{Tensor Processing Unit}
\end{acronym}
\newpage
#+END_LATEX

* Einleitung
** Neuronale Netze

Bei dem Forschungsbereich der neuronalen Netze handelt es sich um eine Teilmenge des Bereichs der künstlichen Intelligenz.
Genauer lassen sich künstliche neuronale Netze als einen Teil des maschinellen Lernens einordnen.
Die Inspiration für \acp{ANN} kommt ursprünglich aus der Biologie und der dort vorkommenden neuronalen Verbindungen in Nervensystemen von Lebewesen.
Jedoch sind die Forschungsgebiete zu neuronalen Netzen aus der Biologie und der Informatik/Mathematik zum Großteil disjunkt.
cite:10.5555/3086952

#+LABEL: fig:network
#+CAPTION[Diagramm eines künstlichen neuronalen Netzes]: Diagramm eines fully connected \ac{ANN}, mit einem Hidden Layer (hier blau gekennzeichnet).
#+CAPTION: Es ist gut zu erkennen, wie benachbarte Schichten
#+CAPTION: [[https://commons.wikimedia.org/wiki/File:NeuralNetwork.png][[quelle]​]]
#+ATTR_LATEX: :float wrap :width 8cm :center nil
[[./resources/neural_network.png]]

*** Historisches
Obwohl \ac{ANN}'s erst ca. 2008 ihre Blütezeit erreicht haben, ist die zu Grunde liegende Technologie bereits seit
Mitte bis Ende des 20. Jahrhunderts bekannt.
So schuf Frank Rosenblatt im Jahre 1975 das in Abbildung ref:fig:perceptron dargestellte Modell eines Perceptrons cite:werbos1975beyond, eine
mathematische Abstraktion des aus der Biologie bekannten Neuron.
Das Perceptron wird bis heute als Modell für ein alleinstehendes Neuron in einem \ac{ANN} verwendet.
In dem kommenden Kapitel [[perceptron]] wird noch im Detail auf das Modell des Perceptrons eingegangen.
Der Backpropagation Algorithmus, bei dem es sich um eine Implementation der Kettenregel zur automatischen Differenzierung
von Parametern eines \ac{ANN} handelt, wurde im Jahre 1986 publiziert cite:Rumelhart_1986.
Auch heute erweist sich der Backpropagation Algorithmus als eine sehr effiziente Methode die Gradienten[fn:gradient] eines \ac{ANN}'s zu berechnen und
findet beinahe unverändert Einsatz in verschiedensten modernen Deep Learning Frameworks.
Eine häufige Fehlinformation die über den Backpropagation Algorithmus verbreitet wird, ist dieser sei für das "Lernen" des neuronalen Netzes
verantwortlich.
Dies ist inkorrekt, da eigentlich der Gradient Descent Algorithmus cite:Curry_1944 die von dem Backpropagation berechneten Gradienten nutzt um
die Parameter des Netzes so zu manipulieren, dass der Loss[fn:loss] minimiert wird.

*** Strukturelle Beschaffenheit von neuronalen Netzen <<netstruct>>
#+begin_quote
In diesem Kapitel wird ausschließlich der Aufbau eines fully connected[fn:fullyconnected] feed forward[fn:feedforward] neural networks behandelt.
#+end_quote
Ein \ac{ANN} besteht primär aus mehreren Schichten (Layern) $L_1, \dots, L_n$.
Bei dem ersten Layer $L_1$ handelt es sich um den so genannten Input Layer, der für die Aufnahme von Eingabedaten zuständig ist.
Analog fungiert dieser als Eingabe-Interface des neuronalen Netzes für den Anwender des \acp{ANN}.
So erwartet ein Input-Layer mit $8$ Neuronen einen Input Vektor von $8$ Werten.
Schichten $L_2, \dots, L_{n-1}$ werden als Hidden-Layer des neuronalen Netzes bezeichnet, da diese für den Außenstehenden nicht direkt einsichtig sind.
Die letzte Schicht des \acp{ANN}, $L_n$ ist der Output-Layer des Netztes und dient als zweites Interface für den Nutzer.
In Abbildung ref:fig:network lässt sich die Architektur eines einfachen Feed Forward[fn:feedforward] Networks und dessen Layer klar erkennen.
cite:10.5555/3086952

*** Das Perceptron <<perceptron>>
Ein Perceptron ist ein Modell, das eine Reihe von Eingabedaten (Inputs) $a$ auf einen gemeinsamen Output $y$ nach der Form  $\mathbb{R}^n \rightarrow \mathbb{R}$ abbildet.
Die verschiedenen Inputs $a^n$ werden durch Gewichtungen $w^n$ verschieden stark gewichtet, also $a_1 \times w_1 + \dots + a_n \times w_n = y$ oder in Vektorschreibweise $a \times w^T = y$.
In heutigen State of the Art Deep Learning Frameworks enthält das Perceptron zusätzlich noch eine nicht-lineare Komponente in der Form einer Aktivierungsfunktion $\sigma$.
Einige der häufig eingesetzten Aktivierungsfunktionen können in Tabelle ref:tab:aktivierungsfunktion gefunden werden.
In Kapitel ref:activations dieser Arbeit, wird noch im Genaueren auf Zusammenhänge zwischen Aktivierungsfunktion und Sparsity[fn:sparsity] eines \ac{ANN} Modells eingegangen.
cite:Rosenblatt_1958

#+LABEL: tab:aktivierungsfunktion
#+CAPTION[Populäre Aktivierungsfunktionen]: Aktivierungsfunktionen enthalten meist eine nicht-linearität, die nötig ist um neuronale Netze
#+CAPTION: auf nicht lineare Zusammenhänge in Datensätzen trainieren zu können.
| Name                  | Funktion                                                        |
|-----------------------+-----------------------------------------------------------------|
| Logistische Funktion  | $\frac{1}{1+e^t}$                                               |
| Tangens Hyperbolicus  | $\frac{(e^x-e^{-x})}{(e^x+e^{-x})}$                             |
| Rectified Linear Unit | $f(x)= \begin{cases} 0\ for\ x\leq0 \\ x\ for\ x>0 \end{cases}$ |

Ein Perceptron kann statt in einem grafischen Modell visualisiert zu werden auch als eine mathematische Funktion eqref:eq:percept behandelt werden.

\begin{equation}f(a, w)=\sigma(a\times w^T)=y \label{eq:percept}\end{equation}


#+LABEL: fig:perceptron
#+CAPTION[Diagramm eines einfachen Perceptrons]: Abbildung eines einfachen Perceptrons.
#+CAPTION: Es ist gut zu erkennen, wie der Input Vektor des Layers $\begin{pmatrix} x_1 \\ \dots \\ x_m \end{pmatrix}$ und
#+CAPTION: der Weight Vektor $\begin{pmatrix}w_1 \\ \dots \\ w_m \end{pmatrix}$
#+CAPTION: auf die Variable $y$ durch $\begin{pmatrix} x_1 \\ \dots \\ x_m \end{pmatrix} \begin{pmatrix} w_1 \\ \dots \\ w_m \end{pmatrix}^T = y$ abgebildet werden.
#+CAPTION: Der Output des Layers wird durch die Anwendung einer Aktivierungsfunktion auf die Variable $y$ generiert.
#+CAPTION: cite:towardsdatascience
[[./resources/perceptron.png]]

*** Der Datenfluss in einem künstlichen neuronalen Netz
Daten in einem Feed-Forward \ac{ANN} verlaufen immer linear von Input-Layer in Richtung Output-Layer.
Da bereits in Kapitel ref:perceptron auf die Beschaffenheit eines Layers eingegangen wurde, können wir einen Layer $L$ als eine Funktion $f(x)$ betrachten.
Da der jeweilige Output eines Layers $L_i$ als der Input des Layers $L_{i+1}$ dient, können wir ein Netzwerk als eine Verkettung an Funktionen betrachten.
Im Allgemeinen kann dies in der Form eqref:eq:net ausgedrückt werden.

\begin{equation} {f_n(\dots (f_1(x)))=y \ \ \ \label{eq:net} \end{equation}

#+BEGIN_QUOTE
In anderen Layer-Architekturen wie Recurrent oder LSTM ist es durchaus möglich Daten auch an vorherige Layer abzugeben.
Diese Architekturen sind jedoch nicht Teil dieser Arbeit.
#+END_QUOTE
Die Möglichkeit ein \ac{ANN} als eine Verkettung von Funktionen formulieren zu können ist essentiell um Algorithmen wie Backpropagation zur
Differenzierung von Parametern nutzen zu können.

** Einführung in naive Pruning Methoden für künstliche neuronale Netze

Es lässt sich durch Beobachtung der künstlichen neuronalen Netze der letzten Jahre feststellen,
dass die Komplexität und die damit einhergehende Anzahl von Neuronen und deren Verbindungen immer weiter zunehmen. cite:altenberger18:_non_techn_survey_deep_convol
Gleichzeitig werden diese komplexeren und größeren \ac{ANN} Architekturen auch auf schwächeren eingebetteten Geräten eingesetzt.
Dadurch werden Optimierungen an neuronalen Netzen immer relevanter, da dies Inferenz-Zeit und Modellgröße minimieren kann.
Verfahren wie Quantisierung können die Laufzeit und den Speicherverbrauch von \ac{ANN}'s deutlich verbessern, jedoch können auch
Pruning Verfahren massive Verbesserungen versprechen. cite:Frankle2018
Pruning Verfahren versuchen durch das Entfernen von Verbindungen oder auch ganzen Neuronen, die Sparsity eines Modells zu erhöhen.
Weight oder auch Connection Pruning bezeichnet den Vorgang Verbindungen aus einem \ac{ANN} zu entfernen.
Dabei werden die Verbindungen eliminiert, also mit $0$ gewichtet. Die ist in Abbildung ref:fig:naiveweightpruning dargestellt.
Die ausgewählten Verbindungen oder Neuronen werden durch eine Heuristik bestimmt, eine Heuristik könnte beispielsweise die niedrigst gewichteten Verbindungen sein.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import sys
sys.path.append('./condense')
import condense
import matplotlib.pyplot as plt

model = keras.models.load_model('./resources/models/iris.h5')
layer = 1

plt.figure(figsize=(10, 3))

plt.subplot(121)
plt.imshow(abs(model.get_weights()[layer*2]), cmap='inferno', vmax=0.4)
plt.title('Ungepunter Kernel eines Layers')
model.build()
pruned = [condense.optimizer.layer_operations.weight_prune.w_prune_layer(weight, (perc := 0.85)) for weight in model.get_weights()]
model.set_weights(pruned)
plt.subplot(122)
plt.imshow(abs(model.get_weights()[layer*2]), cmap='inferno', vmax=0.4)
plt.title(f'Kernel mit {perc*100}% sparsity')
plt.tight_layout()
plt.savefig('./resources/plots/simple-pruning.png')
return './resources/plots/simple-pruning.png'
#+END_SRC

#+LABEL: fig:naiveweightpruning
#+CAPTION[Visualisierung von Weight Pruning]:
#+CAPTION: In diesem hier dargestellten Dense Layers eines neuronalen Netzes, wurde die Sparsity des Modells durch Pruning der Verbindungen auf $85\%$ erhöht.
#+CAPTION: Jeder Pixel repräsentiert dabei eine Verbindung von Neuron zu Neuron in einem \ac{ANN}.
#+CAPTION: Es ist gut zu beobachten, wie nur leicht gewichtete Verbindungen durch Pruning deaktiviert werden, hier durch schwarze Pixel zu erkennen.
#+CAPTION: Bei dem Netz handelt es sich um ein durch TensorFlow 2.0 trainiertes Modell. Bei dem Training wurde der Iris Datensatz genutzt. cite:fisher36lda
#+RESULTS[f1da55ec2e6ff2cbe9f43d6f7fa78e44cbe49ee5]:
[[file:./resources/plots/simple-pruning.png]]

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import sys
sys.path.append('./condense')
import condense
import matplotlib.pyplot as plt

model = keras.models.load_model('./resources/models/iris.h5')
layer = 1

plt.figure(figsize=(10, 3))
model.build()
plt.subplot(121)
X = model.get_weights()[layer*2].flatten()
X = X[X != 0]
plt.hist(X, density=True, rwidth=.5, bins=70)
plt.title('Ungepunter Kernel eines Layers')
pruned = [condense.optimizer.layer_operations.weight_prune.w_prune_layer(weight, (perc := 0.85)) for weight in model.get_weights()]
model.set_weights(pruned)
plt.subplot(122)
X = model.get_weights()[layer*2].flatten()
X = X[X != 0]
plt.hist(X, rwidth=.5, bins=70)
plt.title(f'Kernel mit {perc * 100}% sparsity')
plt.tight_layout()
plt.savefig('./resources/plots/simple-pruning-hist.png')
return './resources/plots/simple-pruning-hist.png'
#+END_SRC

#+CAPTION: Histogramm der Weight Verteilung nach Pruning
#+RESULTS[baeb75d12efcc3d62cab14ad65b7b589a832050b]:
[[file:./resources/plots/simple-pruning-hist.png]]

** Heuristiken zur Bestimmung von zu prunenden Parametern
Es gibt verschiedenste Heuristiken um Parameter zu bestimmen, die wohl am nächstliegendsten sind Unit- und Weight-Pruning.
Die Weight-Pruning Heuristik wählt jeweils die am niedrigst gewichtetsten Gewichtungen eines Layers oder des ganzen Modells aus.
In Abbildung ref:fig:naiveweightpruning ist der in der Grafik visualisierte Layer Kernel durch einfaches Weight-Pruning auf $85\%$ Sparsity optimiert worden.
Die Auswahl der Verbindungen (visualisiert durch jeweils einen Pixel) folgt dabei keinem spezifischen Muster, da sich die Heuristik ausschließlich
auf die absoluten Gewichtung jeder Verbindung bezieht.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import sys
import matplotlib.pyplot as plt
sys.path.append('./condense')
from condense.optimizer.layer_operations.unit_prune import u_prune_layer

layer= 1
model = keras.models.load_model('./resources/models/iris.h5')
unit_pruned = u_prune_layer(model.get_weights()[0::2][layer], (t := .4))
plt.figure(figsize=(10, 3))
plt.subplot(121)
plt.title('Weights des zu prunenden Layers')
plt.imshow(abs(model.get_weights()[0::2][layer]), cmap='inferno', vmin=0)
plt.subplot(122)
plt.title(f'Weights des geprunten Layers ({t*100}% Sparsity)')
plt.imshow(abs(unit_pruned), cmap='inferno', vmin=0)
plt.tight_layout()
plt.savefig('./resources/plots/iris-unit-pruning.png')
return './resources/plots/iris-unit-pruning.png'
#+END_SRC

#+LABEL: fig:naiveunitpruning
#+CAPTION[Visualisierung von Unit Pruning]: Als Vergleich zum in Abbildung ref:fig:naiveweightpruning gezeigten Weight-Pruning eine Visualisierung von Unit(Neuron)-Pruning.
#+CAPTION: Wie auch in Abbildung ref:fig:naiveweightpruning entspricht jeder Pixel einer Verbindung in einem neuronalen Netz.
#+CAPTION: Vertikale angeordnete Pixel entsprechen dem Gewichtungs-Vektor eines einzelnen Neuronen.
#+RESULTS[e7ab1be8e0a42dd70c420d41d96c2a07c8f51b4c]:
[[file:./resources/plots/iris-unit-pruning.png]]

Etwas anders funktioniert Neuron/Unit-Pruning, dabei wird nicht jede Gewichtung einzeln betrachtet, sondern die Summe aller Gewichtungen $w$ eines Neurons $V$ also
$$
\text{Gewichtung eines Neurons $V$}=\sum_{n=0}^V\ w_n.
$$
Das unterste $p\%$ aller Neuronen in einer Schicht, werden daraufhin aus dem Modell eliminiert. D.h. alle Gewichtungen der Neuronen werden auf $0$ gesetzt.
Dies lässt sich in Abbildung ref:fig:naiveunitpruning, durch die schwarzen (eliminierten) vertikalen Linien in der Gewichtungs-Matrix gut erkennen.
Unit-Pruning bietet sowohl Vorteile als auch Nachteile im Vergleich zu Weight-Pruning.
Zum einen werden eventuell wichtige Verbindungen für das Modell aufgrund von Kollateralschaden eliminiert.
Als Szenario wäre zum Beispiel denkbar, dass ein Neuron alle Input Variablen als schwach gewichtet sieht, jedoch auf gerade einen oder zwei sehr sensibel reagiert.
Eventuell kann gerade diese Reaktion auf diesen Input sehr wichtig für nachfolgende Schichten sein.
Durch Unit Pruning würde dieses Neuron vermutlich aus dem Modell gestrichen werden, da die Summe aller Gewichtungen in diesem Neuron nicht sehr signifikant sein wird.
Weight-Pruning hingegen berücksichtigt diese eine sehr aktive Verbindung und wird nur auch tatsächlich schwache Gewichtungen streichen.
Die Frage ob es überhaupt wünschenswert ist Neuronen in einem \ac{ANN} zu behalten, die so sensibel auf bestimmte Inputs reagieren steht natürlich auch noch im Raum.

Einen enormen Vorteil, den Unit-Pruning mit sich bringt, ist die einfache Nutzung der Sparsity (genaueres zu diesem Thema in Kapitel ref:kapit)
die durch Unit-Pruning erzeugt wird. So ist es recht einfach möglich das Neuron nicht nur in der Weight-Matrix mit $0$ zu füllen, sondern dieses komplett
aus der Matrix (oder Tensor) zu entfernen. So wird die Weight Matrix einer Schicht mit $12$ Neuronen und der Dimensionen $(12,6)$ (also $72$ Parameter)
auf $(12, 5)$ (also $60$ Parameter) reduziert.
Somit lässt sich durch die Entfernung eines Neurons direkt ca. $16\%$ Speicher sparen.
Die Sparsity die durch Weight-Pruning erzeugt wird, lässt sich sehr schwer nutzen, da es nicht möglich ist aus Matrizen oder Tensoren nur einzelne Werte zu eliminieren.
Jede Dimension muss immer die gleiche Anzahl an Elementen enthalten.

** Verlust von Accuracy durch Pruning
Natürlich ist in den meisten Fällen ein Verlust von Genauigkeit des Netzes zu erwarten, wie in Abbildung ref:fig:naive-pruning-loss zu sehen ist.
Jedoch kann durch empirisches Beobachten festgestellt werden, dass Pruning Eigenschaften eines Regularizers aufweist.
So verschlechtert sich meist der Training-Loss[fn:training_loss] jedoch verbessert sich im Kontrast der Evaluation-Loss[fn:evaluation_loss] meist erheblich. (siehe Zitat ref:quo:regular).

#+LABEL: quo:regular
#+begin_quote
"Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error.
These strategies are known collectively as regularization." cite:10.5555/3086952

/- Ian Goodfellow, Yoshua Bengio and Aaron Courville/
#+end_quote

Jedoch werden im weiteren Verlauf dieser Arbeit auch Methoden zur Kompensierung dieses Effekts behandelt.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import numpy as np
import sys
sys.path.append('condense')
from scripts import calculate_model_sparsity
from condense.optimizer.layer_operations.weight_prune import w_prune_layer
from copy import deepcopy
from random import choice

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
dataset = list(tfds.as_numpy(ds))
train = dataset[:int(len(dataset)*0.3)]
test = dataset[int(len(dataset)*0.7):]

def generator(batch_size, dataset):
    while True:
        X, y = [], []
        for _ in range(batch_size):
            _X, _y = choice(dataset)
            X.append(_X)
            y.append(_y)
        X, y = np.array(X), np.array(y)
        yield X.reshape(batch_size,4), keras.utils.to_categorical(y, 3).reshape(batch_size,3)

gen = generator(100, train)
eva = generator(100, test)

x = []
data = []
model = keras.models.load_model('./resources/models/iris.h5')
baseline = model.evaluate(eva, steps=10)
original_weights = deepcopy(model.get_weights())

for i in (X := np.linspace(0.1, 0.95, 25)):
    model.set_weights(original_weights)
    pruned_weights = [w_prune_layer(layer, i) for layer in model.get_weights()]
    pruned_weights[-1] = model.get_weights()[-1]
    model.set_weights(pruned_weights)
    x.append(calculate_model_sparsity(model.get_weights()))
    data.append(model.evaluate(eva, steps=10))

plt.subplots(figsize=(6, 3))
plt.plot(X*100, np.array(data), label="Eval Loss")
plt.plot(X*100, [baseline]*len(X), label="Base Eval Loss", ls='--')
plt.legend()
plt.xlabel('Model Sparsity in %')
plt.ylabel('Model Loss')
plt.grid()
plt.tight_layout()
plt.savefig('./resources/plots/iris-accuracy.png')
return './resources/plots/iris-accuracy.png'
#+END_SRC

#+LABEL: fig:naive-pruning-loss
#+CAPTION[Verfall von Genauigkeit mit zunehmend aggressiverem Pruning]:
#+CAPTION: In dieser Grafik wird das identische Modell aus Abbildung ref:fig:naiveweightpruning & ref:fig:naiveneuronpruning durch immer aggressiveres Weight-Pruning optimiert.
#+CAPTION: Wie sich beobachten lässt, verschlechtert sich die Genauigkeit mit zunehmender Stärke des Prunings rapide.
#+ATTR_LATEX: :float wrap :width 8cm :center nil
#+RESULTS[e8ea95281a7861c704c0a4b58c5c1163257e10d3]:
[[file:./resources/plots/iris-accuracy.png]]

** Industrierelevanz
Pruning von künstlichen neuronalen Netzen bietet vielen Unternehmen die Möglichkeit Optimierungen an schon bestehenden \ac{ANN} Modellen vorzunehmen.
Diese Optimierungen können unter Umständen ermöglichen komplexere Modelle auf schwächeren Computern zu nutzen.
Beispielsweise eingebettete Geräte können dabei effizienter Daten durch neuronale Netze auswerten.
Besonders in Situationen in denen das Modell möglichst schnell ein Prognose abgeben soll, wie beispielsweise bei Teilen von
selbständig fahrenden Autos bietet Pruning Chancen auf enorme Verbesserungen.
Zudem bietet Pruning eine Möglichkeit, \ac{ANN} Modelle ohne signifikante Einbußen von Genauigkeit zu optimieren. cite:Frankle2018
Dies sollte Pruning Methoden auf deutlich mehr \ac{ANN} Modellen einsetzbar machen.

** Ziel dieser Arbeit <<ziel>>
*** Erstellung eines Pruning Frameworks <<ziel_framework>>
Ziel dieser Arbeit ist es primär ein Python Framework zu entwickeln, das mehrere verschiedene Typen von Pruning Methoden implementieren soll.
Ein wichtiger Fokus sollte bei der Architektur des Framework sein, dies in Zukunft möglichst einfach erweitern zu können.
Dokumentation der verschiedenen Module ist aus diesem Grund sehr wichtig und sollte im Laufe der Arbeit auch immer aktualisiert werden.
Bei dem Design der Nutzer-Schnittstellen sollte auf eine möglichst einfachere und saubere Architektur geachtet werden,
da auch Nutzer ohne ausgiebige Python Erfahrung im Stande sein sollte die Tools dieses Frameworks zu nutzen.

#+begin_src mermaid :file resources/plots/pruning-framework.png :theme forest :background transparent
graph LR
    input(Input Model) --> interface(High Level Interface)
    interface --> parser(Model Parser)
    pruning(Pruning Engine) --> output(Pruned Model)
    parser --> pruning
#+end_src

#+LABEL: fig:rough-project-structure
#+CAPTION[Pruning Framework Konzept]: Der hier gezeigte Graph soll das grobe Konzept, des im Laufe dieser Arbeit entstehenden Pruning Frameworks zeigen.
#+RESULTS:
[[file:resources/plots/pruning-framework.png]]

Zudem sollte das Framework kompatibel mit aktueller Deep Learning Software und deren Formate kompatibel sein.
Kompatibilität mit \ac{TF} [fn:tensorflow]/Keras[fn:keras] steht bei diesem Projekt im Vordergrund, da auch intern \ac{TF} für Trainings-Operationen genutzt wird.
Optional sollte auch die Möglichkeit bestehen ein Modell in dem ONNX[fn:onnx] Format zu exportieren, um auch Kompatibilität mit anderen Frameworks sicherzustellen.

*** Erkenntnisse über Pruning Methoden
Zudem sollte diese Arbeit durch die Forschung an diversen Pruning Methoden auch neue allgemeine Erkenntnisse hervorbringen.
Alle Resultate dieser Arbeit sollten klar nachvollziehbar und durch das Lesen dieser Arbeit erkenntlich sein.
Um Lesern die Resultate dieser Arbeit möglichst nachvollziehbar zu gestalten, werden alle nötigen Dateien dieser Arbeit in Form eines
GitHub Repositories[fn:github] veröffentlicht.
Der Quellcode um alle Grafiken/Diagramme und Resultate dieser Arbeit zu erzeugen ist dort zu finden.

* Methodik
** Erstellung des Frameworks
Wie bereits in Kapitel ref:ziel_framework eingegangen wurde, sollte bei der Erstellung des Frameworks ein
großer Fokus auf die zukünftige Erweiterbarkeit liegen.
Aus diesem Grund wird im Besonderen auf die Architektur, Tests und der Dokumentation sehr viel Wert gelegt.
Zudem ist das gesamte Projekt als öffentliches GitHub Repository angelegt.
Dementsprechend handelt es sich bei dem Framework auch um ein Open Source Projekt.
Dies soll eventuell in Zukunft zu einer Weiterentwicklung des Projektes führen.

*** Wahl der Sprache & Frameworks
Python 3 lag als primäre Programmiersprache nahe, da diese sehr weit in der KI/ML Gemeinschaft verbreitet ist.
So basieren die meisten Frameworks für neuronale Netze wie Theano, Tensorflow oder Torch auf Python oder einer Implementation in C/C++
die mit der Hilfe von der CPython Bridge angesprochen werden kann.
Somit ist eine sehr gute Leistung trotz einfacher API's erreichbar.
Zudem können in der weiteren Entwicklung dieses Projekt, gewisse Module auf eine performantere Sprache wie C/C++ ausgelagert werden.
Jedoch sind die meisten rechenaufwendigen Opertionen wie Matrix-Operationen bereits in Frameworks wie ~numpy~[fn:numpy] bereits implementiert.

Eine noch effizienter Methode, die sich besonders bei neuronalen Netzen als eine enorme Leistungssteigerung erwiesen hat,
ist die Nutzung von \acp{GPU} oder \acp{TPUS} um einen hohen Grad von Parallelisierung erreichen zu können.
Das Framework ~TensorFlow~[fn:tensorflow] stellt Implementationen von auf \ac{GPU} durchführbaren mathematischen Operationen
in Form eines Python Interfaces bereit.
Besonders Matrix Operationen lassen sich meist bestens parallelisieren und Deep Learning besteht zum Großteil aus genau diesen.

*** Architektur
Die Architektur sollte für den Nutzer so nachvollziehbar wie möglich gestaltet werden.
So sollte es einfach sein Änderungen in Form von collaborative Programming im weiteren Verlauf dieses Projekts vorzunehmen.

*** Dokumentation

**** Allgemeine Dokumentation des Projekts

Durch GitHub Pages[fn:pages] und dem Tool Docsify[fn:docsify] ist es sehr einfach möglich eine ausgesprochen zugängliche Dokumentation
bzw. Landing Page für das Projekt zu generieren.
Der Inhalt dieser Dokumentation ist manuell erstellt und soll dem Benutzer nur einen groben Überblick über die wichtigsten Aspekte des Frameworks geben.
Detailliertere Informationen zu internen Schnittstellen können jedoch trotzdem sehr einfach über die Modul Dokumentation aus Unterpunkt ref:pdoc eingesehen werden.

**** Automatisierte Generierung von Dokumentation aus Source Code des Projektes <<pdoc>>

Durch das Tool pdoc3[fn:pdoc] kann aus dem Source Code eines Python Modules und dessen Docstrings[fn:docstring] eine Dokumentation in Form einer
HTML Seite generiert werden.
Diese ist in die Allgemeine Dokumentation des Projekts direkt eingebettet und erfordert keine separate Website.
Da bei der Generierung dieser Dokumentation keine weitere manuelle Arbeit geleistet werden muss, kann diese ohne weitere Umstände automatisiert
über GitHub Actions[fn:actions] realisiert werden.
So wird beispielsweise bei einer Änderung des Modules auf dem ~master~ Branch des Projekts ein Script ausgelöst, die eine aktualisierte Dokumentation auf
der öffentlichen Webseite zur Verfügung stellt.
Natürlich koaliert die Qualität der generierten Dokumentation direkt mit der Qualität der im Source Code verfassten Docstrings,
somit ist zudem sicherzustellen, dass auch hier ein gewisser Qualitätsstandart einzuhalten ist.
Wie dies innerhalb dieses Projekts implementiert wurde wird in Kapitel ref:tests Punkt ref:docstyle_tests genauer erläutert.

*** Tests <<tests>>

**** Unit Tests

Um sicherzustellen, dass die Qualität der Software einen gewissen Standard erfüllt, sind Unit Tests mit Sicherheit ein essentieller Bestandteil dieses Projekts.
Dazu wurde das sehr weit verbreitete Testing Framework pytest[fn:pytest] genutzt.
Zusätzlich werden Daten über die Test-Coverage der Tests Dank des pytest-cov plugins für pytest generiert.

**** Linting

Um eine ästhetisch ansprechende Formatierung des Quellcodes im Laufe des Projekts beizubehalten.
Durch das Tool pylint wird auch dies automatisiert möglich mit der Hilfe von GitHub Actions möglich.
Einige der wichtigsten von der Software überprüften Punkte sind:
- unnötige ~import~ Statements
- korrekte Variablennamen
- Zeichen per Zeile
- Zeilen-Abstände

**** Docstyle Tests <<docstyle_tests>>

Um auch wichtige Teile wie die Dokumentation von Funktionen nicht im Laufe des Projekts zu vernachlässigen, wurde das Tool pydocstyle[fn:pydocstyle] genutzt um auch
Docstrings auf Korrektheit zu überprüfen.
Als Style der Docstrings wurde sich auf den von Google genutzten Styleguide[fn:styleguide] berufen.
Durch diese Methodik, müssen alle Module, Klassen und Funktionen über Docstrings verfügen, da sie sonst nicht auf einen der nicht-feature branches des Repositories gepullt werden können.
Dadurch lässt sich eine enorm detaillierte Dokumention aller öffentlichen Schnittstellen automatisiert generieren.

** Datensätze <<datensatz>>
Die meisten Datensätze, die in dieser Arbeit verwendet wurden, wurden durch das Python 3 Modul ~tensorflow_datasets~ cite:TFDS bezogen.
In Tabelle ref:tab:dataset sind alle in dieser Arbeit verwendeten Datensätze gelistet.

#+LABEL: tab:dataset
#+CAPTION[In dieser Arbeit verwendeten Datensätze]: Eine Liste von, in dieser Arbeit vewendeten Datensätzen.
| Datensatz                                            | Beschreibung | Quelle                                       |
|------------------------------------------------------+--------------+----------------------------------------------|
| Iris Dataset cite:fisher36lda                        |              | https://archive.ics.uci.edu/ml/datasets/iris |
| ImageNet cite:imagenet_cvpr09                        |              | http://www.image-net.org                     |
| ImageNet V2                                          |              | https://github.com/modestyachts/ImageNetV2   |
| MNIST cite:lecun-gradientbased-learning-applied-1998 |              | http://yann.lecun.com/exdb/mnist/            |

* Implementierung

** Sparsity Mask <<sparsity_mask>>
Der Begriff Sparsity Mask/Tensor bezieht sich in dieser Arbeit auf einen Binären Tensor der definiert, welche Felder aus einem Weights Tenor eine $0$ enthalten.
In anderen Worten maskiert der Sparsity Tensor die Parameter Tensoren eines Modells.
Die Sparsity Mask ist ein essentieller Bestandteil für fast jede Pruning Methode und bei der Implementierung dreht es sich primär darum,
diese Maske effizient anzuwenden und zu generieren.
Deswegen ist die Klärung dieses Begriffs auch sehr wichtig für kommende Kapitel dieser Arbeit.
Auswirkungen der Sparsity Mask auf ein Array wird in Abbildung ref:fig:simplesparsity dargestellt.

#+BEGIN_SRC python :exports results :results file :cache yes
import numpy as np
import matplotlib.pyplot as plt

a = np.random.rand(10, 20)
m = np.random.rand(10, 20) < 0.4

plt.figure(figsize=(10, 2))
plt.subplot(131)
plt.imshow(a)
plt.title('Ursprüngliches Array $a$')
plt.subplot(132)
plt.imshow(m*-1, cmap='binary')
plt.title('Sparsity Mask $m$')
plt.subplot(133)
plt.imshow(a*m)
plt.title('Maske auf Array angewandt $a \\times m$')
plt.tight_layout()
plt.savefig('resources/plots/masking.png')
return 'resources/plots/masking.png'
#+END_SRC

#+LABEL: fig:simplesparsity
#+CAPTION[Anwendung von einer Sparsity Mask auf ein einfaches Array]:
#+CAPTION: In dieser Grafik wird jedes Feld durch weiß ($1$) und schwarz ($0$) binär visualisiert.
#+CAPTION: Die Multiplikation der Maske $m$ mit dem Array $a$ resultiert in der durch $m$ maskierte Version des ursprünglichen Arrays $a$.
#+RESULTS[b489619f2ae1bf26670eaaa816a599d772d79015]:
[[file:resources/plots/masking.png]]

Bei der Implementierung der Pruning Operation bieten sich primär zwei Zeitpunkte während des Trainings an,
an denen dies Sparsity Maske angewandt werden kann.
1. Nach jedem Schritt des Optimizers, werden die maskierten Felder wieder auf $0$ zurückgesetzt.
2. Die Parameter werden zur Initialisierung einmal maskiert.
   Daraufhin werden ausschließlich die Gradienten maskiert um somit die Parameter Sparsity beizubehalten.

~condense~ verwendet beide dieser Ansätze um die gewünschte Sparsity eines Modells zu erzielen.

** ~condense~ Modul
Wie in Kapitel ref:ziel erwähnt ist auch die Erstellung eines Python Frameworks ein großer Teil dieser Arbeit.
Das englische Wort /condense/ (zu Deutsch /kondensieren/ oder auch /verdichten/) beschreibt die Operation des Prunings von neuronalen Netzen sehr gut,
da Teile des Netzes gelöscht, bzw. verdichtet werden.

*** Struktur
Eine klare und einfach verständliche Strukturierung der Schnittstellen ist (wie in Kapitel ref:ziel_framework beschrieben) ein wichtiges
Ziel dieser Arbeit.
Die grobe Struktur des ~condense~ Moduls wird in Abbildung ref:fig:condense_structure dargestellt.
In folgenden Kapiteln werden die einzelnen Untermodule dieses Projekts genauer erläutert.

#+BEGIN_SRC mermaid :file resources/plots/condense-module.png :theme forest :background transparent :cache yes
graph TD
    condense --- keras
    condense --- torch
    condense --- optimizer
    condense --- utils
    condense -.- compressor
    condense --- o_shot(one_shot)
    keras --- prune_model(prune_model)
    utils --- model_utils
    utils --- layer_utils
    optimizer --- one_shot(one_shot)
#+END_SRC

#+LABEL: fig:condense_structure
#+CAPTION[Condense Modul Architektur]: Diagram des Python Moduls ~condense~ und dessen Untermodule.
#+CAPTION: Funktionen/Methoden sind hierbei mit abgerundeten Kanten visualisiert und Module/Klassen mit scharfen Kanten.
#+RESULTS[599357935a3a84530721237a8f915f8496c0ccec]:
[[file:resources/plots/condense-module.png]]

**** Keras Kompatibilitäts-Modul (~condense.keras~)

Um eine direkte Integration in das Keras Machine Learning Framework bieten zu können, existiert im Umfang dieses Moduls ein Keras/\ac{TF} kompatibles Sub-Module.
Dieses Nutzt im Vergleich zum im Punkt ref:optimization_module beschriebenen Optimierungs-Modul keine ~numpy~[fn:numpy] Arrays sondern \ac{TF} Tensoren.
Die Implementation von Pruning Methoden als TensorFlow Tensoren bietet die Möglichkeit diese auf \acp{GPU} ausführen zu können,
was einen enormen Leistungszuwachs mit sich bringt.
Jedoch verursacht die Arbeit mit \ac{TF} Tensoren auch deutlich mehr Aufwand bei der Implementation.
In Kapitel ref:keras_module wird noch genauer auf die Methodik hinter diesem Modul eingegangen.

**** Torch (PyTorch) Kompatibilitäts-Modul (~condense.torch~)

Das zweite von ~condense~ unterstützte Framework ist PyTorch (o.a. Torch).
Ähnlich wie das Keras Kompatibilitäts-Modul, werden Network Operationen dadurch auf GPU's oder TPU's durchgeführt.
In Kapitel ref:pytorch wird noch näher auf die PyTorch spezifische Implementation eingegangen.

**** Optimierungs-Modul (~condense.optimizer~) <<optimization_module>>

Dieses Modul stellt Pruning Implementationen für ~numpy~[fn:numpy] Arrays bereit.
Bei dir Implementierung wurde besonders auf die Utilisation von ~numpy~ Methoden Wert gelegt um best mögliche Leistung zu erreichen.
Teilweise werden auch Sequenzielle ~keras~ Modelle von der API unterstützt,
wobei für nähere Informationen auf die Dokumentation verwiesen werden sollte: https://sirbubbls.github.io/condense/#/pdoc/condense/optimizer/index.html.

**** Utils (~condense.utils~)

Dieses Submodule soll Nutzern eine Hand voll nützlicher Tools bereitstellen, beispielsweise die Berechnung der Sparsity eines Layers.

**** Modul zur Kompression von optimierten Modellen (~condense.compressor~)

Um Modelle in einer effizienten Form zu speichern und in den Arbeitsspeicher laden zu können muss dieses auch komprimiert werden.
In Zukunft soll dies in Form dieses Sub-Modules realisiert werden, jedoch ist dies nicht Ziel dieser Arbeit.
*** Installation
Natürlich ist es enorm wichtig, wie potentielle Nutzer das ~condense~ Modul installieren können.
Da das Projekt unter einem öffentlichen GitHub Repository[fn:gh_condense] entwickelt wird, können Nutzer durch einfaches herunterladen des
Repositories sich Zugang zu Implementationen gewähren.
Durch das Python Modul ~pip~[fn:pip], ein Module Management System für Python, ist es einfach möglich Module aus dem öffentlichen PyPi[fn:pypi]
Repository herunterladen und zu installieren.
Module und deren Versionen müssen jeweils bei einer neuen Version neu eingereicht werden.
Dies ist durch GitHub Actions Framework automatisierbar indem durch Änderungen des Master Branches eine Veröffentlichung
ausgelöst wird, vorausgesetzt es werden alle Tests des Moduls erfolgreich ausgeführt.

#+begin_quote
Eine Installation durch ~pip~ ist durch das Kommando: ~pip install condense~ möglich.
#+end_quote

** One-Shot Pruning <<one-shot>>
Die wohl trivialste Methode Parameter eines \ac{ANN} Modells prunen, ist ein sogenanntes One-Shot Pruning Verfahren zu anzuwenden.
Bei diesem Typ von Pruning werden keine Refitting (Kapitel ref:refitting) Operationen angewandt, also auch keine Datensätze benötigt.
Nachteile sind jedoch eine deutlich verlustbehafteten Optimierung im Kontrast zu iterativen Pruning (Kapitel ref:iterative).
Die Implementierung von One-Shot Pruning gestaltet sich im Vergleich mit iterativen Pruning als trivial.
Es wird eine Maskierungs-Funktion benötigt, um die zu prunenden Felder der Matrix zu ermitteln.
Eine einfache Maskierungs-Funktion wäre beispielsweise die Auswahl aller Felder, die unter eine festgelegten Threshold $t$ liegen.
Die durch diese Funktion resultierende Matrix $S$ wird auch Sparsity-Mask (siehe Kapitel ref:sparsity_mask) genannt.
$$
W = \begin{pmatrix} 0.4 && 2 \\ 1.4 && 0\end{pmatrix} \text{ mit } t = 1.0 \Rightarrow S = \begin{pmatrix} 0 && 1 \\ 1 && 0 \end{pmatrix}
$$
In Abbildung ref:fig:oneshot ist ein derartiges triviales Pruning von Feldern aus einer Matrix in visueller Form dargestellt.

#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
w = np.random.random((50,100))
t = 0.5
plt.figure(figsize=(15, 3))
plt.subplot(1, 4, 1)
plt.imshow(w, vmin=0.0, vmax=.4)
plt.title('Source Matrix')

plt.subplot(1, 4, 2)
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=.4)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.subplot(1, 4, 3)
t += .4
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=.4)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.subplot(1, 4, 4)
t += .4
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=.4)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.tight_layout()
plt.savefig('resources/plots/one-shot-random.png')
return 'resources/plots/one-shot-random.png'
#+END_SRC

#+LABEL: fig:oneshot
#+CAPTION[Visualisierung von One Shot Pruning]: Visualisierung von One Shot Pruning mit verschiedenen Thresholds $t$.
#+CAPTION: Die das Pruning durchführende Operation ~w[w < t] = 0~ setzt alle sich in der Maske ~w < t~ befindenden Felder der Matrix auf $0$.
#+CAPTION: Die Maske ~w < t~ kann durch eine beliebige (Maskierungs-)Funktion ersetzt werden.
#+RESULTS[21ff15ac885246e704d3550b609f135c8edfd96e]:
[[file:resources/plots/one-shot-random.png]]

Das Modul ~condense~ bietet dabei mehrere verschiedene Implementationen und Schnittstellen für Nutzer um Datenstrukturen zu prunen.
Die universellste Methode One-Shot Pruning auf Tensoren durchzuführen, ist die ~condense.optimizer.layer_operations~ API zu nutzen.
Diese erlaubt Pruning auf ~numpy.ndarrays~ durchzuführen.

#+BEGIN_SRC
model = keras.models.load_model('...')
pruned = condense.optimizer.one_shot(model, 0.75)
#+END_SRC

** Iteratives Pruning <<iterative>>
Das in Kapitel ref:one-shot angesprochene One-Shot Pruning-Verfahren, verursacht einen signifikanten Verlust von Modell Accuracy.
Jedoch lässt sich  One-Shot Pruning einfach in ein iteratives Pruning-Verfahren umwandeln.

*** Refitting <<refitting>>
Nach dem Pruning eines Modells kann dieses erneut trainiert werden, somit kann sich das Modell an die manipulierten Parameter adjustieren.
Es ist jedoch zwingend notwendig die Sparsity Mask (Kapitel ref:sparsity_mask) der jeweiligen Layer zu speichern,
da diese auf die Parameter nach dem Training angewandt werden muss cite:Frankle2018.
Was passiert wenn dies nicht gemacht wird, zeigt Abbildung ref:fig:refitting.

#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
import sys
import keras
import tensorflow_datasets as tfds
sys.path.append('./condense')
import condense

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
ds = ds.repeat()

model = keras.models.load_model('resources/models/iris.h5')
layer = 2
pruning_intensity = 0.8

plt.figure(1, figsize=(10, 5))
plt.subplot(221)

plt.imshow(model.get_weights()[layer], vmax=.4, vmin=.0)
plt.savefig('resources/plots/iterative-1.png')
plt.title('Weight Matrix eines trainierten Layers \n ohne Pruning Optimierungen')

plt.subplot(222)

plt.imshow((pruned := condense.one_shot(model, pruning_intensity)).get_weights()[layer], vmax=.4, vmin=.0)
plt.savefig('resources/plots/iterative-1.png')
plt.title(f'Weight Matrix des optimierten Layers mit {round(condense.utils.model_utils.calc_model_sparsity(pruned) * 100, 2)}% \n Modell Sparsity')

pre_training = pruned.get_weights()[layer]

# Retraining
plt.subplot(223)

pruned.compile('adam', 'mse')
pruned.fit(ds.batch(30), epochs=15, steps_per_epoch=50)
plt.imshow(pruned.get_weights()[layer], vmin=.0, vmax=.4)
plt.title(f'Optimiertes Modell nach Refitting ({round(condense.utils.model_utils.calc_model_sparsity(pruned) * 100, 2)}% Modell Sparsity)')

# Diff plot
plt.subplot(224)
plt.imshow(np.abs(pre_training - pruned.get_weights()[layer]), cmap='binary')
plt.title('Änderungen der Gewichtungen durch Refitting')

plt.tight_layout()
plt.savefig('resources/plots/iterative-1.png')

return 'resources/plots/iterative-1.png'
#+END_SRC

#+LABEL: fig:refitting
#+CAPTION[Weights eines Layers nach refitting]: In dieser Grafik wird der Layer eines Modells,
#+CAPTION: durch naives Weight und Neuron Pruning optimiert und anschließend durch Refitting erneut trainiert.
#+CAPTION: Durch das Refitting ohne Sparsity Maske wird die Sparsity des Modells von ca. $77\%$ auf fast $0\%$ geändert.
#+CAPTION: Änderungen die durch das Refitting verursacht werden, sind in der rechts-unteren Abbildung visualisiert.
#+RESULTS[585ffcab6c0d18d41ee6c9266eae6f62449f73e8]:
[[file:resources/plots/iterative-1.png]]

** Implementierung der Lottery Ticket Hypothesis <<lottery_ticket>>
Der durch Jonathan Frankle und Michael Carbin publizierte Artikel
"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"cite:Frankle2018,
beschreibt eine Methode ein bereits gepruntes Modell zu trainieren um ein schnelleres Training zu ermöglichen.

Dabei wird folgende Prozedur vorgeschlagen:
1. Initialisieren und Speichern aller Parameter des Modells
2. Trainieren des Modells
3. Prunen dieses Modells und Speichern der Layer Masken
4. Reinitialisieren des Modells durch die in Schritt 1 gespeicherten Parameter
5. Anwendung der durch das Prunen generierten Sparsity Masken
6. Erneutes trainieren des Modells unter Berücksichtigung der Sparsity Maske

Durch dieses Verfahren ist es möglich nicht nur kleinere Modelle schneller und effizienter zu trainieren, sondern auch eine allgemein bessere
Test-Accuracy des Netzes zu erreichen.
Besonders gefördert wird dadurch das Generalisierungspotential des Netzes.

*** Beispiele MNIST <<bsp_mnist>>
In diesem Beispiel werden verschiedene \acp{ANN} auf den ~MNIST~ cite:lecun-gradientbased-learning-applied-1998,TFDS
Datensatz (siehe Kapitel ref:datensatz) trainiert.
Ein Beispiel wie diese Pruning Vorgehensweise durch ~condense~ Methoden realisiert werden kann ist in Abbildung ref:code:mnist gegeben.
Der Nutzer kann die Klasse ~condense.keras.Trainer(model, target_sparsity)~ nutzen um automatisiert einen Trainingsprozess zu starten.
Sobald die Klasse initialisiert wurde, muss nur noch die Methode ~.train()~ aufgerufen werden, dabei können dieselben Funktionsargumente
wie bei der ~keras~ Methode ~.fit()~ übergeben werden.
Die unten gezeigten Graphen in Abbildung ref:fig:lottery-mnist-1 & ref:fig:lottery-mnist-2 zeigen Training/Testing Loss während des
automatisierten Trainings.
Beide geprunten Modelle wurden dabei mit einer Ziel-Sparsity von $80\%$ trainiert.
Aus den Graphen geht hervor, wie der Testing-Loss des optimierten Modells trotz der hohen Pruning Rate nicht schlechter ausfällt,
als bei dem selben noch vollständigen Modell.

#+CAPTION[Lottery Ticket Pruning durch Condense]: Dank der einfachen Nutzerschnittstelle von ~condense~  label:code:mnist
#+CAPTION: ist es sehr einfach Möglich automatisiert Pruning auf dem Ziel-Modell auszuführen.
#+BEGIN_SRC
import condense
import tensorflow_datasets as tfds

ds_train, ds_test = tfds.load('mnist', split=['train', 'test'],
                               shuffle_files=True,
                               as_supervised=True)

model = ...
model.compile(keras.optimizers.Adam(learning_rate=0.001),
              keras.losses.SparseCategoricalCrossentropy(from_logits=True))

trainer = condense.keras.Trainer(model, .75)
trainer.train(ds_train.batch(50),
              epochs=20,
              steps_per_epoch=2,
              eval_data=ds_test.batch(50))
#+END_SRC

#+BEGIN_SRC python :exports results :results file :cache yes
import sys
sys.path.append('condense')
import condense
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense
from copy import deepcopy
import tensorflow_datasets as tfds

EPOCHS = 50
SPARSITY = 0.8
ds_train, ds_test = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True)

model = keras.models.Sequential(layers=[
    keras.layers.Flatten(input_shape=(28,28,1)),
    Dense(1024, activation='relu', input_shape=(784,)),
    Dense(512, activation='relu'),
    Dense(256, activation='relu'),
    Dense(50, activation='relu'),
    Dense(10, name='output')
])
model.compile(keras.optimizers.Adam(learning_rate=0.001), keras.losses.SparseCategoricalCrossentropy(from_logits=True))
model.build()

INITIAL_WEIGHTS = deepcopy(model.get_weights())

classical_training = model.fit(ds_train.batch(50), epochs=EPOCHS, steps_per_epoch=2, validation_data=ds_test.batch(50), validation_steps=2)
assert (model.get_weights()[0] != INITIAL_WEIGHTS[0]).any()
model.set_weights(INITIAL_WEIGHTS)
model.compile(keras.optimizers.Adam(learning_rate=0.001), keras.losses.SparseCategoricalCrossentropy(from_logits=True))
assert (model.get_weights()[0] == INITIAL_WEIGHTS[0]).all()

trainer = condense.keras.Trainer(model, 0.75)
hist = trainer.train(ds_train.batch(50).cache(), EPOCHS, steps_per_epoch=2, eval_data=ds_test.batch(50).cache())
plt.figure(figsize=(10, 5))
plt.title(f'MNIST model pruned by {SPARSITY*100}%')
plt.plot(hist.history['val_loss'], label='Training on Sparse Model (Validation Loss)', lw=4)
plt.plot(hist.history['loss'], label='Training on Sparse Model (Training Loss)', ls='--')
plt.plot(classical_training.history['loss'], label='Training on Full Model (Training Loss)', ls='--')
plt.plot(classical_training.history['val_loss'], label='Training on Full Model (Validation Loss)')
plt.plot(trainer.history['ticket_search'].history['loss'], label='Search for winning ticket', ls=':')
plt.grid()
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.axis([0, EPOCHS, 0, 10])
plt.tight_layout()
plt.savefig('resources/plots/lottery-1.png')
return 'resources/plots/lottery-1.png'
#+END_SRC

#+LABEL: fig:lottery-mnist-1
#+CAPTION[Lottery Ticket Hypothesis auf MNIST]:
#+CAPTION: In dieser Grafik wird der Trainingsprozess eines geprunten Modells im Gegensatz zu einem unoptimierten Modell gezeigt.
#+CAPTION: Es sollte sich primär an den rot und blau gekennzeichneten Linien orientiert werden, da diese jeweils dem Evaluation-Loss entsprechen.
#+RESULTS[333f96f9eb909265676ad73c871fd79f451bba41]:
[[file:resources/plots/lottery-1.png]]


#+BEGIN_SRC python :exports results :results file :cache yes
import sys
sys.path.append('condense')
import condense
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense, Conv2D
from copy import deepcopy
import tensorflow_datasets as tfds

EPOCHS = 50
SPARSITY = 0.8
ds_train, ds_test = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True)

model = keras.models.Sequential(layers=[
    Conv2D(24, input_shape=(28,28,1,), kernel_size=5, activation='relu', padding='same'),
    keras.layers.MaxPool2D(),
    Conv2D(48, kernel_size=5, activation='relu', padding='same'),
    keras.layers.MaxPool2D(),
    keras.layers.Flatten(),
    Dense(256, activation='relu'),
    Dense(10, activation='softmax', name='output')
])
model.compile(keras.optimizers.Adam(learning_rate=0.001), keras.losses.SparseCategoricalCrossentropy(from_logits=True))
model.build()

INITIAL_WEIGHTS = deepcopy(model.get_weights())

classical_training = model.fit(ds_train.batch(50), epochs=EPOCHS, steps_per_epoch=20, validation_data=ds_test.batch(50), validation_steps=2)
model.set_weights(INITIAL_WEIGHTS)
model.compile(keras.optimizers.Adam(learning_rate=0.001), keras.losses.SparseCategoricalCrossentropy(from_logits=True))

trainer = condense.keras.Trainer(model, 0.75)
hist = trainer.train(ds_train.batch(50).cache(), EPOCHS, steps_per_epoch=20, eval_data=ds_test.batch(50).cache())
plt.figure(figsize=(10, 5))
plt.title(f'MNIST model pruned by {SPARSITY*100}%')
plt.plot(hist.history['val_loss'], label='Training on Sparse Model (Validation Loss)', lw=4)
plt.plot(hist.history['loss'], label='Training on Sparse Model (Training Loss)', ls='--')
plt.plot(classical_training.history['loss'], label='Training on Full Model (Training Loss)', ls='--')
plt.plot(classical_training.history['val_loss'], label='Training on Full Model (Validation Loss)')
plt.plot(trainer.history['ticket_search'].history['loss'], label='Search for winning ticket', ls=':')
plt.grid()
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.axis([0, EPOCHS, 0, 3])
plt.tight_layout()
plt.savefig('resources/plots/lottery-2.png')
return 'resources/plots/lottery-2.png'
#+END_SRC

#+LABEL: fig:lottery-mnist-2
#+CAPTION[Lottery Ticket Pruning auf einem Modell mit convolutional Schichten]: Pruning auf einem Modell das auf dem MNIST Datensatz
#+CAPTION: mit der Hilfe von convolutional Schichten trainiert wurde.
#+RESULTS[8ccf04d8d219d20fe157a85a06af4dbc79be47f4]:
[[file:resources/plots/lottery-2.png]]

*** Vorbehalte
Der Vergleich geprunter und ungeprunter Modelle gestaltet sich jedoch nicht so trivial wie in Kapitel ref:bsp_mnist dargestellt.
Die Reduktion eines Großteils der Parameter wirkt sich extrem auf die Effekte von Hyperparametern[fn:hyperparameter] aus.
Somit ist es für einen aussagekräftigen Vergleich nötig, auch Werte wie Learning Rate vor dem Training anzupassen.
Um einen möglichst gerechten Vergleich zustande zu bringen, muss durch Hyperparameter-Tuning[fn:hyperparameter-tuning]
von beiden zu vergleichenden Modellen jeweils die beste mögliche Wahl von Hyperparametern getroffen werden.

** Keras Kompatibilitäts-Modul <<keras_module>>
Um eine einfache und klare Schnittstelle zu Keras Modellen zu bieten, gibt ~condense~ Nutzern die Möglichkeit
einzelne Keras Layer an einen Wrapper (~condense.keras.PruningWrapper~) zu übergeben.
Dieser implementiert Schnittstellen für andere Komponenten des Frameworks.
Diese werden genutzt um verschiedenste Layer-Manipulationen vornehmen zu können.

#+BEGIN_QUOTE
Die Hilfsfunktion ~condense.keras.wrap_layer(model, sparsity_function)~ instantiiert ein Modell und augmentiert alle möglichen Layer durch ~PruningWrapper~.
Für die meisten Use-Cases ist dies der empfohlene Weg Keras Modelle zu augmentieren/optimieren.
#+END_QUOTE

Dem ~PruningWrapper~ muss zusätzlich eine Sparsity Funktion übergeben werden.
Diese definiert wie sich das gewünschte Sparsity Ziel im Laufe des Trainings verhalten wird.
~condense~ stellt einige dieser Funktionen zur Verfügung, bietet aber die Möglichkeit durch die Implementierung
der abstrakten Klasse ~SparsityFunction~ ein anderes Verhalten zu bestimmen.
Beispiele für bereits existierende Funktionen sind:
- ~Constant(t_sparsity)~: Ziel Sparsity bleibt über den kompletten Trainingsprozess konstant
- ~Linear(t_sparsity)~: Ziel Sparsity nimmt mit laufenden Training bis zu dem letztendlichen Wert ~t_sparsity~ zu

In dem unten gezeigten Code Beispiel wird ein sequentielles Keras Modell mit $4$ Schichten erzeugt.
Dabei werden Input (Schicht $0$) und Output (Schicht $4$) nicht durch Pruning optimiert.
Dabei werden die Hidden Layer[fn:hidden] (Schicht $2$ und Schicht $3$) jeweils mit zwei verschiedenen Sparsity Funktionen
optimiert. Schicht $2$ wird mit einer konstanten Ziel Sparsity von ~0.5~ optimiert und Schicht $3$ mit einer über das Training
ansteigenden Ziel Sparsity bis zu ~0.7~.
Bei dem Training (~model.fit(..., callbacks=[PruningCallback()])~) wird durch den ~PruningWrapper~ automatisch eine entsprechende Pruning Operation durchgeführt.

#+BEGIN_EXAMPLE
model = keras.models.Sequential(layers=[
    Dense(20, input_shape=(4,)),
    PruningWrapper(Dense(10), Constant(0.5)),
    PruningWrapper(Dense(40), Linear(0.7)),
    Dense(2)
])
#+END_EXAMPLE

Leider ist es durch die Architektur von Keras/TensorFlow notwendig bei dem Trainings-Funktionsaufruf ~.fit()~ auch den
Callback ~condense.keras.PruningCallback~ zu übergeben.
Dieser ist intern für die den ~.prune()~ Funktionsaufruf der jeweiligen ~PruningWrapper~ Instanzen verantwortlich.

#+CAPTION[Quellcode Beispiel: Pruning durch Keras Wrapper]:
#+CAPTION: Einfaches Pruning eines Modells ~model~ durch Pruning des ~condense.keras~ Moduls.
#+CAPTION:
#+BEGIN_SRC
import keras
import condense
from condense.keras import wrap_model, PruningCallback
from condense.optimizer.sparsity_functions import Constant

...

model = keras.models.load_model('...')
augmented = wrap_model(model, Linear(0.7))

augmented.fit(generator,
              epochs=10,
              steps_per_epoch=20,
              callbacks=[PruningCallback()])
#+END_SRC

** PyTorch Kompatibilitäts-Modul <<pytorch>>
PyTorch hat in den letzten Jahren deutlich an Popularität gewonnen, dies liegt neben mehreren Gründen vor allem
an der einfachen Handhabe im Zusammenhang mit Python.
Aufgrund der Architektur lassen sich Pruning Masken in PyTorch signifikant eleganter und einfacher implementieren,
als in TensorFlow/Keras.
Im Vergleich zu ~Keras~ gestaltete sich die Implementierung von Pruning Masken für die Parameter eines Modells deutlich einfacher
und genereller.
So lassen sich in PyTorch beliebige Modelle durch ~condense~ pruning Implementationen optimieren.
Selbst die Implementation des Training-Loops ist nicht durch ~condense~ vorgeschrieben.
Wie bei der Implementation für Keras ist es notwendig, sein Modell in einer Wrapper Klasse (~condenes.torch.PruningAgent~) zu packen.

#+BEGIN_SRC
from condense.torch import PruningAgent
from condense.optimizer.sparsity_functions import Constant

model = ...
agent = PruningAgent(model, Constant(0.75))
#+END_SRC

Das Modell wurde nun auf eine Sparsity von $75\%$ gepruned und kann nun regulär trainiert werden.
Ein Training-Callback wird bei dieser Implementation nicht benötigt.
Der ~PruningAgent~ führt bei der Initialisierung folgende Operationen durch:
1. Er legt eine ~HashMap~ an, die Parameter des Modells auf einen Tensor gleicher Größe abbildet.
   #+begin_quote
   Um gewisse Parameter oder Module eines Modells vom Pruning auszuschließen, kann das Konstruktor-Argument ~ignored_params~ verwendet werden.
   Es handelt sich dabei um kein benötigtes Argument, jedoch empfiehlt es sich Output Layer eines Netzes nicht zu prunen.
   #+end_quote
2. Jeder Parameter erhält einen Callback der automatisch ausgeführt wird, sobald der jeweilige Gradient berechnet wurde.
   Dieser multipliziert die Maske auf den Gradienten, also eliminiert maskierte Felder.
   Somit wird sichergestellt, dass maskierte Felder durch den Optimizer nicht manipuliert werden können.

** Pruning während des Trainings-Przoesses
#+BEGIN_SRC python :exports results :results file :cache yes
import keras
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import sys
sys.path.append('condense')
import condense

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True).repeat()

model = keras.models.Sequential(layers=[
    keras.layers.Dense(40, input_shape=(4,), activation='relu'),
    keras.layers.Dense(80, activation='relu'),
    keras.layers.Dense(3)
])

model.compile(keras.optimizers.Adam(learning_rate=0.001),
              keras.losses.SparseCategoricalCrossentropy(from_logits=True))
model.build()
w = model.get_weights()
unpruned_loss = model.fit(ds.batch(100), epochs=20, steps_per_epoch=50)
model.compile(keras.optimizers.Adam(learning_rate=0.001),
              keras.losses.SparseCategoricalCrossentropy(from_logits=True))

assert (w[0] != model.get_weights()[0]).any()

model.set_weights(w)

assert (w[0] == model.get_weights()[0]).all()

model = condense.keras.wrap_model(model,
                                  condense.optimizer.sparsity_functions.Constant(.3))
pruned_loss = model.fit(ds.batch(100),
                  epochs=20,
                  steps_per_epoch=50,
                  callbacks=[condense.keras.PruningCallback()])

plt.figure(figsize=(6, 3))
plt.plot(unpruned_loss.history['loss'], label='unpruned loss')
plt.plot(pruned_loss.history['loss'], label='pruned loss')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.tight_layout()
plt.savefig('resources/plots/keras-2.png')
return 'resources/plots/keras-2.png'
#+END_SRC

#+CAPTION: Training-Loss des Modells mit pruning und ohne.
#+ATTR_LATEX: :float wrap :width 8cm :center nil
#+RESULTS[c843eb90e533d65b93173bee9fa04a5247eac527]:
[[file:resources/plots/keras-2.png]]

Eine Möglichkeit ein Modell während des Trainings-Prozesses zu prunen ist die jeweilig zu prunenden Parameter nach bestimmten Trainingsabschnitten immer wieder erneut zu prunen.

Integriert wurde dies in dieser Implementation in der Form eines Wrappers für Keras Layer.
So müssen die Layer eines bestehenden Modells nur an den Constructor der Wrapper Klasse übergeben werden und dieser berechnet einen geeigneten Sparsity-Tensor während des Trainings.
Der Ablauf von Operationen während des Trainings kann in Abbildung ref:fig:pruning-process betrachtet werden.
Durch die Callback API, die von Keras zur Verfügung gestellt wird werden die Pruning Operationen in definierten Intervallen während des Trainings ausgeführt.
Bei der Pruning Operation handelt es sich um die elementare Multiplikation der Sparsity Mask $M$ und des jeweiligen Parameters $p$ Tensors $p_{neu} = p_{alt} \times M$.

#+begin_src mermaid :file resources/plots/training-process.png :theme forest :background transparent :cache yes
graph LR
    fit(.fit) --> forward-pass[Forward Propagation]
    forward-pass --> bp[Back Propagation]
    bp --> update[Update Sparsity Mask]
    update --> pruning[Perform Pruning]
#+end_src

#+LABEL: fig:pruning-process
#+CAPTION: Ablauf des Pruning Prozesses während des Trainings.
#+RESULTS[95245539d25fae59ce110bf3118accc7226369a4]:
[[file:resources/plots/training-process.png]]


#+BEGIN_SRC python :exports results :results file :cache yes
import sys
sys.path.append('condense')
import condense
import keras
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
from keras.layers import Dense

model = keras.models.Sequential(layers=[
    keras.layers.Dense(40, input_shape=(4,), activation='relu'),
    keras.layers.Dense(80, activation='relu'),
    keras.layers.Dense(1)
])
model.compile('adam', 'mse')

t_sparsity = 0.8

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
ds = ds.repeat()
layer = 2

old_weights = model.get_weights()

plt.figure(figsize=(12, 6))
plt.subplot(221)
plt.imshow(abs(model.get_weights()[2]), vmin=0, vmax=.4)
plt.title(f'Layer {layer} des ursprünglichen Modells')

hist1 = model.fit(ds.batch(200), steps_per_epoch=200, epochs=10)
plt.subplot(222)
plt.imshow(abs(model.get_weights()[2]), vmin=0, vmax=.4)
plt.title(f'Layer {layer} des trainierten Modells (ohne Pruning)')

model = condense.keras.wrap_model(model, condense.optimizer.sparsity_functions.Constant(t_sparsity))
hist2 = model.fit(ds.batch(200), steps_per_epoch=200, epochs=10, callbacks=[condense.keras.callbacks.PruningCallback()])

plt.subplot(223)
plt.imshow(abs(model.layers[1].kernel), vmin=0, vmax=.4)
plt.title(f'Layer {layer} des trainierten Modells (mit {t_sparsity*100}% Pruning)')

plt.subplot(224)
plt.imshow(abs(model.layers[1].mask), cmap='gist_gray')
plt.title(f'Sparsity Mask des Layers {layer}')

plt.tight_layout()
plt.savefig('resources/plots/keras-1.png')
return 'resources/plots/keras-1.png'
#+END_SRC

#+CAPTION[Weights training mit Pruning und ohne]: Ausschnitts der Weights eines Layers trainiert mit und ohne Pruning.
#+CAPTION: Das Modell wurde auf dem Iris Dataset cite:TFDS (Kapitel ref:datensatz) trainiert
#+RESULTS[0a481db813a8cbb39cf65b0aa86bce939c3e1e32]:
[[file:resources/plots/keras-1.png]]

** Keras/TensorFlow als Backend
Operationen des neuronalen Netzes wie das Training oder die Evaluierung wird durch das auf neuronale Netze ausgelegte \ac{ML} Framework TensorFlow ausgeführt.
Dies bietet Nutzern erhebliche Vorteile wie die mögliche Ausführung auf verschiedensten Plattformen wie GPU/CPU oder Hardware Beschleunigern.
Zudem lassen sich TensorFlow "Layer" eines neuronalen Netzes einfach durch eine öffentliche API erweitern.
Somit lässt sich im weiteren Verlauf des Projekts eine direkte Integration in das TensorFlow Ökosystem anstreben.
TensorFlow bietet zusätzlich durch das externe Modul ~model-optimization~[fn:model_opt] Optimierungen an einem Keras/TensorFlow Modell vorzunehmen.
Auch Pruning wird derzeit von dem Tool unterstützt, indem ein vorhandenes Modell durch die Augmentation von Layern um Pruning Funktionalität erweitert werden kann.
Es besteht somit die Möglichkeit auch ein schon bestehendes Backend für Pruning Operationen zu nutzen.
Somit können Pruning Operationen zuerst auf das ~model-optimization~ Modul ausgelagert werden und sich auf das Refitting und die Analyse von neuronalen Netzen konzentriert werden.

* Ergebnisse

** One Shot Pruning
Das One Shot Interface des ~condense~ Moduls ist durch ~condense.one_shot()~ zu nutzen.
So ist es sehr leicht möglich Keras Modelle durch One-Shot Pruning zu optimieren.
Im diesem Kapitel werden verschiedenste Modelle durch diese Methode optimiert und evaluiert.
Ein Beispiel für das One-Shot Interface des ~condense~ Moduls ist in Abbildung ref:src:one-shot zu sehen.

#+LABEL: src:one-shot
#+CAPTION: Beispiel des One-Shot Pruning Interface
#+BEGIN_SRC
model = keras.models.load_model(...)  # keras model
pruned = condense.one_shot(model, 0.9)  # 90% target sparsity
#+END_SRC

*** Iris Dataset <<one_shot_iris>>
#+begin_quote
Bei diesem Modell handelt es sich um ein eigens auf den Iris Datensatz (Kapitel ref:datensatz) trainiertes Dense \ac{ANN}.
#+end_quote
Naives Pruning in Form von One-Shot Pruning hat drastische Auswirkungen auf die Modell Accuracy,
wie in Abbildung ref:fig:iris-results-one-shot zu sehen ist.
Aus diesem Grund sollte One-Shot Pruning in den meisten Fällen nicht für die Optimierung von Modellen genutzt werden.

#+BEGIN_SRC python :exports results :results file :cache yes
import numpy as np
import sys
import keras
import matplotlib.pyplot as plt
sys.path.append("condense")
import condense
import tensorflow_datasets as tfds

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
ds = ds.repeat()

model = keras.models.load_model('resources/models/iris.h5')

def testing_function(model, refrence):
    model.compile(optimizer='adam', loss="mse")
    return model.evaluate(ds.batch(200), steps=20)

steps = np.arange(0, .99, .05)

plt.figure(figsize=(8, 4))
plt.plot(steps*100,
         [testing_function(condense.one_shot(model, acc), model) for acc in steps])

plt.title('Verlust von Accuracy mit zunehmender Pruning Stärke')
plt.xlabel('Sparsity des Modells in %')
plt.ylabel('Model Accuracy (mse)')
plt.grid()
plt.savefig('resources/plots/iris-accuracy-1.png')
return 'resources/plots/iris-accuracy-1.png'
#+END_SRC

#+LABEL: fig:iris-results-one-shot
#+CAPTION[One-Shot Pruning Accuracy eines Dense Modells]: Dense Model Accuracy mit zunehmender Pruning Stärke.
#+ATTR_LaTeX: :height 5cm :placement [!htpb]
#+RESULTS[05bfe488e6deb3369cdd62e1398ff7fdb1ac4f60]:
[[file:resources/plots/iris-accuracy-1.png]]

*** Convolutional Model
In diesem Beispiel wird ein Convolutional Model durch immer stärkeres One-Shot Pruning optimiert.
Das Modell umfasst deutlich mehr Parameter als das Dense Modell aus Kapitel ref:one_shot_iris.
Aus der Abbildung ref:fig:mnist-results-one-shot ist zu entnehmen, dass sich die Test-Accuracy des Modells bis zu einer Parameter-Sparsity von $40\%$ kaum negativ verändert.
Daraus lässt sich die relativ niedrige Informationsdichte des Modells schlussfolgern.
Wie bereits angesprochen, eignen sich genau diese Modelle für Pruning Optimierungen deutlich besser, als informationsdichtere Modelle.
In dem in Abbildung ref:src:one-shot-mnist gezeigten Quellcodeausschnitt,
ist das One-Shot Interface des PyTorch Kompatibilitätsmodul gezeigt (Kapitel ref:pytorch).
Anzumerken ist dabei das Konstruktor Argument ~ignored_params~,
bei dem spezifiziert wird die letzte Schicht des Modells (also der Output Layer) nicht zu prunen.
Das Argument ~apply_mask=True~ sorgt für die sofortige Anwendung der generierten Maske und nimmt somit One-Shot Pruning vor.

#+LABEL: src:one-shot-mnist
#+CAPTION: One-Shot Pruning eines Torch Models durch den ~condense.torch.PruningAgent~
#+BEGIN_SRC
model = ...  # pytorch model
pruned = PruningAgent(model,
                      Constant(0.7),
                      # generierte maske wird direkt angewandt
                      apply_mask=True,
                      ignored_params=[list(model.parameters())[-1]])
#+END_SRC

#+BEGIN_SRC python :exports results :results file :cache yes
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import sys
import numpy as np
from datasets import mnist
sys.path.append('condense')
import condense
import torchviz
from graphviz import Source

train_loader, test_loader = mnist()

model = nn.Sequential(
    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=4, stride=2),
    nn.ReLU(),
    nn.Conv2d(in_channels=4, kernel_size=4, out_channels=1, stride=1),
    nn.ReLU(),
    nn.Flatten(),
    nn.Linear(100, out_features=255),
    nn.ReLU(),
    nn.Linear(255, out_features=255),
    nn.ReLU(),
    nn.Linear(255, out_features=128),
    nn.ReLU(),
    nn.Linear(128, out_features=10)
)

loss = torch.nn.CrossEntropyLoss()

optim = torch.optim.Adam(model.parameters(), lr=0.001)
hist, test_hist = [], []

for _, i in enumerate(train_loader):
    model.zero_grad()
    pred = model.forward(i[0])
    l = loss(pred, i[1])
    hist.append(l)
    l.backward()
    optim.step()

    _, (X, y) = next(enumerate(test_loader))
    test_hist.append(loss(model.forward(X), y))

def testing_function(model, acc):
    agent = condense.torch.PruningAgent(model,
                                        condense.optimizer.sparsity_functions.Constant(acc),
                                        apply_mask=True,
                                        ignored_params=[list(model.parameters())[-1]])


    _, (X, y) = next(enumerate(test_loader))
    l = loss(model.forward(X), y)

    return l

plt.figure(figsize=(8, 4))
acc = [testing_function(model, a) for a in np.arange(0, .99, .05)]
plt.plot(np.arange(0, .99, .05)*100, acc)
plt.xlabel('Sparsity des Modells in %')
plt.ylabel('Model Loss (crossentropy)')
plt.title('Verlust von Accuracy mit zunehmender Pruning Stärke')
plt.grid()
plt.tight_layout()
plt.savefig('resources/plots/conv_one_shot.png')
return 'resources/plots/conv_one_shot.png'
#+END_SRC

#+LABEL: fig:mnist-results-one-shot
#+CAPTION[One-Shot Pruning Accuracy eines Convolutional Modells]: Convolutional Model Accuracy mit zunehmender Pruning Stärke.
#+RESULTS[3d38d99a1f6c222d7f832659b987956c2f75d35f]:
[[file:resources/plots/conv_one_shot.png]]

** Training eines Sub-Networks (Lottery Ticket Hypothesis cite:Frankle2018)
Bei vorangegangenen Pruning-Methoden, wurde bisher nur nach oder während des Trainings-Prozesses Pruning vorgenommen.
Jedoch ist es auch möglich das Training auf einem Sub-Network, also ein gepruntes Netz des Originals vorzunehmen cite:Frankle2018.
Für eine genaueren Beschreibung dieser Methodik wird an dieser Stellte auf Kapitel ref:lottery_ticket dieser Arbeit verwiesen.

Durch die Anwendung der "Lottery Ticket Hypothesis" cite:Frankle2018 ist es möglich den initialen Trainingsprozess deutlich zu verbessern.
Diese Pruning Methode ist nur für Modelle möglich, die noch nicht trainiert wurden, für bereits trainierte Modelle sollte ein One-Shot Pruning Ansatz gewählt werden.
So zeigt Abbildung ref:fig:torch-lottery-1 die Auswirkungen der Ticket-Suche und Reinitialisierung auf den Evaluation Loss des letztendlichen Trainingsprozesses.

#+BEGIN_SRC python :exports results :results file :cache yes
import sys
sys.path.append('condense')
import torch
import condense
import torch.nn as nn
import matplotlib.pyplot as plt
import torchvision
from datasets import mnist

d_train, d_test = mnist()

class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.layer1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=4, stride=2)
        self.layer2 = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=2)
        self.dense = nn.Linear(288, out_features=50)
        self.dense2 = nn.Linear(50, out_features=50)
        self.output = nn.Linear(50, out_features=10)


    def forward(self, X):
        X = self.layer1.forward(X)
        X = self.layer2.forward(torch.relu(X))
        X = X.view(torch.relu(X).size(0), -1)
        X = self.dense.forward(torch.relu(X))
        X = self.dense2.forward(torch.relu(X))
        X = self.output.forward(torch.relu(X))
        return X

    def train(self, data, epochs=None, eval_data=None, lr=0.001):
        metrics = {
            "loss": [],
            "eval": []
        }
        criterion = nn.CrossEntropyLoss()
        optim = torch.optim.Adam(self.parameters(), lr=lr)
        for i, (X, y) in enumerate(data):
            if epochs and epochs < i:
                break

            self.zero_grad()
            pred = self.forward(X)
            l = criterion(pred, y)
            l.backward()
            optim.step()
            metrics['loss'].append(l)

            if eval_data:
                _, (X, y) = next(enumerate(eval_data))
                metrics['eval'].append(criterion(self.forward(X), y))

        return metrics

test_net = Network()

model = Network()
model.load_state_dict(test_net.state_dict())
p = condense.torch.PruningAgent(model, condense.optimizer.sparsity_functions.Constant(0.5), apply_mask=False, ignored_params=[model.output])

with condense.torch.TicketSearch(p):
    model.train(d_train, 200, d_test, lr=0.001)

fine_t_data = model.train(d_train, 300, d_test, lr=0.001)

# Training without pruning
without_data = test_net.train(d_train, 500, d_test, 0.001)

plt.figure(figsize=(9, 6))

plt.plot(fine_t_data['eval'], label='Evaluation Loss während des Fine Tuning Prozesses')
plt.plot(without_data['eval'], label='Evaluation Loss während des Trainings ohne Pruning')
plt.title('Training eines geprunten Modells ($50\\%$ Sparsity)')
plt.xlabel('# Epoch')
plt.ylabel('Eval Loss')
plt.grid()
plt.legend()
plt.axis([0, 200, 0, 2.3])
plt.tight_layout()
plt.savefig('resources/plots/torch-lottery-2.png')
return 'resources/plots/torch-lottery-2.png'
#+END_SRC

#+LABEL: fig:torch-lottery-1
#+CAPTION[Vergleich zwischen optimierten und unoptimierten Trainingsverläufen]:
#+CAPTION: Beide hier dargestellten Modelle wurden mit den identischen Parameter Werten initialisiert.
#+CAPTION: Es kann beobachtet werden, dass das "Winning Ticket" Modell deutlich schneller konvergiert, als das unoptimierte Modell.
#+ATTR_LATEX: :float wrap :width 8cm :center nil
#+RESULTS[687cc7ba324250020c1712a56f0185dc27528bb8]:
[[file:resources/plots/torch-lottery-2.png]]

Bei der Wahl verschiedener Learning Raten der Modelle kann, wie in Abbildung ref:fig:torch-lottery-lr gezeigt, konstant eine effizientere
Konvergierung erreicht werden.
Somit bietet das Training auf einem geprunten Modell je nach Datensatz und verwendeter Modell-Architektur enormes Potential ein schnelleres und
eventuell besser generalisierendes Modell zu erhalten.
Demnach ist es durch die Implementation des unter dieser Arbeit entwickelten Frameworks, gelungen die Ergebnisse des ursprünglichen Artikels zu reproduzieren.

#+BEGIN_SRC python :exports results :results file :cache yes
import sys
sys.path.append('condense')
import torch
import condense
import torch.nn as nn
import matplotlib.pyplot as plt
import torchvision
from datasets import mnist

d_train, d_test = mnist()

class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.layer1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=4, stride=2)
        self.layer2 = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=2)
        self.dense = nn.Linear(288, out_features=50)
        self.dense2 = nn.Linear(50, out_features=50)
        self.output = nn.Linear(50, out_features=10)


    def forward(self, X):
        X = self.layer1.forward(X)
        X = self.layer2.forward(torch.relu(X))
        X = X.view(torch.relu(X).size(0), -1)
        X = self.dense.forward(torch.relu(X))
        X = self.dense2.forward(torch.relu(X))
        X = self.output.forward(torch.relu(X))
        return X

    def train(self, data, epochs=None, eval_data=None, lr=0.001):
        metrics = {
            "loss": [],
            "eval": []
        }
        criterion = nn.CrossEntropyLoss()
        optim = torch.optim.Adam(self.parameters(), lr=lr)
        for i, (X, y) in enumerate(data):
            if epochs and epochs < i:
                break

            self.zero_grad()
            pred = self.forward(X)
            l = criterion(pred, y)
            l.backward()
            optim.step()
            metrics['loss'].append(l)

            if eval_data:
                _, (X, y) = next(enumerate(eval_data))
                metrics['eval'].append(criterion(self.forward(X), y))

        return metrics

test_net = Network()

def benchmark_function(lr_search, lr_train):
    model = Network()
    model.load_state_dict(test_net.state_dict())
    p = condense.torch.PruningAgent(model, condense.optimizer.sparsity_functions.Constant(0.7), apply_mask=False, ignored_params=[model.output])

    with condense.torch.TicketSearch(p):
        search = model.train(d_train, 200, d_test, lr=lr_search)

    return model.train(d_train, 300, d_test, lr=lr_train), search


plt.figure(figsize=(14, 6))

for i, (lr_train, c) in enumerate(zip([0.01, 0.005, 0.001], ['#32a852', '#264653', '#9d0208'])):
    fine_t_data, search_data = benchmark_function(lr_train, lr_train)
    # plt.subplot(3,1,i+1)
    plt.plot(fine_t_data['eval'], label=f'Fine Tuning ({lr_train} lr)', lw=1.8, c=c)
    plt.plot(search_data['eval'], label=f'Ticket Searching ({lr_train} lr)', ls='--', c=c)
    plt.xlabel('# Epoch')
    plt.ylabel('Eval Loss')
    plt.grid()
    plt.legend()

plt.title('Convolutional Model trainiert auf MNIST Dataset')
plt.tight_layout()
plt.savefig('resources/plots/torch-lottery.png')
return 'resources/plots/torch-lottery.png'
#+END_SRC

#+LABEL: fig:torch-lottery-lr
#+CAPTION[Lottery Ticket Hypothesis und Learning Rate]: In dem hier gezeigten Diagramm wird ein identisches Modell,
#+CAPTION: bestehend aus convolutional und dense Layern durch die in der Lottery Ticket Hypothesis Methodik optimiert.
#+CAPTION: Dabei symbolisiert jede Farbe eine Learning Rate, die für das Suchen und Trainieren des Modells gewählt wurde.
#+CAPTION: Die gestrichelten Linien repräsentieren den jeweiligen Evaluation-Loss, des Winning Ticket Such-Prozesses.
#+CAPTION: Die durchgezogenen Linien symbolisieren den Evaluation-Loss des Fine-Tuning Prozesses.
#+RESULTS[dcfd136a8394c0ab1b61cdcb81c589cf1c082cc9]:
[[file:resources/plots/torch-lottery.png]]

** Einfluss von Aktivierungs-Funktionen auf Pruning Verfahren <<activations>>
Es lässt sich bei der Analyse von Layer-Sparsity auf neuronalen Netzen, die mit unterschiedlichen Aktivierungsfunktionen
trainiert wurden feststellen, dass Aktivierungsfunktionen starke Auswirkungen auf die Gewichtungsverteilung der Schichten eines \acp{ANN} hat.
Bei dem Betrachten der Abbildung ref:fig:activation_fn, werden die Auswirkungen der Aktivierungsfunktionen ersichtlicher.

#+BEGIN_SRC python :exports results :results file :cache yes
import tensorflow_datasets as tfds
import keras
import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize=(12,8))
AF = ['relu', 'sigmoid', 'tanh']
ds = tfds.load('iris', split=['train'], shuffle_files=True, as_supervised=True)[0]

for i, activation_fn in enumerate(AF):
    plt.subplot(3, len(AF), i+1)
    model = keras.models.Sequential(layers=[
        keras.layers.Dense(20, input_shape=(None, 4), activation=activation_fn),
        keras.layers.Dense(20, activation=activation_fn),
        keras.layers.Dense(3, activation='softmax')
    ])
    model.compile(keras.optimizers.Adam(learning_rate=0.3), 'sparse_categorical_crossentropy')
    model.fit(ds.batch(300), epochs=400, steps_per_epoch=10000)
    plt.imshow(np.abs(model.layers[1].kernel.numpy()), vmin=0.0, vmax=0.7)
    plt.title(f'{activation_fn} ({np.mean(np.abs(model.layers[1].kernel.numpy()))})')
    plt.subplot(3, len(AF), 3+i+1)
    plt.hist(np.abs(model.layers[1].kernel.numpy().flatten()))
    # Diff
    plt.subplot(3, len(AF), 6+i+1)
    plt.title('Weight Pruning $30\\%$')
    w = np.abs(model.layers[1].kernel.numpy())
    w[w > np.sort(w.flatten())[int(np.prod(w.shape) * 0.4)]] = 0
    plt.imshow(w, vmin=0.0, vmax=0.2, cmap='gist_gray')


plt.tight_layout()
plt.savefig('resources/plots/sparsity_activation.png')
return 'resources/plots/sparsity_activation.png'
#+END_SRC

#+LABEL: fig:activation_fn
#+CAPTION[Einfluss von Aktivierungsfunktionen auf die Verteilung von Verbindungsgewichtungen]:
#+CAPTION: Einfluss von verschiedenen Aktivierungsfunktionen auf die Gewichtungs-Verteilung einer \ac{ANN} Schicht.
#+CAPTION: Das dargestellte neuronale Netz wurde auf dem Iris Datensatz trainiert cite:fisher36lda.
#+CAPTION: Bei den dargestellten Werten, handelt es sich jeweils immer um den absoluten Wert einer Gewichtung.
#+RESULTS[9089ef3251e99a9134f406d0e66919b42e142e2d]:
[[file:resources/plots/sparsity_activation.png]]

Eine Aktivierungsfunktion, die tendenziell eine höhere Anzahl von Gewichtungen gegen $0$ optimiert ist für ein zu
prunenden Netzes begehrenswerter.
Somit gehen bei dem Pruning deutlich weniger stark gewichtete Verbindungen verloren.
In Abbildung ref:fig:activation_fn ist in der letzten Zeile zu erkennen, dass deutlich Unterschiede bei den zu prunenden Gewichtungen zu erkennen sind.
So ist es bei dem mit ~ReLU~ trainierten Netz auffällig, wie im Verhältnis die geprunten Verbindungen deutlich schwächer gewichtet sind als bei
den anderen Netzen.

Durch die in diesem Kapitel gezeigten Erkenntnisse, lässt sich empfehlen Aktivierungsfunktionen für das Training von zu prunenden \acp{ANN} zu nutzen,
die Gewichtungen möglichst gegen $0$ zu optimieren.

** Pruning durch Pruning Layer

** Anwendung des Pruning-Frameworks auf bekannte neuronale Netze
#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
import sys
sys.path.append('./condense')
import condense
from condense.utils.model_utils import calc_model_sparsity
from condense.utils.layer_utils import calc_layer_sparsity
from keras.applications.resnet50 import ResNet50

model = ResNet50(weights='imagenet')
pruned = condense.one_shot(model, 0.3)

plt.figure(figsize=(10,4))
ax1 = plt.subplot(111)
ax1.bar(np.arange(0, len(model.get_weights()[10:]), 1),
        height=[np.mean(np.abs(layer)) for layer in model.get_weights()[10:]],
        label=f'Unpruned Model ({round(calc_model_sparsity(model)*100, 2)}% Sparsity)',
        width=3,
        zorder=1)
ax1.bar(np.arange(0, len(model.get_weights()[10:]), 1),
        height=[np.mean(np.abs(layer)) for layer in pruned.get_weights()[10:]],
        label=f'Pruned Model ({round(calc_model_sparsity(pruned)*100, 2)}% Sparsity)',
        width=1,
        zorder=2)

# ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
# ax2.bar(np.arange(0, len(model.get_weights()[10:]), 1),
#         height=[calc_layer_sparsity(layer) for layer in pruned.get_weights()[10:]],
#         label=f'Pruned Model ({round(calc_model_sparsity(pruned)*100, 2)}% Sparsity)',
#         color='grey',
#         alpha=0.7,
#         zorder=2)
ax1.legend()
ax1.set_title('ResNet50 Weight Distribution')
ax1.set_xlabel('Network Layer Nr.')
ax1.set_ylabel('Avg. Layer Weights')

plt.grid()
plt.legend()

plt.savefig('./resources/plots/resnet50_hist.png')
return './resources/plots/resnet50_hist.png'
#+END_SRC

#+CAPTION[ResNet50 Weight Matrix Histogramm]: ResNet50 Weight Matrix Histogramm
#+RESULTS[2b4f93e360e78cf2184b4f9ba587edd4bc1947a4]:
[[file:./resources/plots/resnet50_hist.png]]

* Fazit
** Kapitalisierung von Sparsity in neuronalen Netzen <<kapit>>
Aus der erhöhten Sparsity von neuronalen Netzen, lässt sich leider kein direkter Leistungsanstieg in Form von Inferenz Zeit verzeichnen.
Herkömmliche Multiplikation von Matrizen wird nicht durch die erhöhte Sparsity beschleunigt,
somit kann meist lediglich eine bessere Generalisierung des Modells beobachtet werden.
Jedoch beschäftigen sich bereits viele verschiedene Teams mit der Entwicklung von Hardware und Software Lösungen um das Potential von Sparse \acp{ANN}
auszuschöpfen.
So lässt sich durch spezielle Hardware Beschleuniger eine Verbesserung von Inferenz-Zeit um einen Faktor von $3.6\times$ bis $12.9\times$ erreichen cite:8735526.
Ob sich derartige Hardware Beschleuniger beweisen können, wird sich in Zukunft zeigen.

Eine schon bestehende Software Lösung für derartige Optimierungen könnte das unter Facebook AI Entwickelte Framework ~GLOW~[fn:glow] bieten.
Bei diesem handelt es sich um einen Compiler der \ac{ANN} Modelle direkt in Maschinen-Code übersetzen kann cite:rotem2019glow.
Durch das von ~GLOW~ genutzte ~LLVM~ Backend, lassen sich Optimierungen auf dem ~IR~ des Modell-Codes anwenden.
Besonders für die Entwicklung auf Embedded Devices ist dieses Tool sehr relevant, jedoch bietet es auch äußerst viel-versprechende Optimierungen für Sparse \ac{ANN} Modelle cite:Lewis2018.

* Ausblick
** Weiterentwicklung des Frameworks
** Forschung

#+LATEX: \printbibliography

* Footnotes
[fn:sparsity] Sparsity beschreibt die Anzahl von Feldern in einem Tensor, die einer $0$ entsprechen.
Somit setzt sich die Sparsity eines künstlichen neuronalen Netzes die Sparsity jedes Layers zusammen.
[fn:tensorflow] Ein von Google entwickeltes Deep Learning Framework [[https://www.tensorflow.org][(tensorflow.org)]].
[fn:keras] Ehemalig externes Frontend von \ac{TF}, seit \ac{TF} 2.0 fester Bestandteil des Frameworks.
[fn:onnx] Universales Format für die Persistierung von \ac{ANN} Modellen.
[fn:loss] Als Loss wird der allgemeine Fehler des Netztes auf einem Datensatz bezeichnet.
Die Funktion die den Loss berechnet wird Loss Funktion benannt.
[fn:gradient] Alle Partiellen Ableitungen einer Funktion $f(x_1, \dots, x_n)$ werden als Gradienten $\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \dots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$ bezeichnet.
Im Kontext des maschinellen Lernens wird mit Gradient oft die Werte des eigentlichen Gradienten zu einem bestimmten Punkt gemeint.
[fn:feedforward] Bei einem Feed Forward Netzwerk fließen die Daten immer linear durch das Netz und werden zu keinem Zeitpunkt an vorherige Schichten geleitet.
[fn:fullyconnected] Bei einem fully connected \ac{ANN} ist jedes Neuron aus Schicht $L$ mit allen Neuronen der Schicht $L+1$ verbunden.
[fn:pdoc] Open Source Projekt zur Generierung von Dokumentation aus Python Modulen. (https://pdoc3.github.io/pdoc/)
[fn:docstring] In Python wird ein Kommentar Block, der eine Funktion, Klasse oder ein Modul beschreibt als Docstring bezeichnet.
[fn:docsify] Framework mit dem Dokumentation in Form einer Web-App aus Markdown Dateien generiert werden kann. (https://docsify.js.org)
[fn:pages]  Eine von GitHub angebotene Dienstleistung Webseiten durch ein Repository bereitstellen zu können.(https://pages.github.com)
[fn:actions] Ein Dienst um automatisiert Tests oder Deployment Operationen durchzuführen. (https://github.com/features/actions)
[fn:pytest] Testing Framework für die Python Programmiersprache. (https://docs.pytest.org/)
[fn:pydocstyle] Tool um Docstrings eines Python Modules zu überprüfen. (https://github.com/PyCQA/pydocstyle)
[fn:styleguide] Ein von Google genutzter Styleguide für Python Projekte. (https://google.github.io/styleguide/pyguide.html)
[fn:pylint] Software um statische Syntax Analyse auf Python Source Code durchzuführen. (https://www.pylint.org)
[fn:model_opt] Toolkit für TensorFlow Modell Optimierungen. (https://github.com/tensorflow/model-optimization)
[fn:numpy] Numpy ist eine Mathematik Python Bibliothek, die von vielen wissenschaftlichen Modulen genutzt wird.
Mathematische Operationen, wie Matrix Multiplikation sind in C implementiert und somit sehr performant. (https://numpy.org)
[fn:hidden] Schichten eines Modells mit denen der Nutzer keine direkte Interaktion hat, werden als hidden bezeichnet.
Beispielsweise wären in einem sequentiellen Modell mit $4$ Schichten, Layer $2$ und $3$ hidden.
[fn:github] https://github.com/sirbubbls/pruning-ba
[fn:evaluation_loss] Unter dem Evaluation Loss versteht man die Accuracy, die ein Modell auf einem Test Datensatz erzielt.
Dabei wurde das Modell nicht auf dem Datensatz trainiert und kennt diesen somit auch nicht.
Meist ist der Evaluation Loss die aussage-kräftigste Metrik über die Qualität eines KI Modells.
[fn:training_loss] Der Training-Loss eines Modells beschreibt, wie gut die Accuracy das Modells auf dem Trainings-Datensatz ist.
[fn:glow] https://github.com/pytorch/glow
[fn:hyperparameter] Parameter wie ~learning_rate~, die auf das Training Auswirkungen haben können.
[fn:hyperparameter-tuning] Das justieren der Hyperparameter um ein möglichst gutes Modell durch Training zu erhalten.
[fn:pypi] https://pypi.org
[fn:gh_condense] Das GitHub Verzeichnis des unter dieser Arbeit entwickelten Frameworks (https://github.com/sirbubbls/condense)
[fn:pip] Package Installer for Python ~pip~ (https://pypi.org/project/pip/)
[fn:_]

bibliography:references.bib
bibliographystyle:apalike
