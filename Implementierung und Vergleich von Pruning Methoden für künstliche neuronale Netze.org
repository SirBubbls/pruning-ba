#+TITLE: Implementierung und Vergleich von Pruning Methoden für künstliche neuronale Netze
#+AUTHOR: Lucas Sas Brunschier
#+DESCRIPTION: Bachelor Arbeit
#+LATEX_CLASS: report
#+language: de
#+LATEX_HEADER: \usepackage[ngerman]{babel}
#+LATEX_HEADER: \usepackage{a4wide}
#+LATEX_HEADER: \usepackage[backend=bibtex, style=numeric] {biblatex}
#+LATEX_HEADER: \addbibresource{references.bib}
#+LATEX_HEADER: \usepackage{acronym}
#+STARTUP: showall
#+TOC: tables

# Abbildungsverzeichnis
#+BEGIN_LATEX
\newpage
\listoffigures
\newpage
#+END_LATEX

# Abkürzungsverzeichnis
#+BEGIN_LATEX
\begin{acronym}[Bash]
\acro{ANN}{künstliches neuronales Netz}
\acro{TF}{Tensor Flow}
\end{acronym}
\newpage
#+END_LATEX

* Einleitung
** Neuronale Netze
*** Historisches
Obwohl \ac{ANN}'s erst ca. 2008 ihre Blütezeit erreicht haben, ist die zu Grunde liegende Technlogie bereits seit
Mitte bis Ende des 20. Jahrhunderts bekannt.
So schuf Frank Rosenblatt im Jahre 1975 das in Abbildung ref:fig:perceptron dargestellte Modell eines Perceptrons cite:werbos1975beyond, eine
mathematische Abstraktion des aus der Biologie bekannten Neuron.
Das Perceptron wird bis heute als Modell für ein alleinstehendes Neuron in einem \ac{ANN} verwendet.
In dem kommenden Kapitel [[perceptron]] wird noch im Detail auf das Modell des Perceptrons eingegangen.
Der Backpropagation Algorithmus, bei dem es sich um eine Implementation der Kettenregel zur automatischen Differenzierung
von Parametern eines \ac{ANN} handelt, wurde im Jahre 1986 publiziert cite:Rumelhart_1986.
Auch heute erweist sich der Backpropagation Algorithmus als eine sehr effiziente Methode die Gradienten[fn:gradient] eines \ac{ANN}'s zu berechnen und
findet beinahe unverändert Einsatz in verschiedensten modernen Deep Learning Frameworks.
Eine häufige Fehlinformation die über den Backpropagation Algorithmus verbreitet wird, ist dieser sei für das "Lernen" des neuronalen Netzes
verantwortlich.
Dies ist inkorrekt, da eigentlich der Gradient Descent Algorithmus cite:Curry_1944 die von dem Backpropagation berechneten Gradienten nutzt um
die Parameter des Netzes so zu manipulieren, dass der Loss[fn:loss] minimiert wird.

*** Perceptron <<perceptron>>
#+LABEL: fig:perceptron
#+CAPTION[Digramm eines einfachen Perceptrons]: Abbildung eines einfachen Perceptrons. cite:towardsdatascience
[[./resources/perceptron.png]]

** Einführung in naive Pruning Methoden für künstliche neuronale Netze

Es lässt sich durch Beobachtung der künstlichen neuronalen Netze der letzten Jahre feststellen,
dass die Komplexität und die damit einhergehende Anzahl von Neuronen und deren Verbindungen immer weiter zunehmen. cite:altenberger18:_non_techn_survey_deep_convol
Gleichzeitig werden diese komplexeren und größeren \ac{ANN} Architekturen auch auf schwächeren eingebetteten Geräten eingesetzt.
Dadurch werden Optimierungen an neuronalen Netzen immer relevanter, da dies Inferenz-Zeit und Modellgröße minimieren kann.
Verfahren wie Quantisierung können die Laufzeit und den Speicherverbrauch von \ac{ANN}'s deutlich verbessern, jedoch können auch
Pruning Verfahren massive Verbesserungen versprechen. cite:Frankle2018
Pruning Verfahren versuchen durch das Entfernen von Verbindungen oder auch ganzen Neuronen, die Sparsity[fn:sparsity] eines Modells zu erhöhen.
Weight oder auch Connection Pruning bezeichnet den Vorgang Verbindungen aus einem \ac{ANN} zu entfernen.
Dabei werden die Verbindungen eleminiert, also mit $0$ gewichtet. Die ist in Abbildung ref:fig:naiveweightpruning dargestellt.
Die ausgewählten Verbindungen oder Neuronen werden durch eine Heuristik bestimmt, eine Heuristik könnte beispielsweise die niedrigst gewichteten Verbindungen sein.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
from scripts import prune, quad_plot
import sys
sys.path.append('./condense')
from condense.optimizer.layer_operations.weight_prune import w_prune_layer
model = keras.models.load_model('./resources/models/iris.h5')
layer = 1
quad_plot(w_prune_layer(model.get_weights()[0::2][layer], .85),
          model.get_weights()[0::2][layer],
          './resources/plots/iris-weight-pruning.png')
return './resources/plots/iris-weight-pruning.png'
#+END_SRC

#+LABEL: fig:naiveweightpruning
#+CAPTION[Visualisierung von Weight Pruning]:
#+CAPTION: In diesem hier dargestellten Dense Layers eines neuronalen Netzes, wurde die Sparsity des Modells durch Pruning der Verbindungen auf $85\%$ erhöht.
#+CAPTION: Es ist gut zu beobachten, wie nur leicht gewichtete Verbindungen durch Pruning deaktiviert werden, hier durch schwarze Pixel zu erkennen.
#+CAPTION: Bei dem Netz handelt es sich um ein durch TensorFlow 2.0 trainiertes Modell. Bei dem Training wurde der Iris Datensatz genutzt.
#+RESULTS[24573ae0da3e8308c793bec20681a90bd4bdf89b]:
[[file:./resources/plots/iris-weight-pruning.png]]


Analog zu dem Pruning der Verbindungen existiert auch Neuron-Pruning, also das entfernen ganzer Neuronen und deren Verbindungen aus dem \ac{ANN}.
Dies wird in Abbildung ref:fig:naiveneuronpruning durch die Visualisierung eines Layers vor und nach Neuron-Pruning gezeigt.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
from scripts import quad_plot
import sys
sys.path.append('./condense')
from condense.optimizer.layer_operations.unit_prune import u_prune_layer

layer= 1
model = keras.models.load_model('./resources/models/iris.h5')

quad_plot(u_prune_layer(model.get_weights()[0::2][layer], .4),
          model.get_weights()[0::2][layer], './resources/plots/iris-unit-pruning.png')
return './resources/plots/iris-unit-pruning.png'
#+END_SRC

#+LABEL: fig:naiveneuronpruning
#+CAPTION[Visualisierung von Unit/Neuron Pruning]:
#+CAPTION: In diesem Beispiel wird das oben verwendete Modell durch eine naive Implementation des Neuron Pruning um einen Faktor von $0.4$ optimiert.
#+CAPTION: Vertikale Linien repräsentieren in diesem Diagramm die Weights eines Neuronen.
#+CAPTION: Man kann sehr gut beobachten wie sich ganze Neuronen schwarz färben, also deaktiviert werden.
#+RESULTS[46cefe847e97a816605eebe4407161b26d4eded9]:
[[file:./resources/plots/iris-unit-pruning.png]]


** Industrierelevanz
Pruning von künstlichen neuronalen Netzen bietet vielen Unternehmen die Möglichkeit Optimierungen an schon bestehenden \ac{ANN} Modellen vorzunehmen.
Diese Optimierungen können unter Umständen ermöglichen komplexere Modelle auf schwächeren Computern zu nutzen.
Beispielsweise eingebettete Geräte können dabei effizienter Daten durch neuronale Netze auswerten.
Besonders in Situationen in denen das Modell möglichst schnell ein Prognose abgeben soll, wie beispielsweise bei Teilen von
selbständig fahrenden Autos bietet Pruning Chancen auf enorme Verbesserungen.
Zudem bietet Pruning eine Möglichkeit, \ac{ANN} Modelle ohne signifikante Einbußen von Genauigkeit zu optimieren. cite:Frankle2018
Dies sollte Pruning Methoden auf deutlich mehr \ac{ANN} Modelle einsetzbar machen.

** Ziel dieser Arbeit
*** Erstellung eines Pruning Frameworks
Ziel dieser Arbeit ist es primär ein Python Framework zu entwickeln, das mehrere verschiedene Typen von Pruning Methoden implementieren soll.
Ein wichtiger Fokus sollte bei der Architektur des Framework sein, dies in Zukunft möglichst einfach erweitern zu können.
Dokumentation der verschiedenen Module ist aus diesem Grund sehr wichtig und sollte im Laufe der Arbeit auch immer aktualisiert werden.

#+begin_src mermaid :file resources/plots/pruning-framework.png :theme forest :background transparent
graph LR
    input(Input Model) --> interface(High Level Interface)
    interface --> parser(Model Parser)
    pruning(Pruning Engine) --> output(Pruned Model)
    parser --> pruning
#+end_src

#+LABEL: fig:rough-project-structure
#+CAPTION[Pruning Framework Konzept (Graph)]: Der hier gezeigte Graph soll das grobe Konzept, des im Laufe dieser Arbeit entstehenden Pruning Frameworks zeigen.
#+RESULTS:
[[file:resources/plots/pruning-framework.png]]

Zudem sollte das Framework kompatibel mit aktueller Deep Learning Software und deren Formate kompatibel sein.
Kompatibilität mit \ac{TF} [fn:tensorflow]/Keras[fn:keras] steht bei diesem Projekt im Vordergrund, da auch intern \ac{TF} für Trainings-Operationen genutzt wird.
Optional sollte auch die Möglichkeit bestehen ein Modell in dem ONNX[fn:onnx] Format zu exportieren, um auch Kompatibilität mit anderen Frameworks sicherzustellen.

* Methodik


* Implementierung

* Ergebnisse
* Fazit
* Ausblick
* Literaturvezeichnis
#+LATEX: \printbibliography

* Footnotes
[fn:sparsity] Sparsity beschreibt die Anzahl von Feldern in einem Tensor, die einer $0$ entsprechen.
Somit setzt sich die Sparsity eines künstlichen neuronalen Netzes die Sparsity jedes Layers zusammen.
[fn:tensorflow] Ein von Google entwickeltes Deep Learning Framework [[https://www.tensorflow.org][(tensorflow.org)]].
[fn:keras] Ehemalig externes Frontend von \ac{TF}, seit \ac{TF} 2.0 fester Bestandteil des Frameworks.
[fn:onnx] Universales Format für die Persitierung von \ac{ANN} Modellen.
[fn:loss] Als Loss wird der allgemeine Fehler des Netztes auf einem Datensatz bezeichnet.
Die Funktion die den Loss berechnet wird Loss Funktion benannt.
[fn:gradient] Alle Partiellen Ableitungen einer Funktion $f(x_1, \dots, x_n)$ werden als Gradienten $\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \dots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$ bezeichnet.

bibliography:references.bib
bibliographystyle:apalike
