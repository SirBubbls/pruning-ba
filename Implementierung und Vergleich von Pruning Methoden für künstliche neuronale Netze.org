#+TITLE: Implementierung und Vergleich von Pruning Methoden für künstliche neuronale Netze
#+AUTHOR: Lucas Sas Brunschier
#+DESCRIPTION: Bachelor Arbeit
#+DATE: XX.XX.2020
#+LATEX_CLASS: report
#+language: de
#+LATEX_HEADER: \usepackage[ngerman]{babel}
#+LATEX_HEADER: \usepackage{a4wide}
#+LATEX_HEADER: \usepackage[backend=bibtex, style=numeric] {biblatex}
#+LATEX_HEADER: \addbibresource{references.bib}
#+LATEX_HEADER: \usepackage{acronym}
#+STARTUP: showall
#+TOC: tables

# Title Page
#+begin_src emacs-lisp :exports results :results none :eval export
  (make-variable-buffer-local 'org-latex-title-command)
  (setq org-latex-title-command (concat
     "\\begin{titlepage}\n"
     "\\begin{center}\n"
     "\\includegraphics[width=5cm]{./resources/haw_logo.jpg}\n"
     "\\vspace{2cm}\n"
     "{\\par \\LARGE Hochschule für angewandte Wissenschaften Landshut}\n"
     "\\vspace{0.6cm}"
     "{\\par \\Large Fakultät Informatik} \\vspace{1.2cm}\n"
     "{\\par \\Huge \\bf Bachelor Arbeit} \\vspace{1cm}\n"
     "{\\par \\LARGE %t } \\vspace{1cm}\n"
     "{\\par \\Large \\it von %a} \\vspace{0.2cm}\n"
     "{\\par Matrikel-Nr.: 1088709} \\vspace{1cm} \n"
     "{\\par Abgabedatum: %D} \\vspace{3cm}\n"
     "\\end{center}\n"
     "{\\par Betreuer: Prof. Dr. Mona Riemenschneider}\n"
     "{\\par Zweitkorrektor: Prof. Dr. Abdelmajid Khelil}\n"
     "\\end{titlepage}\n"))
#+end_src

# Abbildungsverzeichnis
#+BEGIN_LATEX
\newpage
\listoffigures
\newpage
#+END_LATEX

# Abkürzungsverzeichnis
#+BEGIN_LATEX
\begin{acronym}[Bash]
\acro{ANN}{künstliches neuronales Netz}
\acro{TF}{Tensor Flow}
\acro{ML}{Machine Learning oder zu deutsch maschinellen Lernen}
\end{acronym}
\newpage
#+END_LATEX

* Einleitung
** Neuronale Netze

Bei dem Forschungsbereich der neuronalen Netze handelt es sich um eine Teilmenge des Bereichs der künstlichen Intelligenz.
Genauer lassen sich künstliche neuronale Netze als einen Teil des maschinellen Lernens einordnen.
Die Inspiration für \acp{ANN} kommt ursprünglich aus der Biologie und der dort vorkommenden neuronalen Verbindungen in Nervensystemen von Lebewesen.
Jedoch sind die Forschungsgebiete zu neuronalen Netzen aus der Biologie und der Informatik/Mathematik zum Großteil disjunkt.
cite:10.5555/3086952

#+LABEL: fig:network
#+CAPTION[Diagramm eines künstlichen neuronalen Netzes]: Diagramm eines fully connected \ac{ANN}, mit einem Hidden Layer (hier blau gekennzeichnet).
#+CAPTION: Es ist gut zu erkennen, wie benachbarte Schichten
#+CAPTION: [[https://commons.wikimedia.org/wiki/File:NeuralNetwork.png][[quelle]​]]
#+ATTR_LATEX: :float wrap :width 8cm :center nil
[[./resources/neural_network.png]]

*** Historisches
Obwohl \ac{ANN}'s erst ca. 2008 ihre Blütezeit erreicht haben, ist die zu Grunde liegende Technologie bereits seit
Mitte bis Ende des 20. Jahrhunderts bekannt.
So schuf Frank Rosenblatt im Jahre 1975 das in Abbildung ref:fig:perceptron dargestellte Modell eines Perceptrons cite:werbos1975beyond, eine
mathematische Abstraktion des aus der Biologie bekannten Neuron.
Das Perceptron wird bis heute als Modell für ein alleinstehendes Neuron in einem \ac{ANN} verwendet.
In dem kommenden Kapitel [[perceptron]] wird noch im Detail auf das Modell des Perceptrons eingegangen.
Der Backpropagation Algorithmus, bei dem es sich um eine Implementation der Kettenregel zur automatischen Differenzierung
von Parametern eines \ac{ANN} handelt, wurde im Jahre 1986 publiziert cite:Rumelhart_1986.
Auch heute erweist sich der Backpropagation Algorithmus als eine sehr effiziente Methode die Gradienten[fn:gradient] eines \ac{ANN}'s zu berechnen und
findet beinahe unverändert Einsatz in verschiedensten modernen Deep Learning Frameworks.
Eine häufige Fehlinformation die über den Backpropagation Algorithmus verbreitet wird, ist dieser sei für das "Lernen" des neuronalen Netzes
verantwortlich.
Dies ist inkorrekt, da eigentlich der Gradient Descent Algorithmus cite:Curry_1944 die von dem Backpropagation berechneten Gradienten nutzt um
die Parameter des Netzes so zu manipulieren, dass der Loss[fn:loss] minimiert wird.

*** Strukturelle Beschaffenheit von neuronalen Netzen <<netstruct>>
#+begin_quote
In diesem Kapitel wird ausschließlich der Aufbau eines fully connected[fn:fullyconnected] feed forward[fn:feedforward] neural networks behandelt.
#+end_quote
Ein \ac{ANN} besteht primär aus mehreren Schichten (Layern) $L_1, \dots, L_n$.
Bei dem ersten Layer $L_1$ handelt es sich um den so genannten Input Layer, der für die Aufnahme von Eingabedaten zuständig ist.
Analog fungiert dieser als Eingabe-Interface des neuronalen Netzes für den Anwender des \acp{ANN}.
So erwartet ein Input-Layer mit $8$ Neuronen einen Input Vektor von $8$ Werten.
Schichten $L_2, \dots, L_{n-1}$ werden als Hidden-Layer des neuronalen Netzes bezeichnet, da diese für den Außenstehenden nicht direkt einsichtig sind.
Die letzte Schicht des \acp{ANN}, $L_n$ ist der Output-Layer des Netztes und dient als zweites Interface für den Nutzer.
In Abbildung ref:fig:network lässt sich die Architektur eines einfachen Feed Forward[fn:feedforward] Networks und dessen Layer klar erkennen.
cite:10.5555/3086952

*** Das Perceptron <<perceptron>>
Ein Perceptron ist ein Modell, das eine Reihe von Eingabedaten (Inputs) $a$ auf einen gemeinsamen Output $y$ nach der Form  $\mathbb{R}^n \rightarrow \mathbb{R}$ abbildet.
Die verschiedenen Inputs $a^n$ werden durch Gewichtungen $w^n$ verschieden stark gewichtet, also $a_1 \times w_1 + \dots + a_n \times w_n = y$ oder in Vektorschreibweise $a \times w^T = y$.
In heutigen State of the Art Deep Learning Frameworks enthält das Perceptron zusätzlich noch eine nicht-lineare Komponente in der Form einer Aktivierungsfunktion $\sigma$.
Einige der häufig eingesetzten Aktivierungsfunktionen können in Tabelle ref:tab:aktivierungsfunktion gefunden werden.
In Kapitel ref:activations dieser Arbeit, wird noch im Genaueren auf Zusammenhänge zwischen Aktivierungsfunktion und Sparsity[fn:sparsity] eines \ac{ANN} Modells eingegangen.
cite:Rosenblatt_1958

#+LABEL: tab:aktivierungsfunktion
#+CAPTION[Populäre Aktivierungsfunktionen]: Aktivierungsfunktionen enthalten meist eine nicht-linearität, die nötig ist um neuronale Netze
#+CAPTION: auf nicht lineare Zusammenhänge in Datensätzen trainieren zu können.
| Name                  | Funktion                                                        |
|-----------------------+-----------------------------------------------------------------|
| Logistische Funktion  | $\frac{1}{1+e^t}$                                               |
| Tangens Hyperbolicus  | $\frac{(e^x-e^{-x})}{(e^x+e^{-x})}$                             |
| Rectified Linear Unit | $f(x)= \begin{cases} 0\ for\ x\leq0 \\ x\ for\ x>0 \end{cases}$ |

Ein Perceptron kann statt in einem grafischen Modell visualisiert zu werden auch als eine mathematische Funktion eqref:eq:percept behandelt werden.

\begin{equation}f(a, w)=\sigma(a\times w^T)=y \label{eq:percept}\end{equation}


#+LABEL: fig:perceptron
#+CAPTION[Diagramm eines einfachen Perceptrons]: Abbildung eines einfachen Perceptrons.
#+CAPTION: Es ist gut zu erkennen, wie der Input Vektor des Layers $\begin{pmatrix} x_1 \\ \dots \\ x_m \end{pmatrix}$ und
#+CAPTION: der Weight Vektor $\begin{pmatrix}w_1 \\ \dots \\ w_m \end{pmatrix}$
#+CAPTION: auf die Variable $y$ durch $\begin{pmatrix} x_1 \\ \dots \\ x_m \end{pmatrix} \begin{pmatrix} w_1 \\ \dots \\ w_m \end{pmatrix}^T = y$ abgebildet werden.
#+CAPTION: Der Output des Layers wird durch die Anwendung einer Aktivierungsfunktion auf die Variable $y$ generiert.
#+CAPTION: cite:towardsdatascience
[[./resources/perceptron.png]]

*** Der Datenfluss in einem künstlichen neuronalen Netz
Daten in einem Feed-Forward \ac{ANN} verlaufen immer linear von Input-Layer in Richtung Output-Layer.
Da bereits in Kapitel ref:perceptron auf die Beschaffenheit eines Layers eingegangen wurde, können wir einen Layer $L$ als eine Funktion $f(x)$ betrachten.
Da der jeweilige Output eines Layers $L_i$ als der Input des Layers $L_{i+1}$ dient, können wir ein Netzwerk als eine Verkettung an Funktionen betrachten.
Im Allgemeinen kann dies in der Form eqref:eq:net ausgedrückt werden.

\begin{equation} {f_n(\dots (f_1(x)))=y \ \ \ \label{eq:net} \end{equation}

Die Möglichkeit ein \ac{ANN} als eine Verkettung von Funktionen formulieren zu können ist essentiell um Algorithmen wie Backpropagation zur
Differenzierung von Parametern nutzen zu können.

** Einführung in naive Pruning Methoden für künstliche neuronale Netze

Es lässt sich durch Beobachtung der künstlichen neuronalen Netze der letzten Jahre feststellen,
dass die Komplexität und die damit einhergehende Anzahl von Neuronen und deren Verbindungen immer weiter zunehmen. cite:altenberger18:_non_techn_survey_deep_convol
Gleichzeitig werden diese komplexeren und größeren \ac{ANN} Architekturen auch auf schwächeren eingebetteten Geräten eingesetzt.
Dadurch werden Optimierungen an neuronalen Netzen immer relevanter, da dies Inferenz-Zeit und Modellgröße minimieren kann.
Verfahren wie Quantisierung können die Laufzeit und den Speicherverbrauch von \ac{ANN}'s deutlich verbessern, jedoch können auch
Pruning Verfahren massive Verbesserungen versprechen. cite:Frankle2018
Pruning Verfahren versuchen durch das Entfernen von Verbindungen oder auch ganzen Neuronen, die Sparsity eines Modells zu erhöhen.
Weight oder auch Connection Pruning bezeichnet den Vorgang Verbindungen aus einem \ac{ANN} zu entfernen.
Dabei werden die Verbindungen eliminiert, also mit $0$ gewichtet. Die ist in Abbildung ref:fig:naiveweightpruning dargestellt.
Die ausgewählten Verbindungen oder Neuronen werden durch eine Heuristik bestimmt, eine Heuristik könnte beispielsweise die niedrigst gewichteten Verbindungen sein.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
from scripts import quad_plot
import sys
sys.path.append('./condense')
from condense.optimizer.layer_operations.weight_prune import w_prune_layer
model = keras.models.load_model('./resources/models/iris.h5')
layer = 1
quad_plot(w_prune_layer(model.get_weights()[0::2][layer], .85),
          model.get_weights()[0::2][layer],
          './resources/plots/iris-weight-pruning.png')
return './resources/plots/iris-weight-pruning.png'
#+END_SRC

#+LABEL: fig:naiveweightpruning
#+CAPTION[Visualisierung von Weight Pruning]:
#+CAPTION: In diesem hier dargestellten Dense Layers eines neuronalen Netzes, wurde die Sparsity des Modells durch Pruning der Verbindungen auf $85\%$ erhöht.
#+CAPTION: Es ist gut zu beobachten, wie nur leicht gewichtete Verbindungen durch Pruning deaktiviert werden, hier durch schwarze Pixel zu erkennen.
#+CAPTION: Bei dem Netz handelt es sich um ein durch TensorFlow 2.0 trainiertes Modell. Bei dem Training wurde der Iris Datensatz genutzt.
#+RESULTS[ab87174a1c51a0676afdc27b4d18b3dda9742200]:
[[file:./resources/plots/iris-weight-pruning.png]]


Analog zu dem Pruning der Verbindungen existiert auch Neuron-Pruning, also das entfernen ganzer Neuronen und deren Verbindungen aus dem \ac{ANN}.
Dies wird in Abbildung ref:fig:naiveneuronpruning durch die Visualisierung eines Layers vor und nach Neuron-Pruning gezeigt.

#+BEGIN_SRC python :exports results :results file :cache yes
import keras
from scripts import quad_plot
import sys
sys.path.append('./condense')
from condense.optimizer.layer_operations.unit_prune import u_prune_layer

layer= 1
model = keras.models.load_model('./resources/models/iris.h5')

quad_plot(u_prune_layer(model.get_weights()[0::2][layer], .4),
          model.get_weights()[0::2][layer], './resources/plots/iris-unit-pruning.png')
return './resources/plots/iris-unit-pruning.png'
#+END_SRC

#+LABEL: fig:naiveneuronpruning
#+CAPTION[Visualisierung von Unit/Neuron Pruning]:
#+CAPTION: In diesem Beispiel wird das oben verwendete Modell durch eine naive Implementation des Neuron Pruning um einen Faktor von $0.4$ optimiert.
#+CAPTION: Vertikale Linien repräsentieren in diesem Diagramm die Weights eines Neuronen.
#+CAPTION: Man kann sehr gut beobachten wie sich ganze Neuronen schwarz färben, also deaktiviert werden.
#+RESULTS[e9612a231390b236029f616c4dcf02b09ad4cd99]:
[[file:./resources/plots/iris-unit-pruning.png]]


** Industrierelevanz
Pruning von künstlichen neuronalen Netzen bietet vielen Unternehmen die Möglichkeit Optimierungen an schon bestehenden \ac{ANN} Modellen vorzunehmen.
Diese Optimierungen können unter Umständen ermöglichen komplexere Modelle auf schwächeren Computern zu nutzen.
Beispielsweise eingebettete Geräte können dabei effizienter Daten durch neuronale Netze auswerten.
Besonders in Situationen in denen das Modell möglichst schnell ein Prognose abgeben soll, wie beispielsweise bei Teilen von
selbständig fahrenden Autos bietet Pruning Chancen auf enorme Verbesserungen.
Zudem bietet Pruning eine Möglichkeit, \ac{ANN} Modelle ohne signifikante Einbußen von Genauigkeit zu optimieren. cite:Frankle2018
Dies sollte Pruning Methoden auf deutlich mehr \ac{ANN} Modellen einsetzbar machen.

** Ziel dieser Arbeit
*** Erstellung eines Pruning Frameworks
Ziel dieser Arbeit ist es primär ein Python Framework zu entwickeln, das mehrere verschiedene Typen von Pruning Methoden implementieren soll.
Ein wichtiger Fokus sollte bei der Architektur des Framework sein, dies in Zukunft möglichst einfach erweitern zu können.
Dokumentation der verschiedenen Module ist aus diesem Grund sehr wichtig und sollte im Laufe der Arbeit auch immer aktualisiert werden.

#+begin_src mermaid :file resources/plots/pruning-framework.png :theme forest :background transparent
graph LR
    input(Input Model) --> interface(High Level Interface)
    interface --> parser(Model Parser)
    pruning(Pruning Engine) --> output(Pruned Model)
    parser --> pruning
#+end_src

#+LABEL: fig:rough-project-structure
#+CAPTION[Pruning Framework Konzept]: Der hier gezeigte Graph soll das grobe Konzept, des im Laufe dieser Arbeit entstehenden Pruning Frameworks zeigen.
#+RESULTS:
[[file:resources/plots/pruning-framework.png]]

Zudem sollte das Framework kompatibel mit aktueller Deep Learning Software und deren Formate kompatibel sein.
Kompatibilität mit \ac{TF} [fn:tensorflow]/Keras[fn:keras] steht bei diesem Projekt im Vordergrund, da auch intern \ac{TF} für Trainings-Operationen genutzt wird.
Optional sollte auch die Möglichkeit bestehen ein Modell in dem ONNX[fn:onnx] Format zu exportieren, um auch Kompatibilität mit anderen Frameworks sicherzustellen.
*** Erkenntnisse über Pruning Methoden

* Methodik
** Erstellung des Frameworks
*** Architektur
*** Dokumentation
**** Allgemeine Dokumentation des Projekts

Durch GitHub Pages[fn:pages] und dem Tool Docsify[fn:docsify] ist es sehr einfach möglich eine ausgesprochen zugängliche Dokumentation
bzw. Landing Page für das Projekt zu generieren.
Der Inhalt dieser Dokumentation ist manuell erstellt und soll dem Benutzer nur einen groben Überblick über die wichtigsten Aspekte des Frameworks geben.
Detailliertere Informationen zu internen Schnittstellen können jedoch trotzdem sehr einfach über die Modul Dokumentation aus Unterpunkt ref:pdoc eingesehen werden.

**** Automatisierte Generierung von Dokumentation aus Source Code des Projektes <<pdoc>>

Durch das Tool pdoc3[fn:pdoc] kann aus dem Source Code eines Python Modules und dessen Docstrings[fn:docstring] eine Dokumentation in Form einer
HTML Seite generiert werden.
Diese ist in die Allgemeine Dokumentation des Projekts direkt eingebettet und erfordert keine separate Website.
Da bei der Generierung dieser Dokumentation keine weitere manuelle Arbeit geleistet werden muss, kann diese ohne weitere Umstände automatisiert
über GitHub Actions[fn:actions] realisiert werden.
So wird beispielsweise bei einer Änderung des Modules auf dem ~master~ Branch des Projekts ein Script ausgelöst, die eine aktualisierte Dokumentation auf
der öffentlichen Webseite zur Verfügung stellt.
Natürlich koaliert die Qualität der generierten Dokumentation direkt mit der Qualität der im Source Code verfassten Docstrings,
somit ist zudem sicherzustellen, dass auch hier ein gewisser Qualitätsstandart einzuhalten ist.
Wie dies innerhalb dieses Projekts implementiert wurde wird in Kapitel ref:tests Punkt ref:docstyle_tests genauer erläutert.

*** Tests <<tests>>

**** Unit Tests

Um sicherzustellen, dass die Qualität der Software einen gewissen Standard erfüllt, sind Unit Tests mit Sicherheit ein essentieller Bestandteil dieses Projekts.
Dazu wurde das sehr weit verbreitete Testing Framework pytest[fn:pytest] genutzt.
Zusätzlich werden Daten über die Test-Coverage der Tests Dank des pytest-cov plugins für pytest generiert.

**** Linting

Um eine ästhetisch ansprechende Formatierung des Quellcodes im Laufe des Projekts beizubehalten.
Durch das Tool pylint wird auch dies automatisiert möglich mit der Hilfe von GitHub Actions möglich.
Einige der wichtigsten von der Software überprüften Punkte sind:
- unnötige ~import~ Statements
- korrekte Variablennamen
- Zeichen per Zeile
- Zeilen-Abstände

**** Docstyle Tests <<docstyle_tests>>

Um auch wichtige Teile wie die Dokumentation von Funktionen nicht im Laufe des Projekts zu vernachlässigen, wurde das Tool pydocstyle[fn:pydocstyle] genutzt um auch
Docstrings auf Korrektheit zu überprüfen.
Als Style der Docstrings wurde sich auf den von Google genutzten Styleguide[fn:styleguide] berufen.
Durch diese Methodik, müssen alle Module, Klassen und Funktionen über Docstrings verfügen, da sie sonst nicht auf einen der nicht-feature branches des Repositories gepullt werden können.
Dadurch lässt sich eine enorm detaillierte Dokumention aller öffentlichen Schnittstellen automatisiert generieren.

** Datensätze <<datensatz>>

#+CAPTION: In dieser Arbeit verwendeten Datensätze
| Datensatz    | Beschreibung | Source                                       |
|--------------+--------------+----------------------------------------------|
| Iris Dataset |              | https://archive.ics.uci.edu/ml/datasets/iris |
| ImageNet     |              | http://www.image-net.org                     |
| ImageNet V2  |              | https://github.com/modestyachts/ImageNetV2   |

* Implementierung
** Sparsity Mask <<sparsity_mask>>

Der Begriff Sparsity Mask/Tensor bezieht sich in dieser Arbeit auf einen Binären Tensor der definiert, welche Felder aus einem Weights Tenor eine $0$ enthalten.
Die Sparsity Mask ist ein essentieller Bestandteil für fast jede Pruning Methode.
Deswegen ist die Klärung dieses Begriffs auch sehr wichtig für kommende Kapitel dieser Arbeit.
Auswirkungen der Sparsity Mask auf ein Array wird in Abbildung ref:fig:simplesparsity dargestellt.

#+BEGIN_SRC python :exports results :results file :cache yes
import numpy as np
import matplotlib.pyplot as plt

a = np.random.rand(10, 20)
m = np.random.rand(10, 20) < 0.4

plt.figure(figsize=(10, 2))
plt.subplot(131)
plt.imshow(a)
plt.title('Ursprüngliches Array $a$')
plt.subplot(132)
plt.imshow(m*-1, cmap='binary')
plt.title('Sparsity Mask $m$')
plt.subplot(133)
plt.imshow(a*m)
plt.title('Maske auf Array angewandt $a \\times m$')
plt.tight_layout()
plt.savefig('resources/plots/masking.png')
return 'resources/plots/masking.png'
#+END_SRC

#+LABEL: fig:simplesparsity
#+CAPTION[Anwendung von einer Sparsity Mask auf ein einfaches Array]:
#+CAPTION: In dieser Grafik wird jedes Feld durch weiß ($1$) und schwarz ($0$) binär visualisiert.
#+CAPTION: Die Multiplikation der Maske $m$ mit dem Array $a$ resultiert in der durch $m$ 'gefilterten' Version des ursprünglichen Arrays $a$.
#+RESULTS[b489619f2ae1bf26670eaaa816a599d772d79015]:
[[file:resources/plots/masking.png]]

** Keras/TensorFlow als Backend
Operationen des neuronalen Netzes wie das Training oder die Evaluierung wird durch das auf neuronale Netze ausgelegte \ac{ML} Framework TensorFlow ausgeführt.
Dies bietet Nutzern erhebliche Vorteile wie die mögliche Ausführung auf verschiedensten Plattformen wie GPU/CPU oder Hardware Beschleunigern.
Zudem lassen sich TensorFlow "Layer" eines neuronalen Netzes einfach durch eine öffentliche API erweitern.
Somit lässt sich im weiteren Verlauf des Projekts eine direkte Integration in das TensorFlow Ökosystem anstreben.
TensorFlow bietet zusätzlich durch das externe Modul ~model-optimization~[fn:model_opt] Optimierungen an einem Keras/TensorFlow Modell vorzunehmen.
Auch Pruning wird derzeit von dem Tool unterstützt, indem ein vorhandenes Modell durch die Augmentation von Layern um Pruning Funktionalität erweitert werden kann.
Es besteht somit die Möglichkeit auch ein schon bestehendes Backend für Pruning Operationen zu nutzen.
Somit können Pruning Operationen zuerst auf das ~model-optimization~ Modul ausgelagert werden und sich auf das Refitting und die Analyse von neuronalen Netzen konzentriert werden.

** One-Shot Pruning
Unter One-Shot Pruning wird verstanden ein bereits trainiertes \ac{ANN} Modell durch eine Pruning Operation einmalig zu manipulieren.
Bei diesem Typ von Pruning werden keine Refitting (Kapitel ref:refitting) Operationen angewandt, also auch keine Datensätze benötigt.
Nachteile sind jedoch eine deutlich verlustbehafteten Optimierung im Kontrast zu iterativen Pruning (Kapitel ref:iterative).
Die Implementierung von One-Shot Pruning gestaltet sich im Vergleich mit iterativen Pruning als trivial.
Es wird eine Maskierungs-Funktion benötigt, um die zu prunenden Felder der Matrix zu ermitteln.
Eine einfache Maskierungs-Funktion wäre beispielsweise die Auswahl aller Felder, die unter eine festgelegten Threshold $t$ liegen.
Die durch diese Funktion resultierende Matrix $S$ wird auch Sparsity-Mask (siehe Kapitel ref:sparsity_mask) genannt.
$$
\caption{Beispiel: }\ W = \begin{pmatrix} 0.4 && 2 \\ 1.1 && 0\end{pmatrix} \text{ mit } t = 1.0 \Rightarrow S = \begin{pmatrix} 0 && 1 \\ 1 && 0 \end{pmatrix}
$$

#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
w = np.random.random((50,100))
t = 0.5
plt.figure(figsize=(15, 3))
plt.subplot(1, 4, 1)
plt.imshow(w, vmin=0.0, vmax=1.0)
plt.title('Source Matrix')

plt.subplot(1, 4, 2)
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=1.0)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.subplot(1, 4, 3)
t += .1
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=1.0)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.subplot(1, 4, 4)
t += .1
w[w < t] = 0
plt.imshow(w, vmin=0.0, vmax=1.0)
plt.title(f'One-Shot pruned Matrix mit Threshold {t}')

plt.tight_layout()
plt.savefig('resources/plots/one-shot-random.png')
return 'resources/plots/one-shot-random.png'
#+END_SRC

#+LABEL: fig:oneshot
#+CAPTION[Visualisierung von One Shot Pruning]: Visualisierung von One Shot Pruning mit verschiedenen Thresholds $t$.
#+CAPTION: Die das Pruning durchführende Operation ~w[w < t] = 0~ setzt alle sich in der Maske ~w < t~ befindenden Felder der Matrix auf $0$.
#+CAPTION: Die Maske ~w < t~ kann durch eine beliebige (Maskierungs-)Funktion ersetzt werden.
#+RESULTS[4d4296df54d9b9d4b9103dabc59b43cf9a792ad6]:
[[file:resources/plots/one-shot-random.png]]

In Abbildung ref:fig:oneshot ist ein derartiges triviales Pruning von Feldern aus einer Matrix in visueller Form dargestellt.

** Iteratives Pruning <<iterative>>

*** Sparsity Mask <<sparsity_mask>>
Der Begriff Sparsity Mask/Tensor bezieht sich in dieser Arbeit auf einen Binären Tensor der definiert, welche Felder aus einem Weights Tenor eine $0$ enthalten.
Für iteratives Pruning ist eine Maske zwingend notwendig, da diese beim Refitting angibt, welche Weights geändert werden können. cite:Frankle2018
Wie Abbildung ref:fig:refitting zeigt, wird durch das Refitting ohne jegliche Maske die Sparsity des Modells zerstört.
Es ist gut in der rechts-unteren Grafik ersichtlich, wie das Refitting die Gewichtungen innerhalb eines Layers verändert.

#+BEGIN_SRC python :exports results :results file :cache yes
import matplotlib.pyplot as plt
import numpy as np
import sys
import keras
import tensorflow_datasets as tfds
sys.path.append('./condense')
import condense

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
ds = ds.repeat()

model = keras.models.load_model('resources/models/iris.h5')
layer = 2
pruning_intensity = 0.8

plt.figure(1, figsize=(10, 5))
plt.subplot(221)

plt.imshow(model.get_weights()[layer], vmax=1.0, vmin=.0)
plt.savefig('resources/plots/iterative-1.png')
plt.title('Weight Matrix eines trainierten Layers \n ohne Pruning Optimierungen')

plt.subplot(222)

plt.imshow((pruned := condense.one_shot(model, pruning_intensity)).get_weights()[layer], vmax=1.0, vmin=.0)
plt.savefig('resources/plots/iterative-1.png')
plt.title(f'Weight Matrix des optimierten Layers mit {round(condense.utils.model_utils.calc_model_sparsity(pruned), 4) * 100}% \n Modell Sparsity')

pre_training = pruned.get_weights()[layer]

# Retraining
plt.subplot(223)

pruned.compile('adam', 'mse')
pruned.fit(ds.batch(30), epochs=15, steps_per_epoch=100)
plt.imshow(pruned.get_weights()[layer], vmin=.0, vmax=1.0)
plt.title(f'Optimiertes Modell nach Refitting ({round(condense.utils.model_utils.calc_model_sparsity(pruned), 4) * 100}% Modell Sparsity)')

# Diff plot
plt.subplot(224)
plt.imshow(np.abs(pre_training - pruned.get_weights()[layer]), cmap='binary')
plt.title('Änderungen der Gewichtungen durch Refitting')

plt.tight_layout()

plt.savefig('resources/plots/iterative-1.png')

return 'resources/plots/iterative-1.png'
#+END_SRC

#+LABEL: fig:refitting
#+CAPTION[Weights eines Layers nach refitting]: In dieser Grafik wird der Layer eines Modells,
#+CAPTION: durch naives Weight und Neuron Pruning optimiert und anschließend durch refitting erneut trainiert.
#+RESULTS[c22c44c8b060e3dbd8f39ee7440c4dc97136acce]:
[[file:resources/plots/iterative-1.png]]

*** Refitting <<refitting>>

* Ergebnisse
** Einfluss von Aktivierungs-Funktionen auf Pruning Verfahren <<activations>>
* Fazit
* Ausblick

#+LATEX: \printbibliography

* Footnotes
[fn:sparsity] Sparsity beschreibt die Anzahl von Feldern in einem Tensor, die einer $0$ entsprechen.
Somit setzt sich die Sparsity eines künstlichen neuronalen Netzes die Sparsity jedes Layers zusammen.
[fn:tensorflow] Ein von Google entwickeltes Deep Learning Framework [[https://www.tensorflow.org][(tensorflow.org)]].
[fn:keras] Ehemalig externes Frontend von \ac{TF}, seit \ac{TF} 2.0 fester Bestandteil des Frameworks.
[fn:onnx] Universales Format für die Persistierung von \ac{ANN} Modellen.
[fn:loss] Als Loss wird der allgemeine Fehler des Netztes auf einem Datensatz bezeichnet.
Die Funktion die den Loss berechnet wird Loss Funktion benannt.
[fn:gradient] Alle Partiellen Ableitungen einer Funktion $f(x_1, \dots, x_n)$ werden als Gradienten $\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \dots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$ bezeichnet.
[fn:feedforward] Bei einem Feed Forward Netzwerk fließen die Daten immer linear durch das Netz und werden zu keinem Zeitpunkt an vorherige Schichten geleitet.
[fn:fullyconnected] Bei einem fully connected \ac{ANN} ist jedes Neuron aus Schicht $L$ mit allen Neuronen der Schicht $L+1$ verbunden.
[fn:pdoc] Open Source Projekt zur Generierung von Dokumentation aus Python Modulen. (https://pdoc3.github.io/pdoc/)
[fn:docstring] In Python wird ein Kommentar Block, der eine Funktion, Klasse oder ein Modul beschreibt als Docstring bezeichnet.
[fn:docsify] Framework mit dem Dokumentation in Form einer Web-App aus Markdown Dateien generiert werden kann. (https://docsify.js.org)
[fn:pages]  Eine von GitHub angebotene Dienstleistung Webseiten durch ein Repository bereitstellen zu können.(https://pages.github.com)
[fn:actions] Ein Dienst um automatisiert Tests oder Deployment Operationen durchzuführen. (https://github.com/features/actions)
[fn:pytest] Testing Framework für die Python Programmiersprache. (https://docs.pytest.org/)
[fn:pydocstyle] Tool um Docstrings eines Python Modules zu überprüfen. (https://github.com/PyCQA/pydocstyle)
[fn:styleguide] Ein von Google genutzter Styleguide für Python Projekte. (https://google.github.io/styleguide/pyguide.html)
[fn:pylint] Software um statische Syntax Analyse auf Python Source Code durchzuführen. (https://www.pylint.org)
[fn:model_opt] Toolkit für TensorFlow Modell Optimierungen. (https://github.com/tensorflow/model-optimization)
[fn:_]

bibliography:references.bib
bibliographystyle:apalike
