% Encoding: UTF-8

@Article{Frankle2018,
  author       = {Jonathan Frankle and Michael Carbin},
  title        = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  abstract     = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  date         = {2018-03-09},
  eprint       = {1803.03635v5},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1803.03635v5:PDF},
  journaltitle = {ICLR 2019},
  keywords     = {cs.LG, cs.AI, cs.NE},
}

@Online{hebiri20:_layer_spars_neural_networ,
  author       = {Mohamed Hebiri AND Johannes Lederer},
  title        = {{Layer Sparsity in Neural Networks}},
  year         = 2020,
  archiveprefix= {arXiv},
  eprint       = {2006.15604v1},
  primaryclass = {cs.LG}
}

@Online{guo16:_dynam_networ_surger_effic_dnns,
  author       = {Yiwen Guo AND Anbang Yao AND Yurong Chen},
  title        = {{Dynamic Network Surgery for Efficient DNNs}},
  year         = 2016,
  archiveprefix= {arXiv},
  eprint       = {1608.04493v2},
  primaryclass = {cs.NE}
}

@Online{altenberger18:_non_techn_survey_deep_convol,
  author       = {Felix Altenberger AND Claus Lenz},
  title        = {{A Non-Technical Survey on Deep Convolutional Neural Network
                  Architectures}},
  year         = 2018,
  archiveprefix= {arXiv},
  eprint       = {1803.02129v1},
  primaryclass = {cs.CV}
}

@book{werbos1975beyond,
  title={Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
  author={Werbos, P.J.},
  url={https://books.google.de/books?id=z81XmgEACAAJ},
  year={1975},
  publisher={Harvard University}
}

@Article{towardsdatascience,
  author = {towardsdatascience},
  title  = {perceptron},
  url    = {https://towardsdatascience.com/the-perceptron-3af34c84838c},
}

@Book{Linnainmaa1976,
  author       = {Seppo Linnainmaa},
  date         = {1.6.1976},
  title        = {Taylor expansion of the accumulated rounding error},
  doi          = {10.1007/bf01931367},
  number       = {2},
  pages        = {146--160},
  publisher    = {Springer Science and Business Media {LLC}},
  url          = {https://link.springer.com/article/10.1007/BF01931367},
  volume       = {16},
  journal      = {{BIT}},
  journaltitle = {Taylor expansion of the accumulated rounding error},
  month        = {jun},
  year         = {1976},
}

@Article{Rumelhart_1986,
  author    = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  title     = {Learning representations by back-propagating errors},
  doi       = {https://doi.org/10.1038/323533a0},
  number    = {6088},
  pages     = {533--536},
  volume    = {323},
  journal   = {Nature},
  month     = {oct},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {1986},
}

@Article{Curry_1944,
  author    = {Haskell B. Curry},
  title     = {The method of steepest descent for non-linear minimization problems},
  doi       = {https://doi.org/10.1090/qam/10667},
  number    = {3},
  pages     = {258--261},
  volume    = {2},
  journal   = {Quarterly of Applied Mathematics},
  month     = {oct},
  publisher = {American Mathematical Society ({AMS})},
  year      = {1944},
}

@Article{Rosenblatt_1958,
  author    = {F. Rosenblatt},
  title     = {The perceptron: A probabilistic model for information storage and organization in the brain.},
  doi       = {10.1037/h0042519},
  number    = {6},
  pages     = {386--408},
  volume    = {65},
  journal   = {Psychological Review},
  publisher = {American Psychological Association ({APA})},
  year      = {1958},
}

@Comment{jabref-meta: databaseType:biblatex;}
@book{10.5555/3086952, author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron}, title = {Deep Learning}, year = {2016}, isbn = {0262035618}, publisher = {The MIT Press}, abstract = {"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.} }

@misc{TFDS,
  title = {{TensorFlow Datasets}, A collection of ready-to-use datasets},
  howpublished = {\url{https://www.tensorflow.org/datasets}},
}

@article{fisher36lda,
  added-at = {2007-09-30T14:27:32.000+0200},
  author = {Fisher, R. A.},
  biburl = {https://www.bibsonomy.org/bibtex/2c9d8d78a8e1bb5adecc7602490f4323f/gromgull},
  interhash = {8475c3a3460b33d2d62c8c992e3044cf},
  intrahash = {c9d8d78a8e1bb5adecc7602490f4323f},
  journal = {Annals of Eugenics},
  keywords = {classic classification linear-classification linear-discriminant-analysis},
  number = 7,
  pages = {179-188},
  timestamp = {2007-09-30T14:27:32.000+0200},
  title = {The Use of Multiple Measurements in Taxonomic Problems},
  volume = 7,
  year = 1936
}
@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}
